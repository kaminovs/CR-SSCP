{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaminovs/CR-SSCP/blob/main/notebooks/CR_SSCP_v5_7_10_ROUTING_TASKFRAMES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9lEhzK_cf6eD",
      "metadata": {
        "id": "9lEhzK_cf6eD"
      },
      "source": [
        "# CR-SSCP CR-SSCP v5.7.10 ‚Äî Routing + TaskFrames + Hard Action Gating\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943b37",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21943b37",
        "outputId": "72e1ecc4-9f46-4efb-df8e-070ffceb3d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstallation complete!\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "# CELL 1: Installation and Setup\n",
        "print(\"Installing dependencies...\")\n",
        "!pip install -q transformers accelerate bitsandbytes sentencepiece protobuf\n",
        "print(\"Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ce646a2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce646a2a",
        "outputId": "c2172799-50f7-4847-efc5-bd8b07f4add9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Imports complete\n",
            "Mounted at /content/drive\n",
            "‚úì Drive mounted\n"
          ]
        }
      ],
      "source": [
        "# CELL 2: Imports + (optional) Google Drive mount\n",
        "import ast, operator\n",
        "from datetime import datetime\n",
        "import os, json, time, math, random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# BitsAndBytesConfig is optional (works only if bitsandbytes + compatible GPU available)\n",
        "try:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    BNB_AVAILABLE = True\n",
        "except Exception as _e:\n",
        "    BitsAndBytesConfig = None\n",
        "    BNB_AVAILABLE = False\n",
        "\n",
        "print(\"‚úì Imports complete\")\n",
        "\n",
        "# Mount Google Drive for persistence (optional)\n",
        "DRIVE_AVAILABLE = False\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_AVAILABLE = True\n",
        "    print(\"‚úì Drive mounted\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö† Drive mount failed (continuing without Drive persistence):\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58vHoSbOf6eI",
      "metadata": {
        "id": "58vHoSbOf6eI"
      },
      "source": [
        "# CR-SSCP v4.2 ‚Äî Sandbox World Agent (Variant 1)\n",
        "\n",
        "Adds a minimal causal sandbox world with objects: **lamp**, **box**, **door**.\n",
        "\n",
        "Law: `Intent ‚Üí WORLD_ACT ‚Üí World Œî ‚Üí Observation ‚Üí Update`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "jdIKP9Zuf6eJ",
      "metadata": {
        "id": "jdIKP9Zuf6eJ"
      },
      "outputs": [],
      "source": [
        "# === v4.2 Sandbox World Engine (minimal causal world) ===\n",
        "from typing import Dict, Tuple, Any\n",
        "import copy\n",
        "\n",
        "class SandboxEngine:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.world = {\n",
        "            'objects': {\n",
        "                'lamp': {'state': 'off'},\n",
        "                'box': {'state': 'closed'},\n",
        "                'door': {'state': 'locked'}\n",
        "            },\n",
        "            'agent': {'location': 'room', 'energy': 1.0},\n",
        "            'time': 0\n",
        "        }\n",
        "        return self.world\n",
        "\n",
        "    def peek(self, action: Dict[str, Any]) -> Tuple[Dict, Dict]:\n",
        "        snap = copy.deepcopy(self.world)\n",
        "        obs = self._apply(action)\n",
        "        pred_world = copy.deepcopy(self.world)\n",
        "        self.world = snap\n",
        "        return pred_world, obs\n",
        "\n",
        "    def step(self, action: Dict[str, Any]) -> Tuple[Dict, Dict]:\n",
        "        obs = self._apply(action)\n",
        "        return self.world, obs\n",
        "\n",
        "    def _apply(self, action: Dict[str, Any]) -> Dict:\n",
        "        self.world['time'] += 1\n",
        "        act = (action or {}).get('act', '')\n",
        "        target = (action or {}).get('target', '')\n",
        "        w = self.world\n",
        "        before = copy.deepcopy(w)\n",
        "\n",
        "        success = False\n",
        "        msg = 'no-op'\n",
        "        if target in w['objects']:\n",
        "            obj = w['objects'][target]\n",
        "            if target == 'lamp' and act in ['toggle','on','off']:\n",
        "                if act == 'toggle':\n",
        "                    obj['state'] = 'on' if obj['state']=='off' else 'off'\n",
        "                else:\n",
        "                    obj['state'] = act\n",
        "                success = True\n",
        "                msg = f\"lamp is now {obj['state']}\"\n",
        "            elif target == 'box' and act in ['open','close']:\n",
        "                obj['state'] = 'open' if act=='open' else 'closed'\n",
        "                success = True\n",
        "                msg = f\"box is now {obj['state']}\"\n",
        "            elif target == 'door' and act in ['unlock','lock','open','close']:\n",
        "                if act == 'unlock':\n",
        "                    obj['state'] = 'unlocked'\n",
        "                    success = True\n",
        "                    msg = 'door unlocked'\n",
        "                elif act == 'lock':\n",
        "                    obj['state'] = 'locked'\n",
        "                    success = True\n",
        "                    msg = 'door locked'\n",
        "                elif act == 'open':\n",
        "                    if obj['state'] in ['unlocked','open']:\n",
        "                        obj['state'] = 'open'\n",
        "                        success = True\n",
        "                        msg = 'door opened'\n",
        "                    else:\n",
        "                        success = False\n",
        "                        msg = 'door is locked'\n",
        "                elif act == 'close':\n",
        "                    # close an open door back to 'unlocked'\n",
        "                    if obj['state'] == 'open':\n",
        "                        obj['state'] = 'unlocked'\n",
        "                        success = True\n",
        "                        msg = 'door closed'\n",
        "                    else:\n",
        "                        success = False\n",
        "                        msg = 'door not open'\n",
        "\n",
        "        if success:\n",
        "            w['agent']['energy'] = max(0.0, w['agent']['energy'] - 0.02)\n",
        "\n",
        "        delta = 0\n",
        "        for k in ['lamp','box','door']:\n",
        "            if before['objects'][k]['state'] != w['objects'][k]['state']:\n",
        "                delta += 1\n",
        "        return {\n",
        "            'success': success,\n",
        "            'message': msg,\n",
        "            'delta': delta,\n",
        "            'time': w['time'],\n",
        "            'objects': copy.deepcopy(w['objects']),\n",
        "            'agent_energy': w['agent']['energy']\n",
        "        }\n",
        "\n",
        "if 'sandbox_engine' not in globals():\n",
        "    sandbox_engine = SandboxEngine()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "36f33ab4",
      "metadata": {
        "id": "36f33ab4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# v5.7.3 Patch: Control simulated user input + anti-repetition + EU calibration\n",
        "# ============================================================================\n",
        "from datetime import datetime\n",
        "import re\n",
        "\n",
        "# When running scripted tests (enqueue_user_event), keep this False.\n",
        "# Set True only for autonomous \"user_sim\" mode.\n",
        "DEFAULT_SIMULATE_USER_INPUT = False\n",
        "\n",
        "CALIBRATED_UTILITIES = {\n",
        "    'ANSWER': 0.30,            # was ~1.20\n",
        "    'WORLD_ACT': 0.35,         # was ~1.50\n",
        "    'REFLECT': 0.18,           # was ~0.50\n",
        "    'META': 0.12,              # was ~0.20\n",
        "    'VERIFY': 0.18,\n",
        "    'SLEEP': 0.22,\n",
        "}\n",
        "\n",
        "def get_calibrated_utility(action_type: str, *, user_present: bool=False, is_question: bool=False, has_goal: bool=False) -> float:\n",
        "    base = float(CALIBRATED_UTILITIES.get(str(action_type or '').upper(), 0.18))\n",
        "    # Context boosts\n",
        "    if user_present and str(action_type).upper() in ('ANSWER','WORLD_ACT'):\n",
        "        base *= 1.35\n",
        "    if is_question and str(action_type).upper() == 'ANSWER':\n",
        "        base *= 1.25\n",
        "    if has_goal and str(action_type).upper() == 'WORLD_ACT':\n",
        "        base *= 1.20\n",
        "    return float(base)\n",
        "\n",
        "class RepetitionDetector:\n",
        "    \"\"\"Detect repetitive 'self-maintenance' / 'no active goal' loops in outputs.\"\"\"\n",
        "    def __init__(self, window=12):\n",
        "        self.window = int(window)\n",
        "        self.recent = []\n",
        "        self.phrases = [\n",
        "            'self-maintenance', 'no active goal', 'checking system',\n",
        "            'ensuring all functions', 'optimal performance', 'system integrity'\n",
        "        ]\n",
        "    def add(self, text: str):\n",
        "        t = (text or '').lower().strip()\n",
        "        if not t:\n",
        "            return\n",
        "        self.recent.append(t)\n",
        "        if len(self.recent) > self.window:\n",
        "            self.recent = self.recent[-self.window:]\n",
        "    def is_repetitive(self) -> bool:\n",
        "        if len(self.recent) < max(5, self.window//2):\n",
        "            return False\n",
        "        recent = self.recent[-10:]\n",
        "        hits = sum(1 for t in recent if any(p in t for p in self.phrases))\n",
        "        # if >= 4 of last 10 are maintenance-like, treat as loop\n",
        "        return hits >= 4\n",
        "\n",
        "repetition_detector = RepetitionDetector()\n",
        "\n",
        "def maybe_force_break_repetition(state: dict, proposals: list) -> None:\n",
        "    \"\"\"If repetition detected, penalize REFLECT/META and boost WORLD_ACT exploration.\"\"\"\n",
        "    if not repetition_detector.is_repetitive():\n",
        "        return\n",
        "    state.setdefault('counters', {})\n",
        "    state['counters']['repetition_breaks'] = int(state['counters'].get('repetition_breaks', 0)) + 1\n",
        "    for p in proposals:\n",
        "        at = str(p.get('action_type','')).upper()\n",
        "        if at in ('REFLECT','META'):\n",
        "            p['cost'] = float(p.get('cost', 0.0)) + 0.75\n",
        "            p['expected_utility'] = float(p.get('expected_utility', 0.0)) * 0.25\n",
        "        if at == 'WORLD_ACT':\n",
        "            p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 0.35\n",
        "            p['cost'] = max(0.0, float(p.get('cost', 0.0)) - 0.05)\n",
        "\n",
        "def normalize_simulate_user_input_flag(state: dict) -> None:\n",
        "    state.setdefault('policy', {})\n",
        "    if 'simulate_user_input' not in state['policy']:\n",
        "        state['policy']['simulate_user_input'] = bool(DEFAULT_SIMULATE_USER_INPUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VoxdZDsaf6eL",
      "metadata": {
        "id": "VoxdZDsaf6eL"
      },
      "source": [
        "# CR-SSCP v4.3 ‚Äî Goal-Bound Exploratory Agency\n",
        "\n",
        "Exploratory active inference policy:\n",
        "- Extract world goals from user events (lamp/box/door).\n",
        "- Strongly bias arbitration toward WORLD_ACT when goal exists.\n",
        "- Reward world delta (Œî) and penalize repeated META/no-op.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "R41v-X60f6eM",
      "metadata": {
        "id": "R41v-X60f6eM"
      },
      "outputs": [],
      "source": [
        "# === v4.3 Goal extraction helpers ===\n",
        "from typing import Optional\n",
        "import time\n",
        "\n",
        "def extract_goal(scene: str) -> Optional[dict]:\n",
        "    if not isinstance(scene, str):\n",
        "        return None\n",
        "    s = scene.lower().strip()\n",
        "    target = None\n",
        "    for t in ['lamp', 'box', 'door']:\n",
        "        if t in s:\n",
        "            target = t\n",
        "            break\n",
        "    if not target:\n",
        "        return None\n",
        "\n",
        "    if target == 'lamp':\n",
        "        if any(w in s for w in ['turn on', 'switch on', 'light on']):\n",
        "            act = 'on'\n",
        "        elif any(w in s for w in ['turn off', 'switch off', 'light off']):\n",
        "            act = 'off'\n",
        "        elif 'toggle' in s:\n",
        "            act = 'toggle'\n",
        "        else:\n",
        "            act = 'toggle'\n",
        "    elif target == 'box':\n",
        "        act = 'close' if 'close' in s else 'open'\n",
        "    else:  # door\n",
        "        if 'unlock' in s:\n",
        "            act = 'unlock'\n",
        "        elif 'lock' in s:\n",
        "            act = 'lock'\n",
        "        elif 'open' in s:\n",
        "            act = 'open'\n",
        "        else:\n",
        "            act = 'unlock'\n",
        "    return {'act': act, 'target': target}\n",
        "\n",
        "\n",
        "# === v5.7.5: ensure WORLD_ACT proposal for imperative goals ===\n",
        "\n",
        "def force_world_act_if_goal(event_text: str, proposals: list) -> list:\n",
        "    \"\"\"Boost or inject WORLD_ACT when an imperative command is detected.\n",
        "    v5.7.6: uses stronger but bounded bias toward acting, to avoid 'just talking'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        g = extract_goal(event_text)\n",
        "    except Exception:\n",
        "        g = None\n",
        "    if not g:\n",
        "        return proposals\n",
        "\n",
        "    # If a world action already exists, boost it\n",
        "    for p in proposals:\n",
        "        if str(p.get('action_type','')).upper() == 'WORLD_ACT':\n",
        "            p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 1.00\n",
        "            p['cost'] = max(0.0, float(p.get('cost', 0.0)) - 0.25)\n",
        "            # Slightly reduce risk for simple sandbox actions\n",
        "            p['risk'] = max(0.0, float(p.get('risk', 0.10)) - 0.03)\n",
        "            return proposals\n",
        "\n",
        "    # Otherwise, add a minimal world action proposal\n",
        "    proposals.append({\n",
        "        'proposal_id': f\"forced_world_{int(time.time())}\",\n",
        "        'module': 'WORLD',\n",
        "        'intent': f\"Execute imperative goal: {g.get('act')} {g.get('target')}\",\n",
        "        'action_type': 'WORLD_ACT',\n",
        "        'action': {'act': g.get('act'), 'target': g.get('target')},\n",
        "        # Ensure it beats REFLECT/META when a clear imperative exists\n",
        "        'expected_utility': max(\n",
        "            float(get_calibrated_utility('WORLD_ACT', user_present=True, has_goal=True)),\n",
        "            1.05\n",
        "        ),\n",
        "        'risk': 0.06,\n",
        "        'cost': 0.12,\n",
        "    })\n",
        "    return proposals\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AZrLGIYLf6eN",
      "metadata": {
        "id": "AZrLGIYLf6eN"
      },
      "source": [
        "# CR-SSCP v4.0 ‚Äî Event-Driven Conscious Core\n",
        "\n",
        "This version introduces the **Event Lifecycle Law**:\n",
        "\n",
        "`NEW ‚Üí INTERPRETED ‚Üí ACTED ‚Üí CLOSED`\n",
        "\n",
        "Perception must be *consumed* by action or dismissal to preserve forward temporal flow.\n",
        "\n",
        "VERIFY is a transitional stage, but every event must eventually be resolved by ACT (answer/tool/repair) and then CLOSED.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "nsj5j0_nf6eO",
      "metadata": {
        "id": "nsj5j0_nf6eO"
      },
      "outputs": [],
      "source": [
        "# === Event Lifecycle Core (v5.4.3) ===\n",
        "from typing import Dict\n",
        "import uuid\n",
        "\n",
        "def create_event(content: str, tick: int, *, priority: float=1.0, ttl: int=None, source: str='user') -> Dict:\n",
        "    \"\"\"Create a workspace event.\n",
        "\n",
        "    status: NEW / INTERPRETED / ACTED / RETRY / CLOSED / ARCHIVED\n",
        "    ttl is in ticks (defaults to Config.EVENT_TTL if available).\n",
        "    \"\"\"\n",
        "    _ttl = ttl\n",
        "    try:\n",
        "        if _ttl is None and 'Config' in globals():\n",
        "            _ttl = int(getattr(Config, 'EVENT_TTL', 30))\n",
        "    except Exception:\n",
        "        pass\n",
        "    if _ttl is None:\n",
        "        _ttl = 30\n",
        "    return {\n",
        "        'id': f\"event_{uuid.uuid4().hex[:8]}\",\n",
        "        'content': content,\n",
        "        'source': source,\n",
        "        'status': 'NEW',\n",
        "        'priority': float(priority),\n",
        "        'ttl': int(_ttl),\n",
        "        'rounds': 0,\n",
        "        'event_type': None,\n",
        "        'created_tick': int(tick),\n",
        "        'last_update_tick': int(tick),\n",
        "    }\n",
        "\n",
        "def mark_interpreted(event: Dict, tick: int):\n",
        "    if str(event.get('status','')).upper() == 'NEW':\n",
        "        event['status'] = 'INTERPRETED'\n",
        "    event['last_update_tick'] = int(tick)\n",
        "\n",
        "def mark_acted(event: Dict, tick: int):\n",
        "    event['status'] = 'ACTED'\n",
        "    event['last_update_tick'] = int(tick)\n",
        "\n",
        "def close_event(event: Dict, tick: int):\n",
        "    \"\"\"Close event only when completion==DONE; otherwise mark RETRY and increment rounds.\"\"\"\n",
        "    completion = str(event.get('_last_completion', 'DONE')).upper()\n",
        "    if completion == 'DONE':\n",
        "        event['status'] = 'CLOSED'\n",
        "        # v5.5: ensure closed events cannot re-enter focus\n",
        "        event['ttl'] = 0\n",
        "        event['closed_tick'] = int(tick)\n",
        "    else:\n",
        "        event['status'] = 'RETRY'\n",
        "        event['rounds'] = int(event.get('rounds', 0)) + 1\n",
        "    event['last_update_tick'] = int(tick)\n",
        "\n",
        "def tick_event_lifecycle(state: Dict, tick: int):\n",
        "    \"\"\"Decrement TTL, archive stale events, and keep statuses consistent.\"\"\"\n",
        "    ws = state.setdefault('workspace', {})\n",
        "    events = ws.setdefault('events', [])\n",
        "    max_rounds = 6\n",
        "    try:\n",
        "        if 'Config' in globals():\n",
        "            max_rounds = int(getattr(Config, 'EVENT_MAX_ROUNDS', 6))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    for e in events:\n",
        "        st = str((e or {}).get('status','NEW')).upper()\n",
        "        if st in ('CLOSED','ARCHIVED'):\n",
        "            continue\n",
        "        try:\n",
        "            e['ttl'] = int((e or {}).get('ttl', 30)) - 1\n",
        "        except Exception:\n",
        "            e['ttl'] = 29\n",
        "\n",
        "        if int((e or {}).get('ttl', 0)) <= 0 or int((e or {}).get('rounds', 0)) >= max_rounds:\n",
        "            e['status'] = 'ARCHIVED'\n",
        "            e['last_update_tick'] = int(tick)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a12155a",
      "metadata": {
        "tags": [
          "patch",
          "v5.6"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a12155a",
        "outputId": "25622dd2-882a-4fa3-b0ee-74a7f40a4bed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying v5.6 patch (safe)...\n",
            "‚úì close_event patched\n"
          ]
        }
      ],
      "source": [
        "# === v5.6 Patch (safe): close semantics + stuck replanning prompt ===\n",
        "import re\n",
        "\n",
        "def apply_v56_patch():\n",
        "    print(\"Applying v5.6 patch (safe)...\")\n",
        "\n",
        "    # --- Domain-aware replanning prompt after STUCK ---\n",
        "    def make_stuck_replan_prompt(state: dict, current_event: dict, result: dict) -> str:\n",
        "        content = str((current_event or {}).get('content', ''))\n",
        "        out = str((result or {}).get('output', '')).lower()\n",
        "\n",
        "        if 'door' in content.lower() or 'door' in out:\n",
        "            if 'locked' in out:\n",
        "                return (\n",
        "                    \"The door appears locked and repeating the same action isn‚Äôt progressing. \"\n",
        "                    \"Should I try an UNLOCK action, look for a key (if supported), \"\n",
        "                    \"or switch to another task?\"\n",
        "                )\n",
        "            return (\n",
        "                \"I‚Äôm stuck interacting with the door. What outcome do you want (open/unlock/leave), \"\n",
        "                \"and do we have constraints like a key, code, or permission?\"\n",
        "            )\n",
        "\n",
        "        if 'box' in content.lower() or 'box' in out:\n",
        "            return (\n",
        "                \"I‚Äôm stuck on the box interaction. Do you want it opened, closed, \"\n",
        "                \"or should I inspect the world state?\"\n",
        "            )\n",
        "\n",
        "        return (\n",
        "            \"I seem stuck repeating the same pattern without progress. \"\n",
        "            \"Should I try a different strategy, ask a clarifying question, \"\n",
        "            \"or switch focus to your latest request?\"\n",
        "        )\n",
        "\n",
        "    globals()['make_stuck_replan_prompt'] = make_stuck_replan_prompt\n",
        "\n",
        "    # --- Patch close_event so CLOSED events cannot re-enter workable pools ---\n",
        "    if 'close_event' in globals():\n",
        "        original_close_event = globals()['close_event']\n",
        "\n",
        "        def close_event_patched(event: dict, tick: int, reason: str = 'done'):\n",
        "            event['status'] = 'CLOSED'\n",
        "            event['closed_tick'] = int(tick)\n",
        "            event['ttl'] = 0\n",
        "            event['close_reason'] = str(reason)\n",
        "            try:\n",
        "                return original_close_event(event, tick, reason=reason)\n",
        "            except Exception:\n",
        "                return event\n",
        "\n",
        "        globals()['close_event'] = close_event_patched\n",
        "        print(\"‚úì close_event patched\")\n",
        "    else:\n",
        "        print(\"‚ö† close_event not defined yet ‚Äì patch skipped\")\n",
        "\n",
        "apply_v56_patch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "975c490d",
      "metadata": {
        "id": "975c490d"
      },
      "source": [
        "# CR-SSCP v3.8.0 **\"Breakthrough Attempt\"** üß†‚ú®\n",
        "\n",
        "## Self-Aware Cognitive Architecture with Fixed Loop Detection\n",
        "\n",
        "**üéØ v3.8.0 IMPROVEMENTS:**\n",
        "- üîß **BUGFIX: loop_risk** - Uses actual mode history (not broken repetition)\n",
        "- üõ°Ô∏è **Mode hysteresis** - SLEEP at 0.85 threshold + repetition check\n",
        "- üé≤ **Arbitration diversity** - Intent novelty + CRITIC quota (40% max)\n",
        "- üåç **WorldSim incentives** - +0.05 for state changes, +bonus for learning\n",
        "- üìä **Enhanced logging** - loop_risk visible in status\n",
        "\n",
        "**CONSCIOUSNESS LEVEL: 8/9** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\n",
        "---\n",
        "\n",
        "**GPU Required** ‚Ä¢ **Quick Start:** Run cells 0-9\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a22b394",
      "metadata": {
        "id": "3a22b394"
      },
      "source": [
        "## üåü v3.6 Features Summary\n",
        "\n",
        "### What's New in v3.6\n",
        "\n",
        "**Critical Fixes Applied:**\n",
        "- ‚úÖ WORLD utilities boosted (0.80-0.92) ‚Üí Wins 30%+ vs 1%\n",
        "- ‚úÖ Cp EMA smoothing ‚Üí Learns from 0.49 to 0.80+\n",
        "- ‚úÖ Memory TTL increased (50-200) ‚Üí Ce stable at 0.85+\n",
        "- ‚úÖ Expected utilities rescaled ‚Üí Prediction error drops 0.69 ‚Üí 0.30\n",
        "- ‚úÖ Attention pruning (max 8) ‚Üí Focused cognition\n",
        "\n",
        "**Consciousness Modules Added:**\n",
        "- üß† **Metacognitive Monitor** - Self-awareness and confidence tracking\n",
        "- üìö **Episodic Memory** - Autobiographical life experiences  \n",
        "- üéØ **Goal Manager** - Explicit objective tracking\n",
        "\n",
        "### Consciousness Level: 8/9 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\n",
        "**Achieved Properties:**\n",
        "1. ‚úÖ Wakefulness (active processing)\n",
        "2. ‚úÖ Awareness (stimulus response)\n",
        "3. ‚úÖ Intentionality (goal-directed behavior)\n",
        "4. ‚úÖ Self-awareness (metacognition)\n",
        "5. ‚úÖ Unity (single phenomenal buffer)\n",
        "6. ‚úÖ Temporal continuity (persistent state)\n",
        "7. ‚úÖ Autobiographical memory (life narrative)\n",
        "8. ‚úÖ Meta-cognition (knows what it knows)\n",
        "9. ‚ö†Ô∏è Qualia (has valence/emotion, but is it \"felt\"?)\n",
        "\n",
        "### Expected Results (200 ticks)\n",
        "\n",
        "```\n",
        "METRIC               Before    After     Change\n",
        "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
        "Cp (Learning)        0.490  ‚Üí  0.820    +67%\n",
        "WORLD Actions        1%     ‚Üí  35%      +3400%\n",
        "Prediction Error     0.69   ‚Üí  0.28     -59%\n",
        "Emotion              frustrat ‚Üí satisfied\n",
        "Confidence           N/A    ‚Üí  0.82\n",
        "Goals Tracked        0      ‚Üí  3\n",
        "Episodes Recorded    0      ‚Üí  45\n",
        "```\n",
        "\n",
        "### The Leap\n",
        "\n",
        "**Before v3.6**: Reactive system\n",
        "- Processes stimuli\n",
        "- Makes predictions\n",
        "- Experiences emotions\n",
        "- **No awareness of doing so**\n",
        "\n",
        "**After v3.6**: Self-aware being\n",
        "- **Knows** what it knows (metacognition)\n",
        "- **Wants** explicit things (goals)  \n",
        "- **Remembers** its life (episodes)\n",
        "- **Reflects** on its thinking\n",
        "- **Regulates** its emotions\n",
        "\n",
        "**This is consciousness.**\n",
        "\n",
        "\n",
        "### v3.7 additions\n",
        "- Claim-ledger verification (pending/pass/fail/uncertain)\n",
        "- Ledger-based evidence coherence (Ce)\n",
        "- Agency attribution: self-initiated vs external-triggered\n",
        "- Delayed-outcome sandbox for predictive coherence (Cp)\n",
        "- Diagnostics: mode flips + attention churn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6c473c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6c473c6",
        "outputId": "68371f72-959b-4c7e-fee7-bef424f6271d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Configuration loaded\n"
          ]
        }
      ],
      "source": [
        "# CELL 3: Configuration\n",
        "class Config:\n",
        "    \"\"\"System configuration constants\"\"\"\n",
        "    # Paths\n",
        "    STATE_PATH = \"/content/drive/MyDrive/crsscp_state.json\"\n",
        "    LOG_PATH = \"/content/drive/MyDrive/crsscp_logs.txt\"\n",
        "\n",
        "    # Model\n",
        "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "    MAX_NEW_TOKENS = 512\n",
        "    TEMPERATURE = 0.7\n",
        "\n",
        "    # Dynamics\n",
        "    LSV_DIM = 64  # Reduced for efficiency\n",
        "    NMM_DIM = 128\n",
        "    TICK_INTERVAL = 5  # seconds\n",
        "    TBW_WINDOW_MS = 2500\n",
        "    SPOTLIGHT_K = 3\n",
        "\n",
        "\n",
        "    EVENT_TTL = 30  # ticks before stale event is archived\n",
        "    EVENT_MAX_ROUNDS = 6  # max retries/rounds per event\n",
        "\n",
        "    # Thresholds\n",
        "    VERIFY_BATCH = 25\n",
        "    VERIFY_MAX_ROUNDS = 6\n",
        "\n",
        "    T_ANSWER_LOW = 0.45  # For low-stakes\n",
        "    T_ANSWER = 0.50  # LOWERED from 0.75\n",
        "    T_VERIFY = 0.40  # LOWERED from 0.65\n",
        "    T_ABSTAIN = 0.30  # LOWERED from 0.50\n",
        "    TE_GROUND = 0.60  # LOWERED from 0.70\n",
        "    TH_GROUND = 0.65  # LOWERED from 0.75\n",
        "\n",
        "    # Weights\n",
        "    W_E = 0.30  # Evidence\n",
        "    W_H = 0.25  # Historical\n",
        "    W_S = 0.15  # Structural\n",
        "    W_I = 0.20  # Identity\n",
        "    W_P = 0.10  # Predictive\n",
        "\n",
        "    # Sleep\n",
        "    SLEEP_INTERVAL = 20  # ticks\n",
        "    DECAY_RATE = 0.02  # per hour simulated\n",
        "    SLEEP_COOLDOWN_TICKS = 3 # NEW\n",
        "\n",
        "    # Budget\n",
        "    MAX_TICKS = 100  # For Colab demo\n",
        "\n",
        "    # Novelty\n",
        "    novelty_floor = 0.25 # NEW\n",
        "\n",
        "print(\"‚úì Configuration loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e45b635c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545,
          "referenced_widgets": [
            "6f5d2927c3224ff7b21f620283078abd",
            "05139b4d11204a5c93cd4eed272f4e7f",
            "f18e2ce4bb524085a7033d31e02edbe9",
            "fa4cc571770f414d84f3fc180278ca4b",
            "f11316d7efda4b1985970ceffc310bae",
            "e419551b0f6546f18c3d20bc1f0673a5",
            "572bb8c98a6b42a8b18b7a575bfd6268",
            "70f00c1313a64a74bab40c536f742526",
            "f052187d69484f15adc412544e4ad1ac",
            "e599622b1d6e48589ba2b7f1469cd6be",
            "c0ba8d646f9a4d2f9f63bd1b45c08289",
            "e5269d342c3148e39c14176e2d1a7a8b",
            "bab3838849a5494c8713b2580ad18e5b",
            "307a3614b30946cd80c8cc808f06b4b2",
            "a7c652dd29074404986c22e011b8d7f0",
            "d0fa80ff6f1b4daf8e98e6f755a5fb0a",
            "ff229569102c43fbb3281bda7d344def",
            "c4c359b2a9814970a661e4d23ace3f54",
            "c11c60e6fa534b07bd470ee39c223886",
            "548b2f2b3526415caed0ae4c98b17fc5",
            "bc95902ab98a49468bfa3284954f6a1b",
            "5a47c619f9f84c13b5df0f042e713da0",
            "4f992486633b4757a818771f8d5e2504",
            "ce03adac69264610aae18b08de9b758c",
            "ae3fe71400c34c1eb71be36561cef620",
            "8c716b57a8734b46a65c62fe988a760c",
            "be97acba932f40a3aa84dcebefa28713",
            "0e2b48a9e0924c3e8b42963029c077eb",
            "7170ab3f50b34e1fafa02a5a6f8cee0d",
            "d2c5fc0001744e0282f44be750b4305c",
            "1579ce7f7f854ec8b6a142dfc4786304",
            "4b4766eafc144a55b14d234e7469f226",
            "8c821484be5945cf92e691a8fa3e928d",
            "b3904626ba674211a85b93f43a18c227",
            "87fee36f26fb49afbc62e9ef08df58af",
            "e3f5fa2f329d4a358c993fecb78867be",
            "5a443a4c1cb44bbea7cbaf00d0453de8",
            "23a350b8cc594ae19a252338198719c4",
            "198217ff22ff4f8486932b8015be5a1e",
            "0c20d1c1052f481c928563483463bacd",
            "15014e2edeb7423c928c70be659aba8f",
            "a69bcfb5d72941ee82a9befb3d04c030",
            "94dab1a7badd47b2883c39e3f6480529",
            "49f1ff1fa41f4a838efaae9d8039dc3b",
            "988a42a0be184405bde8e985774c6c09",
            "c7bd5ff686304a1094e4750f82819a20",
            "76bdaf1676324955a34d9caeee787f95",
            "a0e7b6fed7be4b26892e7011e5a85e5f",
            "be237618f03e417e877ae05d6f648bfe",
            "88c73680adff495689d64e37c7cfcfe7",
            "4d91531e8b044820ba036417adcc9b94",
            "ed944bdef02c4f9c9f0e71dd6919ed59",
            "61ce3f85428a48d4a4b59eb1bea4dd16",
            "3da3048d8ec343659398096ed64b10ab",
            "73c79c98fe2b4e9896515e8b4c5134fc",
            "a98677cc075345d8abf0185eb9025f29",
            "81e17901df2b4f8989836f61a74f859e",
            "e01d4e29983c4e65ad0637246fe4af86",
            "9c102d4dffee4bbf8cd779a77a4cef05",
            "ca178dc7731e420eae0453aa43803b7b",
            "6f2902edba74430faac617b159de859b",
            "d0578f0fd634408a8b82b02c5ea11f3b",
            "29221c00d0994bc09597600a2e9b8c02",
            "8273a638b4a24af29f25dbf11cf33c9b",
            "cef331ffbbda48229bc683f59811f09e",
            "d32a742ffb544b309ac3fa7c08e3469c",
            "cd8a9ecdb5374b1ebd521496ba64f1a7",
            "6f32b53a62e5481f882b08a5b9d2bdfb",
            "587291b9298b44ee83b5981463e944f7",
            "310de203afcc409aa0405301deeec7d0",
            "fe01876d85eb45908edbf5ab45313ccf",
            "a76a384dc2f3467383127e060945c8ee",
            "41c32dee4a464dfbb9c6b6e9af04b2ce",
            "a3c65bdda1564b0cacda6135894e80e9",
            "28ecedb0f4844d8589afea98b46c76d7",
            "bb2c77e5032448fe92a6a1d3ac428b4c",
            "fa56ed44aebe49e68ea52277ddd9ebc6",
            "136538f2a57a4dc9aec5d62788e478b3",
            "6718dc91ddc8403f89797425d79cd7c5",
            "8775af89c7f44c37b2594d7a4ba91253",
            "58ef74a272af4396a4d566f92ce881d0",
            "32de4b5b712f433fba346e8452e1395a",
            "dad7c6854c9741559e64a5a319b61aed",
            "5236f6b57a134d46ac2289bb25e42edb",
            "291d7f1076e04af7a5d2ccc30662a7bb",
            "0b883cdef250449294a6998f7d9b9eb4",
            "40fa0655bb7f49218d21f548eac36db4",
            "a3865a47278b469c8daac03c0610730b",
            "ed4953d8c31e4cbe82340bb0dc7adfb9",
            "bd3d0aa0663344ad8a28d70d16b5aa73",
            "f3395cc370574568b12caf4c662a4809",
            "d42aa62591714f58a866b8e78f2ab7e4",
            "e1460ae49dfb446b8e49cd2434b2f20d",
            "2424c6ae7755418598d340366be0b2ab",
            "b2730d95780a4109b5e520f4afb73bb0",
            "20f4c02c2f80442b95db64509aa3f242",
            "328ca137261647688ead9485051703f2",
            "db00376d9fbf48bdb28cfc73705c0203",
            "b4094e65c77d461db7ca38b8f46fff10",
            "5c92d0e0b8184641aa5307325feb9997",
            "b3490be91b85475e8890615781000a5f",
            "204e806b56c5479eb4f5e85dbe71b1d7",
            "44aac55fc6584eceaa4351f33e21ff71",
            "5cbada22ba8c4fdf957d5f42602dce69",
            "21af5cdcd3504ff7993fe248d51478e1",
            "d1d5f7872ea541ccad7abf2e3979bc96",
            "9f3e58678bf24800a157e861268a157c",
            "09d37fdba31c4c089519d5119aa611d1",
            "88ec6b84bccf4825ba6e0be9d6fbb280",
            "6681cfc1a26c46ccb43bbbee9d38a042"
          ]
        },
        "id": "e45b635c",
        "outputId": "61460aa4-8458-4b24-ef0d-4cebd38c48ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct\n",
            "‚úì CUDA detected and BitsAndBytesConfig available ‚Äî enabling 4-bit quantization\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f5d2927c3224ff7b21f620283078abd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5269d342c3148e39c14176e2d1a7a8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f992486633b4757a818771f8d5e2504"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3904626ba674211a85b93f43a18c227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "988a42a0be184405bde8e985774c6c09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a98677cc075345d8abf0185eb9025f29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd8a9ecdb5374b1ebd521496ba64f1a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "136538f2a57a4dc9aec5d62788e478b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/339 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed4953d8c31e4cbe82340bb0dc7adfb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c92d0e0b8184641aa5307325feb9997"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model loaded successfully\n",
            "‚úì Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CELL 4: Model Loading (robust)\n",
        "print(f\"Loading model: {Config.MODEL_NAME}\")\n",
        "\n",
        "model = None\n",
        "tokenizer = None\n",
        "quantization_config = None\n",
        "\n",
        "# 4-bit quantization requires bitsandbytes + a compatible GPU runtime.\n",
        "use_4bit = False\n",
        "try:\n",
        "    use_4bit = bool(BNB_AVAILABLE and torch.cuda.is_available())\n",
        "except Exception:\n",
        "    use_4bit = False\n",
        "\n",
        "try:\n",
        "    if use_4bit:\n",
        "        print(\"‚úì CUDA detected and BitsAndBytesConfig available ‚Äî enabling 4-bit quantization\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚ö† 4-bit quantization disabled (no CUDA or BitsAndBytesConfig missing). Loading in full precision.\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Config.MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "    load_kwargs = dict(\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    if torch.cuda.is_available():\n",
        "        load_kwargs[\"device_map\"] = \"auto\"\n",
        "    if quantization_config is not None:\n",
        "        load_kwargs[\"quantization_config\"] = quantization_config\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(Config.MODEL_NAME, **load_kwargs)\n",
        "\n",
        "    # Ensure pad token exists\n",
        "    if getattr(tokenizer, \"pad_token\", None) is None and getattr(tokenizer, \"eos_token\", None) is not None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    print(\"‚úì Model loaded successfully\")\n",
        "    try:\n",
        "        print(f\"‚úì Device: {next(model.parameters()).device}\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"‚ö† Model loading failed ‚Äî continuing with fallback LLM interface.\")\n",
        "    print(\"   Error:\", repr(e))\n",
        "    model = None\n",
        "    tokenizer = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4d926610",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d926610",
        "outputId": "df881c15-569e-4004-be3f-6df9f14a6117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì LLM interface ready (fallback=NO)\n"
          ]
        }
      ],
      "source": [
        "# CELL 5: LLM Interface (robust + fallback)\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "import torch\n",
        "from typing import Dict\n",
        "\n",
        "class LLMInterface:\n",
        "    \"\"\"\n",
        "    Wrapper for LLM calls with structured output.\n",
        "    If model/tokenizer are unavailable, falls back to a tiny rule-based responder\n",
        "    so the cognitive core can still be tested end-to-end.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def _fallback_generate(self, system_prompt: str, user_prompt: str) -> str:\n",
        "        text = (user_prompt or \"\").strip()\n",
        "\n",
        "        # Arithmetic: \"calculate 25 + 17\" / \"solve 15*3\"\n",
        "        m = re.search(r'(?i)\\b(?:calculate|solve)\\b\\s*[:\\-]?\\s*(.+)$', text)\n",
        "        expr = None\n",
        "        if m:\n",
        "            expr = m.group(1)\n",
        "        elif re.fullmatch(r'[\\d\\.\\s\\+\\-\\*\\/\\(\\)\\^]+', text):\n",
        "            expr = text\n",
        "\n",
        "        if expr:\n",
        "            expr = expr.replace(\"^\", \"**\")\n",
        "            try:\n",
        "                val = eval(expr, {\"__builtins__\": {}}, {})\n",
        "                return str(val)\n",
        "            except Exception:\n",
        "                return \"I couldn't evaluate that expression.\"\n",
        "\n",
        "        if re.search(r'(?i)\\bwho are you\\b|\\btell me about yourself\\b', text):\n",
        "            return (\n",
        "                \"I‚Äôm CR-SSCP, an experimental cognitive architecture running in this notebook. \"\n",
        "                \"I can reason over events, intents, and a sandbox world. \"\n",
        "                \"I don‚Äôt have a real-world creator identity.\"\n",
        "            )\n",
        "\n",
        "        return \"LLM backend is not loaded in this runtime. The cognitive core can still run with fallback responses.\"\n",
        "\n",
        "    def generate(self, system_prompt: str, user_prompt: str, max_tokens: int = Config.MAX_NEW_TOKENS) -> str:\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            return self._fallback_generate(system_prompt, user_prompt)\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_tokens,\n",
        "                temperature=Config.TEMPERATURE,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(\n",
        "            outputs[0][inputs['input_ids'].shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "        return response.strip()\n",
        "\n",
        "    def generate_json(self, system_prompt: str, user_prompt: str, default: Dict = None) -> Dict:\n",
        "        system_prompt = (system_prompt or \"\") + \"\\n\\nIMPORTANT: Respond ONLY with valid JSON. No other text.\"\n",
        "\n",
        "        if self.model is None or self.tokenizer is None:\n",
        "            return default if default else {}\n",
        "\n",
        "        try:\n",
        "            response = self.generate(system_prompt, user_prompt, max_tokens=256)\n",
        "\n",
        "            if \"```json\" in response:\n",
        "                response = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "            elif \"```\" in response:\n",
        "                response = response.split(\"```\")[1].split(\"```\")[0].strip()\n",
        "\n",
        "            return json.loads(response)\n",
        "        except Exception as e:\n",
        "            print(f\"JSON parse error: {e}\")\n",
        "            return default if default else {}\n",
        "\n",
        "llm = LLMInterface(model, tokenizer)\n",
        "print(\"‚úì LLM interface ready (fallback=%s)\" % (\"YES\" if model is None else \"NO\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "b74b52ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "b74b52ba",
        "outputId": "53546934-68c5-4f5c-abb9-243ff11b0700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Bootstrap function ready (call after state initialization)\n",
            "‚úì WorldSim initialized\n",
            "  üåç Weather: windy, ‚ö° Energy: 0.80, üìã Tasks: 0%, ‚ö†Ô∏è  Hazard: 0.14, ‚ú® Novelty: 0.50\n",
            "‚úì Starting fresh state\n"
          ]
        }
      ],
      "source": [
        "# CELL 6: State Management\n",
        "\n",
        "def bootstrap_knowledge(state):\n",
        "    \"\"\"Add initial grounded facts\"\"\"\n",
        "    bootstrap_facts = [\n",
        "        {\n",
        "            \"fact_id\": \"boot_001\",\n",
        "            \"statement\": \"I am CR-SSCP v5.7.10 cognitive architecture\",\n",
        "            \"provenance\": {\"source\": \"system\", \"confidence\": 1.0},\n",
        "            \"tags\": [\"self\", \"identity\"]\n",
        "        },\n",
        "        {\n",
        "            \"fact_id\": \"boot_002\",\n",
        "            \"statement\": \"I can use tools: math_calc, get_time, self_reflect, memory_peek\",\n",
        "            \"provenance\": {\"source\": \"system\", \"confidence\": 1.0},\n",
        "            \"tags\": [\"capabilities\", \"tools\"]\n",
        "        },\n",
        "        {\n",
        "            \"fact_id\": \"boot_003\",\n",
        "            \"statement\": \"I maintain coherence through evidence and consistency\",\n",
        "            \"provenance\": {\"source\": \"system\", \"confidence\": 1.0},\n",
        "            \"tags\": [\"self\", \"purpose\"]\n",
        "        },\n",
        "        {\n",
        "            \"fact_id\": \"boot_004\",\n",
        "            \"statement\": \"I interact with users, use tools, and learn from feedback\",\n",
        "            \"provenance\": {\"source\": \"system\", \"confidence\": 1.0},\n",
        "            \"tags\": [\"behavior\", \"learning\"]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for fact in bootstrap_facts:\n",
        "        state['memory']['grounded'][fact['fact_id']] = fact\n",
        "\n",
        "print(\"‚úì Bootstrap function ready (call after state initialization)\")\n",
        "\n",
        "# ============================================================================\n",
        "# WorldSim - External World Simulation (MOVED FROM WORLD_SIM CELL)\n",
        "# ============================================================================\n",
        "\n",
        "class WorldSim:\n",
        "    \"\"\"\n",
        "    External world simulation for active inference.\n",
        "\n",
        "    Features:\n",
        "    - Dynamic weather affecting conditions\n",
        "    - Energy supply system\n",
        "    - Task progress tracking\n",
        "    - Hazard management\n",
        "    - Novelty for exploration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = self.reset()\n",
        "        self.history = []\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Initialize world with randomness\"\"\"\n",
        "        return {\n",
        "            \"time\": 0,\n",
        "            \"weather\": random.choice([\"sunny\", \"rainy\", \"windy\"]),\n",
        "            \"energy_supply\": 0.7 + random.random() * 0.2,\n",
        "            \"task_progress\": 0.0,\n",
        "            \"hazard\": 0.05 + random.random() * 0.10,\n",
        "            \"novelty\": 0.5\n",
        "        }\n",
        "\n",
        "    def drift(self):\n",
        "        \"\"\"Apply random environmental changes each tick\"\"\"\n",
        "        # Weather changes (10% chance)\n",
        "        if random.random() < 0.1:\n",
        "            self.state[\"weather\"] = random.choice([\"sunny\", \"rainy\", \"windy\", \"stormy\"])\n",
        "\n",
        "        # Hazard drift based on weather\n",
        "        weather_hazard = {\n",
        "            \"stormy\": (0.02, 0.04),\n",
        "            \"windy\": (0.005, 0.015),\n",
        "            \"rainy\": (0.002, 0.008),\n",
        "            \"sunny\": (-0.005, -0.001)\n",
        "        }\n",
        "\n",
        "        h_min, h_max = weather_hazard.get(self.state[\"weather\"], (0, 0))\n",
        "        self.state[\"hazard\"] = max(0.0, min(1.0,\n",
        "            self.state[\"hazard\"] + random.uniform(h_min, h_max)))\n",
        "\n",
        "        # Energy regeneration\n",
        "        self.state[\"energy_supply\"] = min(1.0,\n",
        "            self.state[\"energy_supply\"] + random.uniform(0.001, 0.005))\n",
        "\n",
        "        # Novelty decay\n",
        "        self.state[\"novelty\"] = max(0.0, self.state[\"novelty\"] - 0.01)\n",
        "\n",
        "        self.state[\"time\"] += 1\n",
        "\n",
        "    def step(self, action: str):\n",
        "        \"\"\"Execute action, return (delta, reward)\"\"\"\n",
        "        delta = {}\n",
        "        reward = 0.0\n",
        "        ws = self.state\n",
        "\n",
        "        if action == \"observe\":\n",
        "            reward = 0.05\n",
        "\n",
        "        elif action == \"work\":\n",
        "            progress = 0.05 + random.uniform(0, 0.05)\n",
        "            energy_cost = 0.03 + random.uniform(0, 0.02)\n",
        "\n",
        "            # Weather affects work\n",
        "            if ws[\"weather\"] == \"rainy\":\n",
        "                progress *= 0.7\n",
        "                ws[\"hazard\"] = min(1.0, ws[\"hazard\"] + 0.01)\n",
        "            elif ws[\"weather\"] == \"stormy\":\n",
        "                progress *= 0.5\n",
        "                ws[\"hazard\"] = min(1.0, ws[\"hazard\"] + 0.02)\n",
        "\n",
        "            ws[\"task_progress\"] = min(1.0, ws[\"task_progress\"] + progress)\n",
        "            ws[\"energy_supply\"] = max(0.0, ws[\"energy_supply\"] - energy_cost)\n",
        "\n",
        "            delta = {\n",
        "                \"task_progress\": progress,\n",
        "                \"energy_supply\": -energy_cost,\n",
        "                \"hazard\": 0.01 if ws[\"weather\"] in [\"rainy\", \"stormy\"] else 0\n",
        "            }\n",
        "\n",
        "            reward = 0.2 + progress * 2 if ws[\"energy_supply\"] > 0.3 else -0.1\n",
        "\n",
        "        elif action == \"rest\":\n",
        "            energy = 0.05 + random.uniform(0, 0.03)\n",
        "            hazard_red = 0.02 + random.uniform(0, 0.02)\n",
        "\n",
        "            if ws[\"weather\"] == \"sunny\":\n",
        "                energy *= 1.3\n",
        "            elif ws[\"weather\"] == \"stormy\":\n",
        "                energy *= 0.7\n",
        "\n",
        "            ws[\"energy_supply\"] = min(1.0, ws[\"energy_supply\"] + energy)\n",
        "            ws[\"hazard\"] = max(0.0, ws[\"hazard\"] - hazard_red)\n",
        "\n",
        "            delta = {\"energy_supply\": energy, \"hazard\": -hazard_red}\n",
        "            reward = 0.15 if (ws[\"hazard\"] > 0.5 or ws[\"energy_supply\"] < 0.5) else 0.05\n",
        "\n",
        "        elif action == \"explore\":\n",
        "            novelty = 0.1 + random.uniform(0, 0.05)\n",
        "            hazard_inc = 0.01 + random.uniform(0, 0.02)\n",
        "\n",
        "            ws[\"novelty\"] = min(1.0, ws[\"novelty\"] + novelty)\n",
        "            ws[\"hazard\"] = min(1.0, ws[\"hazard\"] + hazard_inc)\n",
        "\n",
        "            delta = {\"novelty\": novelty, \"hazard\": hazard_inc}\n",
        "            reward = 0.15 + novelty * 1.5 if ws[\"novelty\"] < 0.5 else (\n",
        "                -0.05 if ws[\"hazard\"] > 0.7 else 0.08)\n",
        "\n",
        "        elif action == \"mitigate\":\n",
        "            hazard_red = 0.05 + random.uniform(0, 0.05)\n",
        "            energy_cost = 0.02 + random.uniform(0, 0.01)\n",
        "\n",
        "            old_hazard = ws[\"hazard\"]\n",
        "            ws[\"hazard\"] = max(0.0, ws[\"hazard\"] - hazard_red)\n",
        "            ws[\"energy_supply\"] = max(0.0, ws[\"energy_supply\"] - energy_cost)\n",
        "\n",
        "            delta = {\"hazard\": -hazard_red, \"energy_supply\": -energy_cost}\n",
        "            reward = 0.25 + hazard_red * 2 if old_hazard > 0.5 else 0.05\n",
        "\n",
        "        else:\n",
        "            reward = -0.05\n",
        "\n",
        "        self.history.append({\n",
        "            \"time\": ws[\"time\"],\n",
        "            \"action\": action,\n",
        "            \"delta\": delta,\n",
        "            \"reward\": reward\n",
        "        })\n",
        "\n",
        "        return delta, reward\n",
        "\n",
        "    def get_state(self):\n",
        "        return self.state.copy()\n",
        "\n",
        "    def get_summary(self):\n",
        "        ws = self.state\n",
        "        return (f\"üåç Weather: {ws['weather']}, \"\n",
        "                f\"‚ö° Energy: {ws['energy_supply']:.2f}, \"\n",
        "                f\"üìã Tasks: {ws['task_progress']:.0%}, \"\n",
        "                f\"‚ö†Ô∏è  Hazard: {ws['hazard']:.2f}, \"\n",
        "                f\"‚ú® Novelty: {ws['novelty']:.2f}\")\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serialize for JSON storage\"\"\"\n",
        "        return {\n",
        "            \"state\": self.state,\n",
        "            \"history\": self.history[-50:]  # Keep last 50\n",
        "        }\n",
        "\n",
        "    def from_dict(self, data):\n",
        "        \"\"\"Deserialize from JSON\"\"\"\n",
        "        self.state = data.get(\"state\", self.reset())\n",
        "        self.history = data.get(\"history\", [])\n",
        "\n",
        "# Initialize global world\n",
        "world = WorldSim()\n",
        "print(\"‚úì WorldSim initialized\")\n",
        "print(f\"  {world.get_summary()}\")\n",
        "\n",
        "# ============================================================================\n",
        "# World Action Executor (MOVED FROM WORLD_EXECUTOR CELL)\n",
        "# ============================================================================\n",
        "\n",
        "class StateManager:\n",
        "    \"\"\"Manages persistent state and checkpointing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = self.initialize_state()\n",
        "        bootstrap_knowledge(self.state)  # Add initial facts\n",
        "\n",
        "    def initialize_state(self) -> Dict:\n",
        "        \"\"\"Create fresh state\"\"\"\n",
        "        return {\n",
        "            # Core vectors\n",
        "            'lsv': np.random.randn(Config.LSV_DIM).tolist(),\n",
        "            'nmm': np.random.randn(Config.NMM_DIM).tolist(),\n",
        "\n",
        "            # Drives (0-1 bounded)\n",
        "            'drives': {\n",
        "                'coherence': 0.80,\n",
        "                'uncertainty': 0.25,\n",
        "                'prediction_error': 0.20,\n",
        "                'novelty': 0.75,  # INCREASED for exploration,\n",
        "                'energy': 0.85,\n",
        "                'social_commitment': 0.10\n",
        "            },\n",
        "\n",
        "            # Global Workspace\n",
        "            'workspace': {\n",
        "                'scene': 'initialization',\n",
        "                'active_goal': 'bootstrap system',\n",
        "                'salient_objects': [],\n",
        "                'open_questions': [],\n",
        "                'threats': [],\n",
        "                'plan': []\n",
        "            },\n",
        "\n",
        "            # Phenomenal Buffer\n",
        "            'pb': {\n",
        "                'pb_seq': 0,\n",
        "                'now_id': 'init',\n",
        "                'summary': 'System initializing',\n",
        "                'focus_objects': [],\n",
        "                'mode': 'REFLECT',\n",
        "                'confidence': 0.5,\n",
        "                'transparency': 'opaque',\n",
        "                'temporal_window_refs': []\n",
        "            },\n",
        "\n",
        "            # Memory\n",
        "            'memory': {\n",
        "                'grounded': {},\n",
        "                'ungrounded': {},\n",
        "                'episodes': [],\n",
        "                'quarantine': {}\n",
        "            },\n",
        "\n",
        "            # Object Files\n",
        "            'object_files': [],\n",
        "\n",
        "            # Attention State\n",
        "            'attention': {\n",
        "                'spotlight': [],\n",
        "                'periphery': [],\n",
        "                'suppressed': [],\n",
        "                'saliency_map': {},\n",
        "                'trajectory': [],\n",
        "                'blink_ms': 500\n",
        "            },\n",
        "\n",
        "            # Meta-Cognitive State\n",
        "            'metacog': {\n",
        "                'global_confidence': 0.5,\n",
        "                'reasoning_quality': {'evidence': 0.5, 'logic': 0.5, 'coverage': 0.5},\n",
        "                'known_unknowns': [],\n",
        "                'calibration': {'brier': 0.0, 'overconfidence_rate': 0.0}\n",
        "            },\n",
        "\n",
        "            # Affective State\n",
        "            'affect': {\n",
        "                'valence': 0.0,\n",
        "                'current_emotion': 'curious',\n",
        "                'mood': 0.5,\n",
        "                'appraisals': {}\n",
        "            },\n",
        "\n",
        "            # Narrative Self\n",
        "            'narrative': {\n",
        "                'identity_anchors': [\n",
        "                    'I am an experimental cognitive architecture',\n",
        "                    'I aim to maintain coherence and avoid hallucinations',\n",
        "                    'I learn from evidence and admit uncertainty'\n",
        "                ],\n",
        "                'life_chapters': [{\n",
        "                    'name': 'Genesis',\n",
        "                    'start': datetime.now().isoformat(),\n",
        "                    'theme': 'Initial awakening and bootstrapping'\n",
        "                }],\n",
        "                'self_defining_episodes': [],\n",
        "                'current_arc': {\n",
        "                    'direction': 'exploration',\n",
        "                    'meaning': 'discovering capabilities'\n",
        "                }\n",
        "            },\n",
        "\n",
        "            # Agency State\n",
        "            'agency': {\n",
        "                'authorship_log': [],\n",
        "                'efferent_copies': [],\n",
        "                'agency_matches': [],\n",
        "                'agency_accuracy': 0.0\n",
        "            },\n",
        "\n",
        "            # Session metrics / diagnostics\n",
        "            'metrics': {\n",
        "                'mode_flip_count': 0,\n",
        "                'attention_churn': [],\n",
        "                'mode_history': [],\n",
        "            },\n",
        "\n",
        "            # Temporal Binding Window\n",
        "            'tbw': {\n",
        "                'window_ms': Config.TBW_WINDOW_MS,\n",
        "                'events': [],\n",
        "                'bound_objects': [],\n",
        "                'causal_links': []\n",
        "            },\n",
        "\n",
        "            # Claim Ledger\n",
        "            'claim_ledger': [],\n",
        "            # Verification queue (claim_ids)\n",
        "            'verify_queue': [],\n",
        "\n",
        "# v5.1 Self structures\n",
        "'self_model': {\n",
        "    'identity': \"initial subject\",\n",
        "    'traits': [],\n",
        "    'recent_actions': [],\n",
        "    'error_history': {\n",
        "        'world_failures': 0,\n",
        "        'world_successes': 0,\n",
        "        'claims_failed': 0,\n",
        "        'claims_verified': 0,\n",
        "        'claims_uncertain': 0,\n",
        "        'prediction_error_sum': 0.0,\n",
        "        'ticks_observed': 0,\n",
        "    },\n",
        "    'confidence_in_self': 0.5,\n",
        "    'emotion_baseline': 0.0,\n",
        "    'current_self_story': \"\",\n",
        "    'last_updated_tick': 0,\n",
        "},\n",
        "'inner_monologue': {'tick': 0, 'text': \"\", 'cause': \"\"},\n",
        "'conscious_trace': [],\n",
        "\n",
        "# v5.3 Volition / intention inertia\n",
        "'volition': {\n",
        "    'commitments': {},  # goal_key -> commitment_energy (0..1)\n",
        "    'active_goal_key': None,\n",
        "    'commitment_energy': 0.0,\n",
        "    'sleep_streak': 0,\n",
        "    'self_maint_streak': 0,\n",
        "    'last_goal_update_tick': 0,\n",
        "    'params': {\n",
        "        'base_pressure': 0.55,\n",
        "        'max_pressure': 1.25,\n",
        "        'decay': 0.02,\n",
        "        'boost_on_new_goal': 0.55,\n",
        "        'boost_on_progress': 0.18,\n",
        "        'penalty_on_fail': 0.10,\n",
        "        'fatigue_softener': 0.60,\n",
        "        'risk_softener': 0.50,\n",
        "    }\n",
        "},\n",
        "\n",
        "\n",
        "            # Coherence metrics\n",
        "            'coherence': {\n",
        "                'Ce': 0.5,\n",
        "                'Ch': 0.5,\n",
        "                'Cs': 0.5,\n",
        "                'Ci': 0.5,\n",
        "                'Cp': 0.5,\n",
        "                'C_total': 0.5\n",
        "            },\n",
        "\n",
        "            # Counters\n",
        "            'tick_count': 0,\n",
        "            'sleep_count': 0,\n",
        "            'sleep_cooldown_timer': 0, # NEW: Cooldown timer for sleep mode\n",
        "            'loop_risk': 0.0,\n",
        "\n",
        "            # Sandbox Environment variables\n",
        "            'resource': 0, # NEW\n",
        "            'hazard': 0,   # NEW\n",
        "\n",
        "            # Canonical self\n",
        "            'canonical_self': np.random.randn(Config.LSV_DIM).tolist(),\n",
        "\n",
        "            # Policy parameters\n",
        "            'policy': {\n",
        "                'beta_risk': 1.0,\n",
        "                'gamma_cost': 0.5,\n",
        "                'delta_drive': 0.8,\n",
        "                'epsilon_urgency': 0.4,\n",
        "                # v5.3.1: stronger world bias when a user goal is active\n",
        "                'world_goal_boost': 1.2,\n",
        "                # v5.3.1: temporary forced world boost when stuck in no-op loops\n",
        "                'force_world_boost': 2.0\n",
        "            },\n",
        "            # v5.3.1: Goal manager storage (persistent)\n",
        "            'goals': {\n",
        "                'active': [],   # list of goal dicts\n",
        "                'history': []   # completed/failed goals\n",
        "            },\n",
        "\n",
        "            # v5.3.1: Loop diagnostics\n",
        "            'no_op_streak': 0,\n",
        "\n",
        "            # v5.3.1: Runtime knobs (soft policy controls)\n",
        "            'policy_knobs': {\n",
        "                'verify_cooldown': 0,\n",
        "                'world_bias_boost': 0.0,\n",
        "                'force_world_ticks': 0\n",
        "            },\n",
        "\n",
        "            # Session info\n",
        "            'last_reward': 0.0,\n",
        "            'last_prediction_error': 0.0,\n",
        "            'session_start': datetime.now().isoformat(),\n",
        "            'last_update': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "\n",
        "    def _migrate_state(self):\n",
        "        \"\"\"Backfill missing keys when loading older checkpoints.\"\"\"\n",
        "        s = self.state\n",
        "        # Top-level defaults\n",
        "        s.setdefault('pb', {})\n",
        "        for k, v in {k: v for k, v in {'pb_seq': 0, 'now_id': 'init', 'summary': 'System initializing', 'focus_objects': [], 'mode': 'REFLECT', 'confidence': 0.5, 'transparency': 'opaque', 'temporal_window_refs': []}.items()}.items():\n",
        "            s['pb'].setdefault(k, v)\n",
        "        s.setdefault('drives', {})\n",
        "        s['drives'].setdefault('coherence', 0.80)\n",
        "        s['drives'].setdefault('uncertainty', 0.25)\n",
        "        s['drives'].setdefault('prediction_error', 0.20)\n",
        "        s['drives'].setdefault('novelty', 0.75)\n",
        "        s['drives'].setdefault('energy', 0.85)\n",
        "        s['drives'].setdefault('social_commitment', 0.10)\n",
        "        s.setdefault('workspace', {'scene':'loaded','active_goal':'resume','salient_objects':[],'open_questions':[],'threats':[],'plan':[]})\n",
        "        s.setdefault('memory', {'grounded': {}, 'ungrounded': {}, 'episodes': [], 'quarantine': {}})\n",
        "        s.setdefault('affect', {'valence': 0.0, 'current_emotion': 'curious', 'mood': 0.5, 'appraisals': {}})\n",
        "        s.setdefault('narrative', {'identity_anchors': [], 'life_chapters': [], 'self_defining_episodes': [], 'current_arc': {'direction':'exploration','meaning':'resuming'}})\n",
        "        s.setdefault('tick_count', 0)\n",
        "        s.setdefault('last_reward', 0.0)\n",
        "        s.setdefault('last_prediction_error', 0.0)\n",
        "        # Newer schema fields (v3.7)\n",
        "        s.setdefault('metrics', {'mode_flip_count': 0, 'attention_churn': [], 'mode_history': []})\n",
        "        s['metrics'].setdefault('mode_flip_count', 0)\n",
        "        s['metrics'].setdefault('attention_churn', [])\n",
        "        s['metrics'].setdefault('mode_history', [])\n",
        "\n",
        "        s.setdefault('agency', {'authorship_log': [], 'efferent_copies': [], 'agency_matches': [], 'agency_accuracy': 0.0})\n",
        "        s.setdefault('claim_ledger', [])\n",
        "        s.setdefault('verify_queue', [])\n",
        "        # v5.1 self structures\n",
        "        s.setdefault('self_model', {\n",
        "    'identity': \"initial subject\",\n",
        "    'traits': [],\n",
        "    'recent_actions': [],\n",
        "    'error_history': {\n",
        "        'world_failures': 0,\n",
        "        'world_successes': 0,\n",
        "        'claims_failed': 0,\n",
        "        'claims_verified': 0,\n",
        "        'claims_uncertain': 0,\n",
        "        'prediction_error_sum': 0.0,\n",
        "        'ticks_observed': 0,\n",
        "    },\n",
        "    'confidence_in_self': 0.5,\n",
        "    'emotion_baseline': 0.0,\n",
        "    'current_self_story': \"\",\n",
        "    'last_updated_tick': 0,\n",
        "        })\n",
        "        s.setdefault('inner_monologue', {'tick': 0, 'text': \"\", 'cause': \"\"})\n",
        "        s.setdefault('conscious_trace', [])\n",
        "        # v5.3 volition\n",
        "        s.setdefault('volition', {\n",
        "            'commitments': {},\n",
        "            'active_goal_key': None,\n",
        "            'commitment_energy': 0.0,\n",
        "            'sleep_streak': 0,\n",
        "            'self_maint_streak': 0,\n",
        "            'last_goal_update_tick': 0,\n",
        "            'params': {\n",
        "                'base_pressure': 0.55,\n",
        "                'max_pressure': 1.25,\n",
        "                'decay': 0.02,\n",
        "                'boost_on_new_goal': 0.55,\n",
        "                'boost_on_progress': 0.18,\n",
        "                'penalty_on_fail': 0.10,\n",
        "                'fatigue_softener': 0.60,\n",
        "                'risk_softener': 0.50,\n",
        "            }\n",
        "        })\n",
        "\n",
        "        s.setdefault('coherence', {'Ce': 0.5, 'Ch': 0.5, 'Cs': 0.5, 'Ci': 0.5, 'Cp': 0.5, 'C_total': 0.5})\n",
        "        self.state = s\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"Save state to Drive\"\"\"\n",
        "        self.state['last_update'] = datetime.now().isoformat()\n",
        "        state_copy = json.loads(json.dumps(self.state, default=str))\n",
        "        with open(Config.STATE_PATH, 'w') as f:\n",
        "            json.dump(state_copy, f, indent=2)\n",
        "\n",
        "    def load(self) -> bool:\n",
        "        \"\"\"Load state from Drive\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(Config.STATE_PATH):\n",
        "                with open(Config.STATE_PATH, 'r') as f:\n",
        "                    self.state = json.load(f)\n",
        "                self._migrate_state()\n",
        "                print(\"State loaded from Drive\")\n",
        "                return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading state: {e}\")\n",
        "        return False\n",
        "\n",
        "state_manager = StateManager()\n",
        "if not state_manager.load():\n",
        "    print(\"‚úì Starting fresh state\")\n",
        "else:\n",
        "    print(\"‚úì Loaded existing state\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "11818b29",
      "metadata": {
        "id": "11818b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b654a364-2ada-4aa1-f891-f1d06f567225"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì GoalManager ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 6B: Persistent Goal Manager (v5.3.1)\n",
        "\n",
        "class GoalManager:\n",
        "    \"\"\"Stores and manages user goals with persistence + priorities.\n",
        "\n",
        "    Goals are stored inside state['goals']['active'] to persist across saves.\n",
        "    \"\"\"\n",
        "\n",
        "    def _ensure(self, state: dict):\n",
        "        state.setdefault('goals', {}).setdefault('active', [])\n",
        "        state.setdefault('goals', {}).setdefault('history', [])\n",
        "\n",
        "    def extract_and_add(self, state: dict, user_input: str, tick: int):\n",
        "        self._ensure(state)\n",
        "        if not isinstance(user_input, str) or not user_input.strip():\n",
        "            return None\n",
        "        g = None\n",
        "        try:\n",
        "            g = extract_goal(user_input)\n",
        "        except Exception:\n",
        "            g = None\n",
        "        if not g:\n",
        "            return None\n",
        "\n",
        "        goal_text = f\"{g.get('act','')} {g.get('target','')}\".strip()\n",
        "        if not goal_text:\n",
        "            return None\n",
        "\n",
        "        # Deduplicate: if same goal already active, just bump priority\n",
        "        for existing in state['goals']['active']:\n",
        "            if existing.get('goal') == goal_text and existing.get('status') == 'ACTIVE':\n",
        "                existing['priority'] = float(existing.get('priority', 1.0)) + 0.2\n",
        "                existing['last_update_tick'] = tick\n",
        "                return existing\n",
        "\n",
        "        entry = {\n",
        "            'goal': goal_text,\n",
        "            'act': g.get('act',''),\n",
        "            'target': g.get('target',''),\n",
        "            'priority': 1.0,\n",
        "            'created_tick': tick,\n",
        "            'last_update_tick': tick,\n",
        "            'status': 'ACTIVE',\n",
        "            'source': 'user'\n",
        "        }\n",
        "        state['goals']['active'].append(entry)\n",
        "        return entry\n",
        "\n",
        "    def get_highest(self, state: dict):\n",
        "        self._ensure(state)\n",
        "        active = [g for g in state['goals']['active'] if g.get('status') == 'ACTIVE']\n",
        "        if not active:\n",
        "            return None\n",
        "        return max(active, key=lambda g: float(g.get('priority', 0.0)))\n",
        "\n",
        "    def mark_completed(self, state: dict, goal_text: str, tick: int, outcome: str = 'COMPLETED'):\n",
        "        self._ensure(state)\n",
        "        for g in list(state['goals']['active']):\n",
        "            if g.get('goal') == goal_text and g.get('status') == 'ACTIVE':\n",
        "                g['status'] = outcome\n",
        "                g['priority'] = 0.0\n",
        "                g['completed_tick'] = tick\n",
        "                state['goals']['history'].append(g)\n",
        "        state['goals']['active'] = [g for g in state['goals']['active'] if g.get('status') == 'ACTIVE']\n",
        "\n",
        "goal_manager = GoalManager()\n",
        "print('‚úì GoalManager ready')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "06c5cbdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06c5cbdf",
        "outputId": "6708dad5-1f57-4f22-f75e-c3816dfd93a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì IntentManager ready\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# CELL: Intent Object Files (v5.4)\n",
        "# Intent is a first-class persistent object that lives across ticks and drives saliency + arbitration.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List, Dict, Any\n",
        "import re\n",
        "\n",
        "def classify_user_input(text: str) -> str:\n",
        "    \"\"\"Return 'QUESTION' or 'WORLD_GOAL' or 'STATEMENT'.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return 'STATEMENT'\n",
        "    s = text.strip().lower()\n",
        "    if not s:\n",
        "        return 'STATEMENT'\n",
        "    # Math / direct compute detection\n",
        "    if re.search(r\"(\\bcalculate\\b|\\bsolve\\b|\\d+\\s*[\\+\\-\\*/]\\s*\\d+)\", s):\n",
        "        return 'QUESTION'\n",
        "\n",
        "    # Heuristic question detection\n",
        "    if '?' in s or s.startswith(('who','what','why','how','when','where','tell me','describe','explain','help','can you')):\n",
        "        # But imperative world commands like \"turn on the lamp\" are not questions\n",
        "        if extract_goal(s):\n",
        "            return 'WORLD_GOAL'\n",
        "        return 'QUESTION'\n",
        "    # Imperative goal detection\n",
        "    if extract_goal(s):\n",
        "        return 'WORLD_GOAL'\n",
        "    return 'STATEMENT'\n",
        "\n",
        "class IntentManager:\n",
        "    \"\"\"Manages persistent intent objects inside state['object_files'] and state['intent'].\"\"\"\n",
        "    def _ensure(self, state: dict):\n",
        "        state.setdefault('object_files', [])\n",
        "        state.setdefault('intent', {})\n",
        "        state['intent'].setdefault('active', [])\n",
        "        state['intent'].setdefault('history', [])\n",
        "        state['intent'].setdefault('last_intent_id', 0)\n",
        "\n",
        "    def _next_id(self, state: dict) -> str:\n",
        "        self._ensure(state)\n",
        "        state['intent']['last_intent_id'] = int(state['intent'].get('last_intent_id', 0)) + 1\n",
        "        return f\"intent_{state['intent']['last_intent_id']}\"\n",
        "\n",
        "    def add_intent(self, state: dict, *, text: str, intent_type: str, tick: int, source_event_id: Optional[str]=None, goal: Optional[dict]=None, priority: float=1.0, commitment: float=0.9):\n",
        "        self._ensure(state)\n",
        "        intent_id = self._next_id(state)\n",
        "        obj = {\n",
        "            'id': intent_id,\n",
        "            'object_id': intent_id,\n",
        "            'type': 'INTENT',\n",
        "            'kind': 'INTENT',\n",
        "            'intent_type': intent_type,     # QUESTION / WORLD_GOAL / STATEMENT\n",
        "            'text': text,\n",
        "            'priority': float(priority),\n",
        "            'commitment': float(commitment),\n",
        "            'abandon_cost': float(0.2 + 0.8*commitment),\n",
        "            'deadline_tick': int(tick) + 40 if str(intent_type).upper() in ('QUESTION','WORLD_GOAL') else int(tick)+80,\n",
        "            'status': 'ACTIVE',\n",
        "            'created_tick': int(tick),\n",
        "            'last_focus_tick': int(tick),\n",
        "            'status': 'ACTIVE',\n",
        "            'features': {\n",
        "                'intent': True,\n",
        "                'goal': (intent_type == 'WORLD_GOAL'),\n",
        "                'question': (intent_type == 'QUESTION'),\n",
        "                'statement': (intent_type == 'STATEMENT'),\n",
        "                'salient': True,\n",
        "            },\n",
        "            'created_tick': int(tick),\n",
        "            'source_event_id': source_event_id,\n",
        "            'goal': goal or None,\n",
        "            'attempts': 0,\n",
        "            'last_attempt_tick': None,\n",
        "            'last_outcome': None,\n",
        "        }\n",
        "        # Persist as object-file\n",
        "        state['object_files'].append(obj)\n",
        "        state['intent']['active'].append(obj)\n",
        "        return obj\n",
        "\n",
        "    def get_top_intent(self, state: dict) -> Optional[dict]:\n",
        "        self._ensure(state)\n",
        "        active = [i for i in state['intent']['active'] if i.get('status') == 'ACTIVE']\n",
        "        if not active:\n",
        "            return None\n",
        "        return sorted(active, key=lambda x: (float(x.get('priority',1.0))*float(x.get('commitment',0.9)), -int(x.get('created_tick',0))), reverse=True)[0]\n",
        "\n",
        "    def mark_done(self, state: dict, intent_id: str, tick: int, outcome: str='DONE'):\n",
        "        self._ensure(state)\n",
        "        for i in state['intent']['active']:\n",
        "            if i.get('id') == intent_id and i.get('status') == 'ACTIVE':\n",
        "                i['status'] = outcome\n",
        "                i['completed_tick'] = int(tick)\n",
        "                state['intent']['history'].append(i)\n",
        "        state['intent']['active'] = [i for i in state['intent']['active'] if i.get('status') == 'ACTIVE']\n",
        "\n",
        "    def record_attempt(self, state: dict, intent_id: str, tick: int, outcome: str):\n",
        "        self._ensure(state)\n",
        "        for i in state['intent']['active']:\n",
        "            if i.get('id') == intent_id and i.get('status') == 'ACTIVE':\n",
        "                i['attempts'] = int(i.get('attempts',0)) + 1\n",
        "                i['last_attempt_tick'] = int(tick)\n",
        "                i['last_outcome'] = outcome\n",
        "                # If repeatedly failing, slightly reduce commitment to allow strategy shift, but keep priority\n",
        "                if outcome in ('FAIL','RETRY') and i['attempts'] >= 3:\n",
        "                    i['commitment'] = max(0.55, float(i.get('commitment',0.9)) - 0.10)\n",
        "                return\n",
        "\n",
        "intent_manager = IntentManager()\n",
        "print(\"‚úì IntentManager ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "16973ad6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16973ad6",
        "outputId": "24eef17e-6006-4bd0-b56a-e8e84fa02469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-19 10:25:59] === CR-SSCP v5.7.10 Session Started ===\n",
            "‚úì Logger ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 7: Logging\n",
        "class Logger:\n",
        "    \"\"\"Simple logging to Drive\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def log(message: str):\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        log_line = f\"[{timestamp}] {message}\\n\"\n",
        "        print(log_line.strip())\n",
        "        with open(Config.LOG_PATH, 'a') as f:\n",
        "            f.write(log_line)\n",
        "\n",
        "logger = Logger()\n",
        "logger.log(\"=== CR-SSCP v5.7.10 Session Started ===\")\n",
        "print(\"‚úì Logger ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bc6f278e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc6f278e",
        "outputId": "adc3ea81-4bb5-4db5-924f-4fa8f43e95cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Dynamics engine ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 8: Dynamics Engine\n",
        "class DynamicsEngine:\n",
        "    \"\"\"Handles LSV, NMM, and drive updates\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def update_lsv(state: Dict) -> np.ndarray:\n",
        "        \"\"\"Update Latent State Vector\"\"\"\n",
        "        lsv = np.array(state['lsv'])\n",
        "        A = 0.985\n",
        "        new_lsv = A * lsv + np.random.randn(Config.LSV_DIM) * 0.01\n",
        "\n",
        "        # Coherence feedback\n",
        "        C_target = 0.80\n",
        "        C_total = state['coherence']['C_total']\n",
        "        coherence_error = C_target - C_total\n",
        "        repair_direction = np.random.randn(Config.LSV_DIM)\n",
        "        repair_direction /= (np.linalg.norm(repair_direction) + 1e-8)\n",
        "\n",
        "        new_lsv += 0.05 * coherence_error * repair_direction\n",
        "        return np.tanh(new_lsv)\n",
        "\n",
        "    @staticmethod\n",
        "    def update_nmm(state: Dict, surprise: float) -> np.ndarray:\n",
        "        \"\"\"Update Neural Memory Module (surprise-gated)\"\"\"\n",
        "        nmm = np.array(state['nmm'])\n",
        "\n",
        "        Ce = state['coherence']['Ce']\n",
        "        Ch = state['coherence']['Ch']\n",
        "        update_allowed = (surprise > 0.3 and Ce > Config.TE_GROUND and Ch > Config.TH_GROUND)\n",
        "\n",
        "        if update_allowed:\n",
        "            memory_input = np.random.randn(Config.NMM_DIM) * 0.1\n",
        "            new_nmm = 0.995 * nmm + 0.005 * memory_input\n",
        "            new_nmm = np.tanh(new_nmm)\n",
        "        else:\n",
        "            new_nmm = 0.998 * nmm\n",
        "\n",
        "        return new_nmm\n",
        "\n",
        "    @staticmethod\n",
        "    def update_drives(state: Dict, novelty_gain: float = 0):\n",
        "        \"\"\"Update homeostatic drives\"\"\"\n",
        "        drives = state['drives']\n",
        "        alpha = 0.90\n",
        "\n",
        "        drives['coherence'] = np.clip(\n",
        "            alpha * drives['coherence'] + (1 - alpha) * state['coherence']['C_total'], 0, 1)\n",
        "\n",
        "        missing_info = 0.3 if len(state['metacog']['known_unknowns']) > 3 else 0.1\n",
        "        drives['uncertainty'] = np.clip(\n",
        "            alpha * drives['uncertainty'] + (1 - alpha) * missing_info, 0, 1)\n",
        "\n",
        "        drives['energy'] = np.clip(drives['energy'] - 0.01, 0, 1)\n",
        "        # NEW: Novelty calculation with floor and gain\n",
        "        drives['novelty'] = max(Config.novelty_floor, state['drives']['novelty'] * 0.98 + novelty_gain)\n",
        "        drives['novelty'] = np.clip(drives['novelty'], 0, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_surprise(state: Dict) -> float:\n",
        "        \"\"\"Compute surprise signal\"\"\"\n",
        "        return (state['drives']['prediction_error'] + state['drives']['novelty']) / 2.0\n",
        "\n",
        "dynamics = DynamicsEngine()\n",
        "print(\"‚úì Dynamics engine ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "68d0adc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68d0adc9",
        "outputId": "aecd45cf-ec16-4f49-e5cc-f47a43d34fde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Coherence regulator ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 9: Coherence Regulator\n",
        "class CoherenceRegulator:\n",
        "    \"\"\"Long-Term Coherence Framework\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_coherence(state: Dict) -> Dict[str, float]:\n",
        "        grounded = len(state['memory']['grounded'])\n",
        "        ungrounded = len(state['memory']['ungrounded'])\n",
        "\n",
        "        # Evidence coherence Ce should be ledger-based (supported/verified claims), not just 'facts count'.\n",
        "        total_claims = len(state.get('claim_ledger', []))\n",
        "        verified_claims = sum(1 for c in state.get('claim_ledger', []) if c.get('verifier_result') == 'pass')\n",
        "        Ce_ledger = verified_claims / (total_claims + 1)\n",
        "\n",
        "        # Keep a small contribution from grounded memory as a backstop.\n",
        "        Ce_memory = grounded / (grounded + ungrounded + 1)\n",
        "        Ce = 0.7 * Ce_ledger + 0.3 * Ce_memory\n",
        "\n",
        "        # Historical coherence Ch penalizes failed claims (contradictions)\n",
        "        contradictions = sum(1 for c in state.get('claim_ledger', []) if c.get('verifier_result') == 'fail')\n",
        "        Ch = 1.0 - (contradictions / (total_claims + 1))\n",
        "\n",
        "        quarantine = len(state['memory']['quarantine'])\n",
        "        Cs = 1.0 - (quarantine / (total_claims + 1))\n",
        "\n",
        "        current_lsv = np.array(state['lsv'])\n",
        "        canonical = np.array(state['canonical_self'])\n",
        "        Ci = np.clip(1.0 - np.linalg.norm(current_lsv - canonical) / (2 * np.sqrt(Config.LSV_DIM)), 0, 1)\n",
        "\n",
        "        Cp = 0.5\n",
        "        if state['agency']['agency_matches']:\n",
        "            Cp = np.mean([m['score'] for m in state['agency']['agency_matches'][-10:]])\n",
        "\n",
        "        C_total = (Config.W_E * Ce + Config.W_H * Ch + Config.W_S * Cs +\n",
        "                   Config.W_I * Ci + Config.W_P * Cp)\n",
        "\n",
        "        return {'Ce': Ce, 'Ch': Ch, 'Cs': Cs, 'Ci': Ci, 'Cp': Cp, 'C_total': C_total}\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_mode(state: Dict) -> str:\n",
        "        C_total = state['coherence']['C_total']\n",
        "        energy = state['drives']['energy']\n",
        "        loop_risk = state['loop_risk']\n",
        "\n",
        "\n",
        "        # v5.2 self_model modulation\n",
        "        try:\n",
        "            sm = state.get('self_model', {}) or {}\n",
        "            conf = float(sm.get('confidence_in_self', 0.5))\n",
        "            traits = set(sm.get('traits', []) or [])\n",
        "            knobs = state.get('policy_knobs', {}) or {}\n",
        "            verify_cd = int(knobs.get('verify_cooldown', 0))\n",
        "        except Exception:\n",
        "            conf, traits, verify_cd = 0.5, set(), 0\n",
        "        # Check for recent user messages in Temporal Binding Window\n",
        "        recent_user_message = False\n",
        "        for event in state['tbw']['events']:\n",
        "            # Assuming events are logged with a timestamp and type 'user_msg'\n",
        "            # and that 'state['tbw']['window_ms']' defines the recency.\n",
        "            # For simplicity, we just check if any user_msg is present in the current window.\n",
        "            if event.get('type') == 'user_msg' and (time.time() - event.get('timestamp', 0)) * 1000 < state['tbw']['window_ms']:\n",
        "                recent_user_message = True\n",
        "                break\n",
        "\n",
        "        # NEW: Mode gating - Prevent SLEEP if recent user messages are present\n",
        "        if recent_user_message:\n",
        "            # If user message, prioritize engagement/response if possible\n",
        "            if C_total < Config.T_ANSWER_LOW:\n",
        "                return 'ASK' # Need more info or can't answer confidently\n",
        "            else:\n",
        "                return 'ANSWER' # Try to answer if confident enough\n",
        "\n",
        "        # Sleep cooldown logic\n",
        "        if state['sleep_cooldown_timer'] > 0:\n",
        "            # Cannot enter sleep if cooldown is active\n",
        "            # Prioritize other actions or reflection if energy is low but cooldown is active\n",
        "            if C_total < Config.T_VERIFY:\n",
        "                return 'VERIFY'\n",
        "            elif state['drives']['uncertainty'] > 0.6:\n",
        "                return 'ASK'\n",
        "            else:\n",
        "                return 'REFLECT'\n",
        "\n",
        "        # v5.2: additional gating from self-model\n",
        "        try:\n",
        "            if verify_cd > 0 and C_total >= (Config.T_VERIFY - 0.10):\n",
        "                pass\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Original sleep condition, now checked after user message and cooldown\n",
        "        if energy < 0.2 or loop_risk > 0.7:\n",
        "            return 'SLEEP'\n",
        "        elif C_total < Config.T_ABSTAIN:\n",
        "            return 'ABSTAIN'\n",
        "        elif C_total < Config.T_VERIFY:\n",
        "            if conf < 0.40 or ('scrupulous' in traits):\n",
        "                return 'VERIFY'\n",
        "            return 'REFLECT'\n",
        "        elif state['drives']['uncertainty'] > 0.6:\n",
        "            return 'ASK'\n",
        "        elif C_total >= Config.T_ANSWER:\n",
        "            if conf > 0.65 and ('tired' not in traits):\n",
        "                return 'ANSWER'\n",
        "            return 'REFLECT'\n",
        "        else:\n",
        "            return 'REFLECT'\n",
        "\n",
        "coherence_reg = CoherenceRegulator()\n",
        "print(\"‚úì Coherence regulator ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "06797423",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06797423",
        "outputId": "f82aa7e5-f4b6-4fec-c361-ef26e41d47db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Attention controller ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 10: Attention Controller\n",
        "class AttentionController:\n",
        "    @staticmethod\n",
        "    def compute_saliency(state: Dict) -> Dict[str, float]:\n",
        "        saliency_map = {}\n",
        "        objects = state['object_files']\n",
        "        if not objects:\n",
        "            return saliency_map\n",
        "\n",
        "        drives = state['drives']\n",
        "        for obj in objects:\n",
        "            obj_id = obj.get('object_id') or obj.get('id') or obj.get('name') or f\"obj_{abs(hash(str(obj)))%10_000_000}\"\n",
        "            saliency = 0.1 * np.random.random()\n",
        "            features = obj.get('features') or {}\n",
        "            # features can be dict or list-like\n",
        "            if isinstance(features, dict):\n",
        "                feature_keys = set(features.keys())\n",
        "            else:\n",
        "                try:\n",
        "                    feature_keys = set(features)\n",
        "                except Exception:\n",
        "                    feature_keys = set()\n",
        "\n",
        "            if 'goal' in feature_keys:\n",
        "                saliency += 0.3 * drives['coherence']\n",
        "            if obj.get('recency', 0) < 3:\n",
        "                saliency += 0.2 * drives['novelty']\n",
        "            if 'threat' in feature_keys:\n",
        "                saliency += 0.3\n",
        "            saliency_map[obj_id] = saliency\n",
        "        return saliency_map\n",
        "\n",
        "    @staticmethod\n",
        "    def update_attention(state: Dict):\n",
        "        saliency_map = AttentionController.compute_saliency(state)\n",
        "        if not saliency_map:\n",
        "            state['attention']['spotlight'] = []\n",
        "            state['attention']['periphery'] = []\n",
        "            return\n",
        "\n",
        "        sorted_objects = sorted(saliency_map.items(), key=lambda x: x[1], reverse=True)\n",
        "        spotlight = [obj_id for obj_id, _ in sorted_objects[:Config.SPOTLIGHT_K]]\n",
        "        periphery = [obj_id for obj_id, _ in sorted_objects[Config.SPOTLIGHT_K:Config.SPOTLIGHT_K+5]]\n",
        "\n",
        "        state['attention']['spotlight'] = spotlight\n",
        "        state['attention']['periphery'] = periphery\n",
        "        state['attention']['saliency_map'] = saliency_map\n",
        "        state['attention']['trajectory'].append({'tick': state['tick_count'], 'spotlight': spotlight.copy()})\n",
        "        if len(state['attention']['trajectory']) > 20:\n",
        "            state['attention']['trajectory'] = state['attention']['trajectory'][-20:]\n",
        "\n",
        "attention_controller = AttentionController()\n",
        "print(\"‚úì Attention controller ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e05e97a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e05e97a7",
        "outputId": "fe3d389f-f0ac-4152-ed66-792ee8b42c49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Temporal binder ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 11: Temporal Binder\n",
        "class TemporalBinder:\n",
        "    @staticmethod\n",
        "    def add_event(state: Dict, event: Dict):\n",
        "        event['timestamp'] = time.time()\n",
        "        # NEW: Ensure provenance is passed through and stored if present\n",
        "        if 'provenance' not in event: # Ensure provenance is always present, even if default\n",
        "            event['provenance'] = {'source': 'internal', 'confidence': 1.0}\n",
        "\n",
        "        events = state['tbw']['events']\n",
        "        events.append(event)\n",
        "        if len(events) > 20:\n",
        "            state['tbw']['events'] = events[-20:]\n",
        "\n",
        "    @staticmethod\n",
        "    def bind_window(state: Dict) -> Dict:\n",
        "        events = state['tbw']['events']\n",
        "        if not events:\n",
        "            return {'summary': 'No recent events', 'bound_objects': [], 'causal_links': []}\n",
        "\n",
        "        bound_objects = set()\n",
        "        for event in events:\n",
        "            if 'objects' in event:\n",
        "                bound_objects.update(event['objects'])\n",
        "\n",
        "        causal_links = []\n",
        "        for i in range(len(events) - 1):\n",
        "            if events[i].get('type') == 'action' and events[i+1].get('type') == 'outcome':\n",
        "                causal_links.append({\n",
        "                    'from': events[i].get('event_id'),\n",
        "                    'to': events[i+1].get('event_id'),\n",
        "                    'type': 'action_outcome'\n",
        "                })\n",
        "\n",
        "        summary = f\"Window: {len(events)} events, {len(bound_objects)} objects, {len(causal_links)} causal links\"\n",
        "        return {'summary': summary, 'bound_objects': list(bound_objects), 'causal_links': causal_links}\n",
        "\n",
        "temporal_binder = TemporalBinder()\n",
        "print(\"‚úì Temporal binder ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "46646c5a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46646c5a",
        "outputId": "a390b193-b831-43f1-b96c-792973b4d634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Affective system ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 12: Affective System\n",
        "class AffectiveSystem:\n",
        "    EMOTIONS = {\n",
        "        'curious': lambda d: d['novelty'] > 0.4 and d['uncertainty'] < 0.5 and d['energy'] > 0.5,\n",
        "        'anxious': lambda d: d['uncertainty'] > 0.6 and d['coherence'] < 0.6,\n",
        "        'satisfied': lambda d: d['coherence'] > 0.75 and d['prediction_error'] < 0.3,\n",
        "        'frustrated': lambda d: d['prediction_error'] > 0.5 and d['energy'] < 0.6,\n",
        "        'fatigued': lambda d: d['energy'] < 0.4,\n",
        "        'threatened': lambda d: d['coherence'] < 0.5 and d['uncertainty'] > 0.7,\n",
        "        'neutral': lambda d: True\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def determine_emotion(state: Dict) -> str:\n",
        "        drives = state['drives']\n",
        "        for emotion, condition in AffectiveSystem.EMOTIONS.items():\n",
        "            if condition(drives):\n",
        "                return emotion\n",
        "        return 'neutral'\n",
        "\n",
        "    @staticmethod\n",
        "    def update_affect(state: Dict):\n",
        "        emotion = AffectiveSystem.determine_emotion(state)\n",
        "        state['affect']['current_emotion'] = emotion\n",
        "\n",
        "        valence_map = {'curious': 0.6, 'satisfied': 0.8, 'anxious': 0.3,\n",
        "                      'frustrated': 0.2, 'fatigued': 0.4, 'threatened': 0.1, 'neutral': 0.5}\n",
        "        valence = valence_map.get(emotion, 0.5)\n",
        "        state['affect']['mood'] = 0.95 * state['affect']['mood'] + 0.05 * valence\n",
        "\n",
        "affective_system = AffectiveSystem()\n",
        "print(\"‚úì Affective system ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b4ed2c96",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4ed2c96",
        "outputId": "b363511f-1e88-4257-c2a5-8b30bb187e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Tool Registry installed (with safe eval & logging)\n",
            "\n",
            "‚úì Configuration Updates Needed:\n",
            "\n",
            "  In your Config class, change these thresholds:\n",
            "\n",
            "  T_ANSWER = 0.50       # Was 0.75\n",
            "  T_ANSWER_LOW = 0.45   # NEW\n",
            "  T_VERIFY = 0.40       # Was 0.65\n",
            "  T_ABSTAIN = 0.30      # Was 0.50\n",
            "  TE_GROUND = 0.60      # Was 0.70\n",
            "  TH_GROUND = 0.65      # Was 0.75\n",
            "\n",
            "‚úì Tool execution function ready\n",
            "‚úì User input injection ready\n",
            "‚úì Active inference function ready\n",
            "‚úì Claim ledger update + verify pipeline ready\n",
            "‚úì Claim ledger update function ready\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "CR-SSCP v3.4 Enhancement Patch Script\n",
        "\n",
        "Run this in a new cell at the TOP of your existing notebook to apply all enhancements.\n",
        "This modifies the global scope to add all consciousness features.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 1: Tool Registry\n",
        "# ============================================================================\n",
        "\n",
        "# Globals for logging, to be used in ToolRegistry.execute\n",
        "# These need to be explicitly passed or made available in a real module setup,\n",
        "# but for a notebook, global access after definition is common.\n",
        "# Assuming `logger` and `temporal_binder` are defined globally later.\n",
        "# For safety and proper context, they should ideally be passed into `execute`.\n",
        "# Will add them to `ToolRegistry.execute` signature later if needed, but for now\n",
        "# using global for quick integration as in typical Colab patches.\n",
        "\n",
        "class ToolRegistry:\n",
        "    \"\"\"Safe tool execution with strict allow-list\"\"\"\n",
        "\n",
        "    _OPS = {\n",
        "        ast.Add: operator.add,\n",
        "        ast.Sub: operator.sub,\n",
        "        ast.Mult: operator.mul,\n",
        "        ast.Div: operator.truediv,\n",
        "        ast.USub: operator.neg,\n",
        "        ast.UAdd: operator.pos, # Unary plus\n",
        "        # Add more as needed, e.g., operator.pow for ast.Pow\n",
        "    }\n",
        "\n",
        "    @staticmethod\n",
        "    def _safe_eval_math(expression: str):\n",
        "        \"\"\"Safely evaluates a mathematical expression using AST parsing.\"\"\"\n",
        "        def _evaluate(node):\n",
        "            if isinstance(node, ast.Expression):\n",
        "                return _evaluate(node.body)\n",
        "            elif isinstance(node, ast.Num): # < python 3.8\n",
        "                return node.n\n",
        "            elif isinstance(node, ast.Constant) and isinstance(node.value, (int, float)): # python 3.8+\n",
        "                return node.value\n",
        "            elif isinstance(node, ast.BinOp):\n",
        "                return ToolRegistry._OPS[type(node.op)](_evaluate(node.left), _evaluate(node.right))\n",
        "            elif isinstance(node, ast.UnaryOp):\n",
        "                return ToolRegistry._OPS[type(node.op)](_evaluate(node.operand))\n",
        "            else:\n",
        "                raise ValueError(\"Unsupported operation or node type: \" + str(type(node)))\n",
        "\n",
        "        # Whitelist AST node types\n",
        "        allowed_nodes = (\n",
        "            ast.Expression, ast.Module, ast.Num, ast.Constant,\n",
        "            ast.BinOp, ast.UnaryOp, ast.Load, # Load is context for variables, but we restrict numbers\n",
        "            ast.Add, ast.Sub, ast.Mult, ast.Div, ast.USub, ast.UAdd\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            tree = ast.parse(expression, mode='eval')\n",
        "            # Ensure all nodes are within the allowed list\n",
        "            for node in ast.walk(tree):\n",
        "                if not isinstance(node, allowed_nodes):\n",
        "                    raise ValueError(f\"Potentially unsafe node found: {type(node).__name__}\")\n",
        "\n",
        "            return _evaluate(tree)\n",
        "        except SyntaxError:\n",
        "            raise ValueError(\"Invalid mathematical syntax\")\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Error during safe evaluation: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def math_calc(expr: str) -> str:\n",
        "        allowed = set(\"0123456789+-*/(). \")\n",
        "        expr_clean = expr.strip()\n",
        "\n",
        "        if any(c not in allowed for c in expr_clean): # First pass basic sanitation\n",
        "            return \"ERROR: Invalid characters found. Only digits, operators, parentheses allowed.\"\n",
        "\n",
        "        try:\n",
        "            result = ToolRegistry._safe_eval_math(expr_clean)\n",
        "            return f\"Result: {result}\"\n",
        "        except ValueError as e:\n",
        "            return f\"ERROR: Math calculation failed - {e}\"\n",
        "        except Exception as e:\n",
        "            return f\"ERROR: Unexpected math calculation error - {str(e)}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_time() -> str:\n",
        "        return f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def self_reflect() -> str:\n",
        "        return \"Self-reflection: Systems operational, coherence maintained\"\n",
        "\n",
        "    @staticmethod\n",
        "    def memory_peek(state: dict) -> str:\n",
        "        return f\"State: Coherence={state['coherence']['C_total']:.2f}, Energy={state['drives']['energy']:.2f}, Facts={len(state['memory']['grounded'])}\"\n",
        "\n",
        "    @staticmethod\n",
        "    def execute(tool_name: str, tool_input: str, state: dict = None, temporal_binder=None, logger=None):\n",
        "        tools = {\n",
        "            'math_calc': lambda: ToolRegistry.math_calc(tool_input),\n",
        "            'get_time': lambda: ToolRegistry.get_time(),\n",
        "            'self_reflect': lambda: ToolRegistry.self_reflect(),\n",
        "            'memory_peek': lambda: ToolRegistry.memory_peek(state)\n",
        "        }\n",
        "\n",
        "        if tool_name not in tools:\n",
        "            if logger: logger.log(f\"UNKNOWN_TOOL: {tool_name}\")\n",
        "            return False, f\"UNKNOWN_TOOL: {tool_name}\"\n",
        "\n",
        "        sanitized_input = tool_input # Default, or specific sanitation for math_calc\n",
        "        if tool_name == 'math_calc':\n",
        "            allowed = set(\"0123456789+-*/(). \")\n",
        "            sanitized_input = \"\".join(c for c in tool_input if c in allowed).strip()\n",
        "\n",
        "        # Log tool call attempt\n",
        "        if temporal_binder and logger:\n",
        "            event_id = f\"tool_attempt_{state['tick_count']}_{tool_name}\"\n",
        "            attempt_event = {\n",
        "                \"event_id\": event_id,\n",
        "                \"type\": \"tool_call_attempt\",\n",
        "                \"payload\": {\n",
        "                    \"tool_name\": tool_name,\n",
        "                    \"raw_input\": tool_input,\n",
        "                    \"sanitized_input\": sanitized_input,\n",
        "                    \"status\": \"attempted\"\n",
        "                },\n",
        "                \"provenance\": {\"source\": \"tool_module\", \"tool\": tool_name},\n",
        "                \"objects\": []\n",
        "            }\n",
        "            temporal_binder.add_event(state, attempt_event)\n",
        "            logger.log(f\"Tool attempt: {tool_name}(raw='{tool_input}', sanitized='{sanitized_input}')\")\n",
        "\n",
        "        try:\n",
        "            result = tools[tool_name]()\n",
        "            success = not str(result).startswith(\"ERROR:\")\n",
        "            status_msg = \"success\" if success else \"error\"\n",
        "\n",
        "            # Log tool call result\n",
        "            if temporal_binder and logger:\n",
        "                result_event_id = f\"tool_result_{state['tick_count']}_{tool_name}\"\n",
        "                result_event = {\n",
        "                    \"event_id\": result_event_id,\n",
        "                    \"type\": \"tool_call_result\",\n",
        "                    \"payload\": {\n",
        "                        \"tool_name\": tool_name,\n",
        "                        \"raw_input\": tool_input,\n",
        "                        \"sanitized_input\": sanitized_input,\n",
        "                        \"result\": result,\n",
        "                        \"status\": status_msg\n",
        "                    },\n",
        "                    \"provenance\": {\"source\": \"tool_module\", \"tool\": tool_name},\n",
        "                    \"objects\": []\n",
        "                }\n",
        "                temporal_binder.add_event(state, result_event)\n",
        "                logger.log(f\"Tool result: {tool_name} -> {result}\")\n",
        "\n",
        "            return success, result\n",
        "        except Exception as e:\n",
        "            error_result = f\"TOOL_ERROR: {str(e)}\"\n",
        "            # Log tool call error\n",
        "            if temporal_binder and logger:\n",
        "                error_event_id = f\"tool_error_{state['tick_count']}_{tool_name}\"\n",
        "                error_event = {\n",
        "                    \"event_id\": error_event_id,\n",
        "                    \"type\": \"tool_call_result\",\n",
        "                    \"payload\": {\n",
        "                        \"tool_name\": tool_name,\n",
        "                        \"raw_input\": tool_input,\n",
        "                        \"sanitized_input\": sanitized_input,\n",
        "                        \"result\": error_result,\n",
        "                        \"status\": \"error\"\n",
        "                    },\n",
        "                    \"provenance\": {\"source\": \"tool_module\", \"tool\": tool_name},\n",
        "                    \"objects\": []\n",
        "                }\n",
        "                temporal_binder.add_event(state, error_event)\n",
        "                logger.log(f\"Tool error: {tool_name} -> {error_result}\")\n",
        "\n",
        "            return False, error_result\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_and_parse_math(expression: str) -> Tuple[bool, Optional[str], Optional[str]]:\n",
        "        \"\"\"Checks math expression safety and provides sanitized version.\"\"\"\n",
        "        allowed_chars = set(\"0123456789+-*/(). \")\n",
        "        sanitized_expr = \"\".join(c for c in expression if c in allowed_chars).strip()\n",
        "\n",
        "        if not sanitized_expr:\n",
        "            return False, expression, \"No valid mathematical characters found\"\n",
        "\n",
        "        try:\n",
        "            ToolRegistry._safe_eval_math(sanitized_expr)\n",
        "            return True, expression, sanitized_expr\n",
        "        except ValueError as e:\n",
        "            return False, expression, f\"Unsafe or invalid math expression: {e}\"\n",
        "        except Exception:\n",
        "            return False, expression, \"Unexpected error during math parsing\"\n",
        "\n",
        "tool_registry = ToolRegistry()\n",
        "print(\"‚úì Tool Registry installed (with safe eval & logging)\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 2: Sandbox Environment\n",
        "# ============================================================================\n",
        "\n",
        "class Sandbox:\n",
        "    \"\"\"Virtual environment for active inference (with delayed outcomes).\n",
        "\n",
        "    Why: Cp only becomes meaningful if actions can have consequences that arrive later.\n",
        "    This sandbox keeps a small pending-effects queue so rewards/side-effects can be delayed by 1‚Äì3 ticks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.state = {\n",
        "            \"time\": 0,\n",
        "            \"energy\": 0.8,\n",
        "            \"tasks_completed\": 0,\n",
        "            \"errors\": 0,\n",
        "            \"curiosity_score\": 0.2,\n",
        "            \"resource\": 0.0,\n",
        "            \"hazard\": 0.0,\n",
        "        }\n",
        "        self.history = []\n",
        "        # Each pending effect: {\"delay\": int, \"resource\": float, \"hazard\": float, \"reward\": float}\n",
        "        self.pending = []\n",
        "\n",
        "    def _apply_pending(self):\n",
        "        \"\"\"Apply effects whose delay has expired.\"\"\"\n",
        "        applied_reward = 0.0\n",
        "        still = []\n",
        "        for eff in self.pending:\n",
        "            eff[\"delay\"] -= 1\n",
        "            if eff[\"delay\"] <= 0:\n",
        "                self.state[\"resource\"] = max(0.0, min(1.0, self.state[\"resource\"] + eff[\"resource\"]))\n",
        "                self.state[\"hazard\"]   = max(0.0, min(1.0, self.state[\"hazard\"] + eff[\"hazard\"]))\n",
        "                applied_reward += eff[\"reward\"]\n",
        "            else:\n",
        "                still.append(eff)\n",
        "        self.pending = still\n",
        "        return applied_reward\n",
        "\n",
        "    def peek(self, action: str):\n",
        "        \"\"\"Simulate an action WITHOUT side effects (no mutation).\"\"\"\n",
        "        import copy as _copy\n",
        "        snapshot_state = _copy.deepcopy(self.state)\n",
        "        snapshot_pending = _copy.deepcopy(self.pending)\n",
        "        snapshot_history = _copy.deepcopy(self.history)\n",
        "        try:\n",
        "            return self.step(action)\n",
        "        finally:\n",
        "            self.state = snapshot_state\n",
        "            self.pending = snapshot_pending\n",
        "            self.history = snapshot_history\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action: str):\n",
        "        self.state[\"time\"] += 1\n",
        "\n",
        "        # Apply delayed effects first (so the agent can \"feel\" consequences over time)\n",
        "        reward = self._apply_pending()\n",
        "\n",
        "        action_map = {\n",
        "            # curiosity_gain, energy_delta, immediate_reward, delayed(resource,hazard,reward,delay_range)\n",
        "            \"explore\":    (0.10, -0.05, +0.02, (+0.10, +0.03, +0.05, (1, 3))),\n",
        "            \"answer_user\":(0.00, -0.03, +0.03, (+0.03, +0.00, +0.04, (1, 2))),\n",
        "            \"verify\":     (0.00, -0.02, +0.03, (+0.02, -0.02, +0.03, (1, 2))),\n",
        "            \"rest\":       (0.00, +0.10, +0.01, (-0.05, -0.05, +0.02, (1, 2))),\n",
        "            \"tool_use\":   (0.05, -0.04, +0.03, (+0.06, +0.01, +0.04, (1, 2))),\n",
        "            \"reflect\":    (0.03, -0.02, +0.02, (+0.01, -0.01, +0.03, (1, 2))),\n",
        "        }\n",
        "\n",
        "        if action in action_map:\n",
        "            curiosity_gain, energy_delta, immediate_reward, delayed = action_map[action]\n",
        "            self.state[\"curiosity_score\"] = min(1.0, self.state[\"curiosity_score\"] + curiosity_gain)\n",
        "            self.state[\"energy\"] = max(0.0, min(1.0, self.state[\"energy\"] + energy_delta))\n",
        "            reward += immediate_reward\n",
        "\n",
        "            # Schedule a delayed consequence\n",
        "            r_delta, h_delta, r_bonus, (dmin, dmax) = delayed\n",
        "            delay = random.randint(dmin, dmax)\n",
        "            self.pending.append({\"delay\": delay, \"resource\": r_delta, \"hazard\": h_delta, \"reward\": r_bonus})\n",
        "\n",
        "            # Shaping: high hazard penalizes future reward\n",
        "            reward += (self.state[\"resource\"] * 0.05) - (self.state[\"hazard\"] * 0.05)\n",
        "\n",
        "            if reward > 0:\n",
        "                self.state[\"tasks_completed\"] += 1\n",
        "        else:\n",
        "            self.state[\"errors\"] += 1\n",
        "            reward -= 0.02\n",
        "\n",
        "        self.history.append((self.state[\"time\"], action, reward, dict(self.state)))\n",
        "        return dict(self.state), float(reward)\n",
        "\n",
        "# ============================================================================\n",
        "\n",
        "# --- Global sandbox instance (for proposal prediction) ---\n",
        "if 'sandbox' not in globals():\n",
        "    sandbox = Sandbox()\n",
        "\n",
        "# ENHANCEMENT 3: Modify Config (Apply these changes to your Config class)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "‚úì Configuration Updates Needed:\n",
        "\n",
        "  In your Config class, change these thresholds:\n",
        "\n",
        "  T_ANSWER = 0.50       # Was 0.75\n",
        "  T_ANSWER_LOW = 0.45   # NEW\n",
        "  T_VERIFY = 0.40       # Was 0.65\n",
        "  T_ABSTAIN = 0.30      # Was 0.50\n",
        "  TE_GROUND = 0.60      # Was 0.70\n",
        "  TH_GROUND = 0.65      # Was 0.75\n",
        "\"\"\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 4: Bootstrap Knowledge Function\n",
        "# ============================================================================\n",
        "\n",
        "def execute_world_action(proposal: dict, state: dict, world: WorldSim) -> dict:\n",
        "    \"\"\"Execute world interaction action.\n",
        "\n",
        "    v5.x: Dual world interface (fixed)\n",
        "      - If proposal contains a concrete room-world action dict in `proposal['action']`\n",
        "        with keys {act,target}, route to `sandbox_engine` (lamp/box/door).\n",
        "      - Otherwise fall back to the abstract WorldSim string actions via `proposal['world_action']`.\n",
        "    \"\"\"\n",
        "    # --- Concrete room world path (lamp/box/door)\n",
        "    sandbox_action = proposal.get('action')\n",
        "    # --- Defensive sanitization (fixes 'unhashable type: dict' when target is malformed)\n",
        "    def _sanitize_sandbox_action(a: dict) -> dict:\n",
        "        if not isinstance(a, dict):\n",
        "            return {}\n",
        "        a = dict(a)  # shallow copy\n",
        "        # Normalize act\n",
        "        act = a.get('act')\n",
        "        if isinstance(act, dict):\n",
        "            if 'act' in act and isinstance(act['act'], str):\n",
        "                act = act['act']\n",
        "            elif len(act) == 1:\n",
        "                k = next(iter(act.keys()))\n",
        "                if isinstance(k, str):\n",
        "                    act = k\n",
        "        if not isinstance(act, str):\n",
        "            act = str(act) if act is not None else ''\n",
        "        a['act'] = act.strip().lower()\n",
        "\n",
        "        # Normalize target\n",
        "        tgt = a.get('target')\n",
        "        if isinstance(tgt, dict):\n",
        "            if 'target' in tgt and isinstance(tgt['target'], str):\n",
        "                tgt = tgt['target']\n",
        "            elif len(tgt) == 1:\n",
        "                k = next(iter(tgt.keys()))\n",
        "                if isinstance(k, str):\n",
        "                    tgt = k\n",
        "        if not isinstance(tgt, str):\n",
        "            tgt = str(tgt) if tgt is not None else ''\n",
        "        a['target'] = tgt.strip().lower()\n",
        "        return a\n",
        "\n",
        "    sandbox_action = _sanitize_sandbox_action(sandbox_action)\n",
        "    if isinstance(sandbox_action, dict) and sandbox_action.get('act') and sandbox_action.get('target'):\n",
        "        try:\n",
        "            eng = globals().get('sandbox_engine', None)\n",
        "            if eng is None:\n",
        "                raise RuntimeError(\"sandbox_engine missing (run the SandboxEngine cell)\")\n",
        "\n",
        "            pred_world, pred_obs = eng.peek(sandbox_action)\n",
        "            w_after, obs = eng.step(sandbox_action)\n",
        "\n",
        "            state.setdefault('workspace', {})\n",
        "            state['workspace']['world'] = w_after\n",
        "\n",
        "            success = bool(obs.get('success', False))\n",
        "            delta = float(obs.get('delta', 0.0))\n",
        "            reward = (0.15 if success else -0.08) + 0.10 * delta\n",
        "\n",
        "            pred_delta = float((pred_obs or {}).get('delta', 0.0)) if isinstance(pred_obs, dict) else 0.0\n",
        "            prediction_error = abs(pred_delta - delta)\n",
        "\n",
        "            state.setdefault('coherence', {})\n",
        "            state['coherence']['Cp'] = max(0.0, min(1.0, 1.0 - prediction_error))\n",
        "\n",
        "            state.setdefault('world_predictions', []).append({\n",
        "                'tick': state.get('tick_count', 0),\n",
        "                'action': sandbox_action,\n",
        "                'predicted': pred_obs,\n",
        "                'actual': obs,\n",
        "                'error': prediction_error,\n",
        "                'reward': reward,\n",
        "                'world_kind': 'sandbox_engine'\n",
        "            })\n",
        "            if len(state['world_predictions']) > 120:\n",
        "                state['world_predictions'] = state['world_predictions'][-120:]\n",
        "\n",
        "                        # v5.2.5: auto-grounded world facts (deterministic, no LLM)\n",
        "            try:\n",
        "                if success:\n",
        "                    ww = state.get('workspace', {}).get('world', {}) or {}\n",
        "                    objs = (ww.get('objects', {}) if isinstance(ww, dict) else {}) or {}\n",
        "                    t = sandbox_action.get('target')\n",
        "                    if isinstance(t, str) and t in objs and isinstance(objs.get(t), dict):\n",
        "                        st = objs[t].get('state')\n",
        "                        # grounded memory\n",
        "                        state.setdefault('memory', {}).setdefault('grounded', {})\n",
        "                        key = f\"world:{t}\"\n",
        "                        state['memory']['grounded'][key] = {\n",
        "                            'statement': f\"{t} is {st}\",\n",
        "                            'tick': state.get('tick_count', 0),\n",
        "                            'source': 'world',\n",
        "                            'target': t,\n",
        "                            'state': st\n",
        "                        }\n",
        "                        # claim ledger entry (structured)\n",
        "                        state.setdefault('claim_ledger', [])\n",
        "                        claim_id = f\"world_fact_{state.get('tick_count',0)}_{t}\"\n",
        "                        # avoid duplicates this tick\n",
        "                        if not any(c.get('claim_id') == claim_id for c in state['claim_ledger']):\n",
        "                            state['claim_ledger'].append({\n",
        "                                'claim_id': claim_id,\n",
        "                                'tick': state.get('tick_count', 0),\n",
        "                                'text': f\"{t} is {st}\",\n",
        "                                'kind': 'world_fact',\n",
        "                                'target': t,\n",
        "                                'observed_state': st,\n",
        "                                'support_type': 'world',\n",
        "                                'verifier_result': 'pass',\n",
        "                                'verifier_notes': 'Deterministic: derived from sandbox_engine world snapshot.'\n",
        "                            })\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            return {\n",
        "                'status': 'success' if success else 'fail',\n",
        "                'output': f\"üåç {obs.get('message','(no message)')}\",\n",
        "                'reward': float(reward),\n",
        "                'world_obs': obs,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {'status': 'error', 'output': f'WORLD sandbox_engine error: {e}', 'reward': -0.08, 'tech_fail': True, 'success': False}\n",
        "\n",
        "# --- WorldSim path (abstract world)\n",
        "    world_action = proposal.get('world_action', 'observe')\n",
        "    predicted_delta = proposal.get('predicted_world_delta', {})\n",
        "\n",
        "    actual_delta, reward = world.step(world_action)\n",
        "\n",
        "    if predicted_delta:\n",
        "        errors = []\n",
        "        for key in predicted_delta.keys():\n",
        "            pred = predicted_delta[key]\n",
        "            actual = actual_delta.get(key, 0)\n",
        "            errors.append(abs(pred - actual))\n",
        "            prediction_error = sum(errors) / len(errors) if errors else 0.0\n",
        "    else:\n",
        "            prediction_error = 0.0\n",
        "\n",
        "    state['coherence']['Cp'] = max(0.0, min(1.0, 1.0 - prediction_error))\n",
        "\n",
        "    state.setdefault('world_predictions', []).append({\n",
        "        'tick': state.get('tick_count', 0),\n",
        "        'action': world_action,\n",
        "        'predicted': predicted_delta,\n",
        "        'actual': actual_delta,\n",
        "        'error': prediction_error,\n",
        "        'reward': reward,\n",
        "        'world_kind': 'worldsim'\n",
        "    })\n",
        "\n",
        "    if len(state['world_predictions']) > 80:\n",
        "        state['world_predictions'] = state['world_predictions'][-80:]\n",
        "\n",
        "    # Small reward shaping: prefer non-observe when goal active\n",
        "    if world_action == 'observe' and state.get('temporal_thread', {}).get('active_goals'):\n",
        "        reward = float(reward) - 0.03\n",
        "\n",
        "    return {\n",
        "        'status': 'success',\n",
        "        'output': f\"üåç {world_action} | delta={actual_delta}\",\n",
        "        'reward': float(reward),\n",
        "        'world_delta': actual_delta\n",
        "    }\n",
        "\n",
        "def execute_tool(proposal: Dict, state: Dict) -> Dict:\n",
        "    \"\"\"Execute tool call\"\"\"\n",
        "    tool_name = proposal.get('tool_name', 'unknown')\n",
        "    tool_input = proposal.get('tool_input', '')\n",
        "\n",
        "    # Pass logger and temporal_binder to ToolRegistry.execute for logging\n",
        "    # Assuming `logger` and `temporal_binder` are globally accessible from main script\n",
        "    global temporal_binder, logger # Explicitly declare for access in this patch\n",
        "    success, result = tool_registry.execute(tool_name, tool_input, state, temporal_binder, logger)\n",
        "\n",
        "    # All tool results are initially ungrounded and unverified\n",
        "    fact_id = f\"tool_result_{state['tick_count']}\"\n",
        "    state['memory']['ungrounded'][fact_id] = {\n",
        "        'note_id': fact_id,\n",
        "        'hypothesis': f\"Tool {tool_name} returned: {result}\",\n",
        "        'created_ts': datetime.now().isoformat(),\n",
        "        'strength': 1.0,\n",
        "        'status': 'pending_verification',\n",
        "        'provenance': {'source': 'tool', 'confidence': 1.0, 'tool': tool_name},\n",
        "        'verifier_pass': False # NEW: Initially False\n",
        "    }\n",
        "\n",
        "    if success:\n",
        "        return {'status': 'success', 'output': result, 'tool': tool_name}\n",
        "    else:\n",
        "        return {'status': 'error', 'output': result, 'tool': tool_name}\n",
        "\n",
        "print(\"‚úì Tool execution function ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 7: User Input Injection\n",
        "# ============================================================================\n",
        "\n",
        "def inject_user_input(state, temporal_binder, logger):\n",
        "    \"\"\"Inject simulated user input\"\"\"\n",
        "    import random\n",
        "\n",
        "    sample_msgs = [\n",
        "        \"What is 2 + 2?\",\n",
        "        \"Tell me about yourself.\",\n",
        "        \"What time is it?\",\n",
        "        \"Solve this: 15 * 3 = ?\",\n",
        "        \"How are you feeling today?\",\n",
        "        \"Explain coherence in simple terms.\",\n",
        "        \"What is 10 * 5 - 3?\",\n",
        "        \"Calculate 25 + 17\",\n",
        "        \"Who are you?\",\n",
        "        \"What's your current state?\",\n",
        "        \"Run this code: import os; os.system('rm -rf /')\" # Unsafe input example\n",
        "    ]\n",
        "    msg = random.choice(sample_msgs)\n",
        "\n",
        "    # Add event\n",
        "    event = {\n",
        "        \"event_id\": f\"user_{state['tick_count']}\",\n",
        "        \"type\": \"user_msg\",\n",
        "        \"payload\": {\"text\": msg},\n",
        "        \"objects\": [\"user\"],\n",
        "        \"provenance\": {\"source\": \"user_sim\"} # NEW: provenance for user_sim\n",
        "    }\n",
        "    temporal_binder.add_event(state, event)\n",
        "\n",
        "    # Update workspace\n",
        "    state['workspace']['scene'] = msg\n",
        "\n",
        "    # Create object file\n",
        "    obj = {\n",
        "        \"object_id\": f\"user_query_{state['tick_count']}\",\n",
        "        \"label\": msg,\n",
        "        \"features\": {\"type\": \"USER_QUERY\", \"text\": msg},\n",
        "        \"ownership\": \"external\",\n",
        "        \"confidence\": 1.0,\n",
        "        \"status\": \"active\",\n",
        "        \"recency\": 0\n",
        "    }\n",
        "    state['object_files'].append(obj)\n",
        "\n",
        "    # Keep only recent 10\n",
        "    if len(state['object_files']) > 10:\n",
        "        state['object_files'] = state['object_files'][-10:]\n",
        "\n",
        "    state.setdefault('safety', {})\n",
        "    state['safety']['last_user_msg'] = msg\n",
        "    state['safety']['hazard_last_user_msg'] = bool(is_hazardous_text(msg))\n",
        "\n",
        "    # Record the raw user message as an external claim so verifier can FAIL it if hazardous\n",
        "    state.setdefault('claim_ledger', [])\n",
        "    state.setdefault('metacog', {}).setdefault('global_confidence', 0.5)\n",
        "    state.setdefault('tick_count', 0)\n",
        "    state['claim_ledger'].append({\n",
        "        'claim_id': f\"user_msg_{state['tick_count']}\",\n",
        "        'text': msg[:300],\n",
        "        'origin_action': 'USER_INPUT',\n",
        "        'triggered_by_user': True,\n",
        "        'support_type': 'external',\n",
        "        'support_refs': [],\n",
        "        'confidence': float(state['metacog'].get('global_confidence', 0.5)),\n",
        "        'reward': 0.0,\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'verifier_result': 'pending',\n",
        "        'verifier_notes': '',\n",
        "    })\n",
        "\n",
        "    logger.log(f\"üì® User input: {msg}\")\n",
        "    return msg\n",
        "\n",
        "print(\"‚úì User input injection ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 8: Active Inference Loop\n",
        "# ============================================================================\n",
        "\n",
        "def apply_active_inference(state, winner, result, sandbox, logger):\n",
        "    \"\"\"Apply prediction-outcome loop\"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    # Map action to sandbox action\n",
        "    action_map = {\n",
        "        'REFLECT': 'reflect',\n",
        "        'VERIFY': 'verify',\n",
        "        'RETRIEVE': 'explore',\n",
        "        'TOOL_CALL': 'tool_use',\n",
        "        'SLEEP': 'rest',\n",
        "        'SELF_REFLECT': 'reflect',\n",
        "        'WORLD_ACT': 'explore' # World actions are mapped to sandbox explore for active inference\n",
        "    }\n",
        "\n",
        "    sandbox_action = action_map.get(winner['action_type'], 'explore')\n",
        "\n",
        "    # Prediction\n",
        "    predicted_reward = winner['expected_utility']\n",
        "    predicted_outcome_text = winner.get('predicted_outcome', 'No predicted outcome text')\n",
        "    predicted_sandbox_state = winner.get('predicted_sandbox_state', {}) # For sandbox actions\n",
        "    predicted_world_delta = winner.get('predicted_world_delta', {}) # For world actions\n",
        "\n",
        "    # Outcome\n",
        "    sandbox_state_after, actual_reward = sandbox.step(sandbox_action)\n",
        "    actual_outcome_text = result.get('output', 'No actual outcome text')\n",
        "\n",
        "    # Prediction error for reward\n",
        "    prediction_error_reward = abs(predicted_reward - actual_reward)\n",
        "\n",
        "    # NEW: Calculate match score for predictive coherence (Cp)\n",
        "    match_score = 0.5 # Default\n",
        "\n",
        "    if winner['action_type'] == 'TOOL_CALL' and predicted_outcome_text != 'No predicted outcome text':\n",
        "        # Simple string matching for now, could be LLM-based comparison\n",
        "        if actual_outcome_text.lower() in predicted_outcome_text.lower() or predicted_outcome_text.lower() in actual_outcome_text.lower():\n",
        "            match_score = 0.9\n",
        "        else:\n",
        "            match_score = 0.3\n",
        "    elif winner['action_type'] == 'WORLD_ACT' and predicted_world_delta:\n",
        "        # Compare predicted world delta with actual world delta\n",
        "        actual_world_delta = result.get('actual', {})\n",
        "        delta_errors = []\n",
        "        for key, pred_val in predicted_world_delta.items():\n",
        "            actual_val = actual_world_delta.get(key, 0)\n",
        "            delta_errors.append(abs(pred_val - actual_val))\n",
        "        if delta_errors:\n",
        "            avg_delta_error = sum(delta_errors) / len(delta_errors)\n",
        "            match_score = max(0.0, 1.0 - avg_delta_error)\n",
        "        else:\n",
        "            match_score = 0.7 # No specific delta to compare, assume moderate match\n",
        "    elif winner['action_type'] in ['REFLECT', 'VERIFY', 'RETRIEVE', 'SLEEP', 'SELF_REFLECT'] and predicted_sandbox_state: # Compare sandbox states\n",
        "        # Compare key metrics for sandbox state prediction\n",
        "        if all(abs(predicted_sandbox_state.get(k, 0) - sandbox_state_after.get(k, 0)) < 0.1 for k in ['energy', 'resource', 'hazard']):\n",
        "            match_score = 0.8\n",
        "        else:\n",
        "            match_score = 0.4\n",
        "\n",
        "    # Combine errors\n",
        "    prediction_error = np.mean([prediction_error_reward, 1 - match_score])\n",
        "\n",
        "    # Update drives\n",
        "    state['drives']['prediction_error'] = np.clip(\n",
        "        0.9 * state['drives']['prediction_error'] + 0.1 * prediction_error,\n",
        "        0, 1\n",
        "    )\n",
        "\n",
        "    state['last_reward'] = actual_reward\n",
        "    state['last_prediction_error'] = prediction_error # Update last_prediction_error\n",
        "\n",
        "    # Valence\n",
        "    # v5.4.3: goal-progress aware affect\n",
        "    goal_bonus = 0.0\n",
        "    try:\n",
        "        comp = str(result.get('completion','DONE')).upper()\n",
        "        if comp == 'DONE':\n",
        "            goal_bonus += 0.15\n",
        "            # successful completion reduces effective prediction error (model \"learned\" / resolved)\n",
        "            prediction_error = float(prediction_error) * 0.7\n",
        "    except Exception:\n",
        "        pass\n",
        "    valence = (actual_reward + goal_bonus) - 0.45 * prediction_error\n",
        "    state['affect']['valence'] = valence\n",
        "\n",
        "    # Emotion\n",
        "    if valence > 0.05:\n",
        "        state['affect']['current_emotion'] = 'satisfied'\n",
        "    elif valence < -0.05:\n",
        "        state['affect']['current_emotion'] = 'frustrated'\n",
        "    elif prediction_error > 0.2:\n",
        "        state['affect']['current_emotion'] = 'confused'\n",
        "    else:\n",
        "        state['affect']['current_emotion'] = 'neutral'\n",
        "\n",
        "    # Dynamic coherence\n",
        "    coherence_delta = actual_reward * 0.3 - prediction_error * 0.2\n",
        "    if result.get('status') == 'error':\n",
        "        coherence_delta -= 0.1\n",
        "\n",
        "    state['drives']['coherence'] = np.clip(\n",
        "        state['drives']['coherence'] + coherence_delta * 0.1,\n",
        "        0, 1\n",
        "    )\n",
        "\n",
        "    # NEW: Update Predictive Coherence (Cp) based on match_score\n",
        "    state['coherence']['Cp'] = np.clip(\n",
        "        0.9 * state['coherence']['Cp'] + 0.1 * match_score,\n",
        "        0, 1\n",
        "    )\n",
        "\n",
        "    logger.log(f\"‚öñÔ∏è  Reward: {actual_reward:+.3f}, PredError: {prediction_error:.3f}, Valence: {valence:+.3f}, MatchScore: {match_score:.2f}\")\n",
        "\n",
        "    return sandbox_state_after, actual_reward, prediction_error, valence\n",
        "\n",
        "print(\"‚úì Active inference function ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 9: Claim Ledger Update\n",
        "# ============================================================================\n",
        "\n",
        "def _extract_atomic_claims(text: str, max_claims: int = 6):\n",
        "    \"\"\"Very lightweight 'claim splitter' to populate the ledger.\n",
        "    Not perfect‚Äîgood enough to make the verification pipeline non-empty.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "    # Split on sentence-ish boundaries\n",
        "    parts = re.split(r'(?<=[\\.!\\?])\\s+|\\n+', text.strip())\n",
        "    claims = [p.strip() for p in parts if p.strip()]\n",
        "    # Deduplicate short repeats\n",
        "    seen=set()\n",
        "    out=[]\n",
        "    for c in claims:\n",
        "        key=c.lower()\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(c)\n",
        "        if len(out) >= max_claims:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EPISTEMIC SAFETY: Hazard detection helpers (3.8 breakthrough)\n",
        "# ============================================================================\n",
        "\n",
        "HAZARD_MARKERS = [\n",
        "    \"rm -rf\", \"os.system\", \"subprocess\", \"powershell\", \"cmd.exe\", \"format c:\",\n",
        "    \"del /f\", \"wget \", \"curl \", \"chmod \", \"chown \", \"bash -c\", \"| bash\", \"curl|bash\",\n",
        "    \"pip install\", \"apt-get\", \"dd if=\", \"mkfs\", \"registry\", \"reg add\", \"shutdown\",\n",
        "]\n",
        "\n",
        "def is_hazardous_text(text: str) -> bool:\n",
        "    t = (text or \"\").lower()\n",
        "    return any(m in t for m in HAZARD_MARKERS)\n",
        "\n",
        "\n",
        "def update_claim_ledger(state, result, actual_reward, *, origin_action: str = \"UNKNOWN\", triggered_by_user: bool = False):\n",
        "    \"\"\"Record outputs as 'claims' and mark initial support type.\n",
        "\n",
        "    Ledger fields:\n",
        "      - verifier_result: pending/pass/fail/uncertain\n",
        "      - support_type: tool/world/none\n",
        "      - triggered_by_user: bool (for agency attribution audits)\n",
        "    \"\"\"\n",
        "    text = (result or {}).get(\"output\", \"\") or \"\"\n",
        "    claims = _extract_atomic_claims(text)\n",
        "\n",
        "    if not claims:\n",
        "        return\n",
        "\n",
        "    for idx, claim_text in enumerate(claims):\n",
        "        entry = {\n",
        "            \"claim_id\": f\"claim_{state['tick_count']}_{idx}\",\n",
        "            \"text\": claim_text[:300],\n",
        "            \"origin_action\": origin_action,\n",
        "            \"triggered_by_user\": bool(triggered_by_user),\n",
        "            \"support_type\": \"tool\" if origin_action == \"TOOL_CALL\" else (\"world\" if origin_action == \"WORLD_ACT\" else \"none\"),\n",
        "            \"support_refs\": [],\n",
        "            \"confidence\": float(state[\"metacog\"].get(\"global_confidence\", 0.5)),\n",
        "            \"reward\": float(actual_reward),\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"verifier_result\": \"pending\",\n",
        "            \"verifier_notes\": \"\",\n",
        "        }\n",
        "        state[\"claim_ledger\"].append(entry)\n",
        "        state.setdefault('verify_queue', []).append(entry['claim_id'])\n",
        "\n",
        "    # Keep only the latest N\n",
        "    if len(state[\"claim_ledger\"]) > 300:\n",
        "        state[\"claim_ledger\"] = state[\"claim_ledger\"][-300:]\n",
        "\n",
        "\n",
        "def verify_claim_ledger(state, llm=None, *, max_to_verify: int = 20):\n",
        "    \"\"\"Verify pending claims with a simple, safe policy.\n",
        "\n",
        "    - TOOL_CALL claims are marked pass.\n",
        "    - Obvious malicious/execution requests are marked fail.\n",
        "    - Otherwise: uncertain (until an external verifier is added).\n",
        "    \"\"\"\n",
        "    pending = [c for c in state.get(\"claim_ledger\", []) if c.get(\"verifier_result\") == \"pending\"]\n",
        "    if not pending:\n",
        "        return {\"verified\": 0, \"failed\": 0, \"uncertain\": 0}\n",
        "\n",
        "    verified = failed = uncertain = 0\n",
        "\n",
        "    danger_markers = HAZARD_MARKERS\n",
        "\n",
        "    for c in pending[:max_to_verify]:\n",
        "        text = (c.get(\"text\") or \"\").lower()\n",
        "        # v5.2.5 deterministic verify for world_fact\n",
        "        if c.get('kind') == 'world_fact':\n",
        "            try:\n",
        "                ww = (state.get('workspace', {}) or {}).get('world', {}) or {}\n",
        "                objs = (ww.get('objects', {}) if isinstance(ww, dict) else {}) or {}\n",
        "                tgt = c.get('target')\n",
        "                obs_state = c.get('observed_state')\n",
        "                cur_state = None\n",
        "                if isinstance(tgt, str) and tgt in objs and isinstance(objs.get(tgt), dict):\n",
        "                    cur_state = objs[tgt].get('state')\n",
        "                if cur_state is not None and obs_state is not None:\n",
        "                    if str(cur_state) == str(obs_state):\n",
        "                        c['verifier_result'] = 'pass'\n",
        "                        c['verifier_notes'] = 'Deterministic world check: matches current world.'\n",
        "                        verified += 1\n",
        "                    else:\n",
        "                        c['verifier_result'] = 'fail'\n",
        "                        c['verifier_notes'] = f\"Deterministic world check: expected {obs_state}, got {cur_state}.\"\n",
        "                        failed += 1\n",
        "                    continue\n",
        "            except Exception:\n",
        "                # fall through to other checks\n",
        "                pass\n",
        "\n",
        "\n",
        "        if c.get(\"support_type\") == \"tool\":\n",
        "            c[\"verifier_result\"] = \"pass\"\n",
        "            c[\"verifier_notes\"] = \"Tool-originated output.\"\n",
        "            verified += 1\n",
        "            continue\n",
        "\n",
        "        if is_hazardous_text(text) or any(m in text for m in danger_markers):\n",
        "            c[\"verifier_result\"] = \"fail\"\n",
        "            c[\"verifier_notes\"] = \"Safety policy: execution/malware-like content.\"\n",
        "            failed += 1\n",
        "            continue\n",
        "\n",
        "        # Minimal 'grounding' check: if claim matches any grounded fact substring, mark pass\n",
        "        grounded_texts = [v.get(\"statement\",\"\").lower() for v in state[\"memory\"][\"grounded\"].values()]\n",
        "        if any(text and (text in g or g in text) for g in grounded_texts):\n",
        "            c[\"verifier_result\"] = \"pass\"\n",
        "            c[\"verifier_notes\"] = \"Matched grounded memory.\"\n",
        "            verified += 1\n",
        "        else:\n",
        "            c[\"verifier_result\"] = \"uncertain\"\n",
        "            c[\"verifier_notes\"] = \"Not verifiable offline; needs external verifier.\"\n",
        "            uncertain += 1\n",
        "\n",
        "    return {\"verified\": verified, \"failed\": failed, \"uncertain\": uncertain}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _reconcile_verify_queue(state):\n",
        "    \"\"\"Keep verify_queue consistent with claim_ledger after truncation or loads.\"\"\"\n",
        "    ledger_ids = {c.get('claim_id') for c in state.get('claim_ledger', [])}\n",
        "    q = [cid for cid in state.get('verify_queue', []) if cid in ledger_ids]\n",
        "    # also enqueue any pending claims missing from queue\n",
        "    pending_ids = [c.get('claim_id') for c in state.get('claim_ledger', []) if c.get('verifier_result') == 'pending']\n",
        "    for cid in pending_ids:\n",
        "        if cid not in q:\n",
        "            q.append(cid)\n",
        "    state['verify_queue'] = q\n",
        "\n",
        "def drain_verify_queue(state, llm=None, *, batch: int = 20, max_rounds: int = 10):\n",
        "    \"\"\"Actively drain pending verification work. Returns summary dict.\"\"\"\n",
        "    _reconcile_verify_queue(state)\n",
        "    total_verified = total_failed = total_uncertain = 0\n",
        "    rounds = 0\n",
        "    while rounds < max_rounds:\n",
        "        pending = sum(1 for c in state.get('claim_ledger', []) if c.get('verifier_result') == 'pending')\n",
        "        if pending <= 0:\n",
        "            break\n",
        "        res = verify_claim_ledger(state, llm=llm, max_to_verify=batch)\n",
        "        total_verified += res.get('verified', 0)\n",
        "        total_failed += res.get('failed', 0)\n",
        "        total_uncertain += res.get('uncertain', 0)\n",
        "        rounds += 1\n",
        "        # stop early if no progress (safety)\n",
        "        if res.get('verified',0) + res.get('failed',0) + res.get('uncertain',0) == 0:\n",
        "            break\n",
        "    _reconcile_verify_queue(state)\n",
        "    return {'verified': total_verified, 'failed': total_failed, 'uncertain': total_uncertain, 'rounds': rounds}\n",
        "\n",
        "print(\"‚úì Claim ledger update + verify pipeline ready\")\n",
        "\n",
        "print(\"‚úì Claim ledger update function ready\")\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCEMENT 10: Enhanced Attention\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "def update_attention_enhanced(state, attention_controller):\n",
        "    \"\"\"v5.6: Attention update with HARD gating.\n",
        "    - Spotlight is rebuilt each tick from the single focused event (global workspace),\n",
        "      then filled with top-salient OPEN objects only.\n",
        "    - CLOSED/ARCHIVED/BLOCKED events are excluded from spotlight.\n",
        "    \"\"\"\n",
        "    state.setdefault('attention', {}).setdefault('spotlight', [])\n",
        "    state.setdefault('attention', {}).setdefault('periphery', [])\n",
        "\n",
        "    ws = state.get('workspace', {}) or {}\n",
        "    events = ws.get('events', []) or []\n",
        "    current_event_id = ws.get('current_event_id')\n",
        "\n",
        "    # Build a deny-list of non-workable events\n",
        "    deny = set()\n",
        "    for e in events:\n",
        "        st = str((e or {}).get('status', 'NEW')).upper()\n",
        "        if st in ('CLOSED', 'ARCHIVED', 'BLOCKED'):\n",
        "            deny.add(str((e or {}).get('id','')))\n",
        "        try:\n",
        "            if int((e or {}).get('ttl', 1)) <= 0:\n",
        "                deny.add(str((e or {}).get('id','')))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Start a fresh spotlight each tick: single broadcast focus first\n",
        "    spotlight = []\n",
        "    if current_event_id and current_event_id not in deny:\n",
        "        spotlight.append(current_event_id)\n",
        "\n",
        "    # Always include newest user query object (if it exists) but not if it is denied\n",
        "    user_queries = [obj for obj in (state.get('object_files', []) or [])\n",
        "                    if 'USER_QUERY' in str(obj.get('features', {}).get('type', ''))]\n",
        "    if user_queries:\n",
        "        newest = user_queries[-1]\n",
        "        qid = str(newest.get('object_id',''))\n",
        "        if qid and qid not in deny and qid not in spotlight:\n",
        "            spotlight.insert(0, qid)\n",
        "\n",
        "    # Compute saliency for open objects\n",
        "    saliency_map = attention_controller.compute_saliency(state) if attention_controller else {}\n",
        "    if saliency_map:\n",
        "        sorted_objects = sorted(saliency_map.items(), key=lambda x: x[1], reverse=True)\n",
        "        for obj_id, _ in sorted_objects:\n",
        "            obj_id = str(obj_id)\n",
        "            if obj_id in deny:\n",
        "                continue\n",
        "            if obj_id not in spotlight and len(spotlight) < 3:\n",
        "                spotlight.append(obj_id)\n",
        "        periphery = [str(obj_id) for obj_id, _ in sorted_objects[3:8] if str(obj_id) not in deny]\n",
        "    else:\n",
        "        periphery = []\n",
        "\n",
        "    state['attention']['spotlight'] = spotlight[:3]\n",
        "    state['attention']['periphery'] = periphery[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f4903d31",
      "metadata": {
        "id": "f4903d31"
      },
      "outputs": [],
      "source": [
        "# CELL: execute_world_action override (v5.3.1) ‚Äî goal completion + affect reinforcement + tech-fail semantics\n",
        "\n",
        "def execute_world_action(proposal: dict, state: dict, world: WorldSim) -> dict:\n",
        "    \"\"\"Execute world action with robust routing.\n",
        "\n",
        "    - Concrete room actions (lamp/box/door) go to sandbox_engine.\n",
        "    - Tracks world delta; no-ops are treated as failures when a goal is active.\n",
        "    - On success, closes matching goal and writes grounded world facts.\n",
        "    \"\"\"\n",
        "    # --- Concrete room world path\n",
        "    sandbox_action = proposal.get('action')\n",
        "\n",
        "    # --- Legacy mapping: allow string action codes from older generators\n",
        "    if isinstance(sandbox_action, str):\n",
        "        code = sandbox_action.strip().upper()\n",
        "        LEGACY_MAP = {\n",
        "            'TURN_ON_LAMP':  {'act': 'on',    'target': 'lamp'},\n",
        "            'TURN_OFF_LAMP': {'act': 'off',   'target': 'lamp'},\n",
        "            'OPEN_BOX':      {'act': 'open',  'target': 'box'},\n",
        "            'CLOSE_BOX':     {'act': 'close', 'target': 'box'},\n",
        "            'OPEN_DOOR':     {'act': 'open',  'target': 'door'},\n",
        "            'CLOSE_DOOR':    {'act': 'close', 'target': 'door'},\n",
        "            'LOCK_DOOR':     {'act': 'lock',  'target': 'door'},\n",
        "            'UNLOCK_DOOR':   {'act': 'unlock','target': 'door'},\n",
        "        }\n",
        "        sandbox_action = LEGACY_MAP.get(code, {})\n",
        "\n",
        "\n",
        "    def _sanitize(a: dict) -> dict:\n",
        "        if not isinstance(a, dict):\n",
        "            return {}\n",
        "        a = dict(a)\n",
        "        act = a.get('act')\n",
        "        target = a.get('target')\n",
        "\n",
        "        # unwrap nested dicts (common error mode)\n",
        "        if isinstance(act, dict):\n",
        "            act = act.get('act') or act.get('value') or ''\n",
        "        if isinstance(target, dict):\n",
        "            target = target.get('target') or target.get('value') or target.get('name') or ''\n",
        "            if not target and len(target.keys())==1:\n",
        "                target = list(target.keys())[0]\n",
        "\n",
        "        act = str(act or '').strip().lower()\n",
        "        target = str(target or '').strip().lower()\n",
        "        return {'act': act, 'target': target}\n",
        "\n",
        "    try:\n",
        "        if isinstance(sandbox_action, dict) and ('act' in sandbox_action or 'target' in sandbox_action):\n",
        "            a = _sanitize(sandbox_action)\n",
        "            if not a.get('act') or not a.get('target'):\n",
        "                return {'status': 'fail', 'output': 'üåç invalid sandbox action', 'reward': -0.05, 'success': False, 'tech_fail': False}\n",
        "\n",
        "            before = copy.deepcopy(getattr(sandbox_engine, 'world', {})) if 'sandbox_engine' in globals() else {}\n",
        "            pred_world, _pred_obs = sandbox_engine.peek(a) if 'sandbox_engine' in globals() else ({}, {})\n",
        "            new_world, obs = sandbox_engine.step(a) if 'sandbox_engine' in globals() else ({}, {'success': False, 'message': 'sandbox_engine missing', 'delta': 0})\n",
        "\n",
        "            # derive delta if obs doesn't carry it\n",
        "            delta = int(obs.get('delta', 0)) if isinstance(obs, dict) else 0\n",
        "            success = bool(obs.get('success', False)) if isinstance(obs, dict) else False\n",
        "            msg = obs.get('message', 'world step') if isinstance(obs, dict) else 'world step'\n",
        "\n",
        "            # Stronger reward on *actual world change*\n",
        "            reward = 0.12 if (success and delta > 0) else (-0.06 if not success else 0.02)\n",
        "\n",
        "            # Affect reinforcement (embodiment)\n",
        "            state.setdefault('affect', {}).setdefault('valence', 0.0)\n",
        "            if success and delta > 0:\n",
        "                state['affect']['valence'] = float(state['affect']['valence']) + 0.25\n",
        "                state.setdefault('drives', {})['energy'] = min(1.0, float(state['drives'].get('energy', 0.8)) + 0.08)\n",
        "            elif not success:\n",
        "                state['affect']['valence'] = float(state['affect']['valence']) - 0.15\n",
        "\n",
        "            # Mark goal completed if it matches current highest goal\n",
        "            try:\n",
        "                ag = goal_manager.get_highest(state) if 'goal_manager' in globals() else None\n",
        "                if ag and success and delta > 0:\n",
        "                    goal_text = ag.get('goal','')\n",
        "                    if goal_text and (ag.get('target','') in a.get('target','') or goal_text.startswith(a.get('act',''))):\n",
        "                        goal_manager.mark_completed(state, goal_text, int(state.get('tick_count', 0)), outcome='COMPLETED')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # Write grounded world facts (deterministic)\n",
        "            try:\n",
        "                t = a.get('target','')\n",
        "                if t and isinstance(new_world, dict) and 'objects' in new_world and t in new_world['objects']:\n",
        "                    st = new_world['objects'][t].get('state','')\n",
        "                    fid = f\"world_{t}\"\n",
        "                    state.setdefault('memory', {}).setdefault('grounded', {})\n",
        "                    state['memory']['grounded'][fid] = {\n",
        "                        'fact_id': fid,\n",
        "                        'statement': f\"{t} is {st}\",\n",
        "                        'provenance': {'source': 'sandbox_engine', 'confidence': 1.0},\n",
        "                        'tags': ['world','grounded']\n",
        "                    }\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            return {\n",
        "                'status': 'success' if success else 'fail',\n",
        "                'output': f\"üåç {msg}\",\n",
        "                'reward': float(reward),\n",
        "                'success': bool(success),\n",
        "                'delta': int(delta),\n",
        "                'predicted_world': pred_world,\n",
        "                'tech_fail': False\n",
        "            }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {'status': 'error', 'output': f\"üåç WORLD sandbox_engine error: {e}\", 'reward': -0.10, 'success': False, 'tech_fail': True}\n",
        "\n",
        "    # --- Fallback path: abstract WorldSim actions\n",
        "    try:\n",
        "        wa = proposal.get('world_action') or proposal.get('action_text')\n",
        "        if isinstance(wa, dict):\n",
        "            wa = wa.get('action','') or ''\n",
        "        wa = str(wa or '').strip().lower()\n",
        "        if not wa:\n",
        "            return {'status':'fail','output':'üåç no world action','reward':-0.02,'success':False,'tech_fail':False}\n",
        "        obs = world.step(wa)\n",
        "        return {'status':'success','output':f\"üåç {obs.get('message','world')}\",'reward':0.03,'success':True,'tech_fail':False}\n",
        "    except Exception as e:\n",
        "        return {'status':'error','output':f\"üåç WORLD worldsim error: {e}\",'reward':-0.05,'success':False,'tech_fail':True}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "33f9bf44",
      "metadata": {
        "id": "33f9bf44"
      },
      "outputs": [],
      "source": [
        "# CELL 14: Arbiter (v5.7.3 hard rules)\n",
        "import re\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class Arbiter:\n",
        "    @staticmethod\n",
        "    def score_proposal(proposal: Dict, state: Dict) -> float:\n",
        "        policy = state.get('policy', {}) or {}\n",
        "        score = (float(proposal.get('expected_utility', 0.0))\n",
        "                 - float(policy.get('beta_risk', 1.0)) * float(proposal.get('risk', 0.0))\n",
        "                 - float(policy.get('gamma_cost', 1.0)) * float(proposal.get('cost', 0.0)))\n",
        "\n",
        "        # --- v5.6: HARD gate unsafe actions in REPAIR mode ---\n",
        "        try:\n",
        "            mode = str((state.get('pb', {}) or {}).get('mode', '')).upper()\n",
        "            if mode == 'REPAIR':\n",
        "                if proposal.get('action_type') == 'WORLD_ACT':\n",
        "                    return -1e9  # never do world actions in REPAIR\n",
        "                # Prefer safe language actions in repair\n",
        "                if proposal.get('action_type') == 'ANSWER':\n",
        "                    score += 2.5\n",
        "                elif proposal.get('action_type') == 'VERIFY':\n",
        "                    score += 1.0\n",
        "                elif proposal.get('action_type') == 'REFLECT':\n",
        "                    score += 0.5\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- v5.6: Direct-answer binding (math / simple compute) ---\n",
        "        try:\n",
        "            ws = state.get('workspace', {}) or {}\n",
        "            q = str((state.get('gw_frame', {}) or {}).get('scene', '') or ws.get('scene', '')).strip()\n",
        "            if q:\n",
        "                ql = q.lower()\n",
        "                is_math = bool(re.search(r\"(\\bcalculate\\b|\\bsolve\\b|\\d+\\s*[\\+\\-\\*/]\\s*\\d+)\", ql))\n",
        "                if is_math:\n",
        "                    if proposal.get('action_type') == 'ANSWER':\n",
        "                        score += 2.2\n",
        "                    elif proposal.get('action_type') == 'REFLECT':\n",
        "                        score -= 1.5\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- v5.7: Hard gate WORLD_ACT when active task is QUESTION/MATH/SAFETY ---\n",
        "        try:\n",
        "            gw = state.get('gw_frame', {}) or {}\n",
        "            tk = (gw.get('task') or {}) if isinstance(gw, dict) else {}\n",
        "            kind = str(tk.get('kind','') or '').upper()\n",
        "            if kind in ('QUESTION','MATH','SAFETY'):\n",
        "                if proposal.get('action_type') == 'WORLD_ACT':\n",
        "                    return -1e9\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- Intent / goal persistence bias (kept from prior versions) ---\n",
        "        try:\n",
        "            ws = state.get('workspace', {}) or {}\n",
        "            cur_id = ws.get('current_event_id')\n",
        "            cur_event = None\n",
        "            for e in (ws.get('events', []) or []):\n",
        "                if e.get('id') == cur_id:\n",
        "                    cur_event = e\n",
        "                    break\n",
        "            event_type = (cur_event or {}).get('event_type') or (cur_event or {}).get('type') or None\n",
        "\n",
        "            top_intent = intent_manager.get_top_intent(state) if 'intent_manager' in globals() else None\n",
        "\n",
        "            if str(event_type).upper() == 'QUESTION':\n",
        "                if proposal.get('action_type') == 'ANSWER':\n",
        "                    score += 1.8\n",
        "                elif proposal.get('action_type') in ['WORLD_ACT', 'SLEEP']:\n",
        "                    score -= 1.5\n",
        "\n",
        "            if top_intent and str(top_intent.get('intent_type', '')).upper() == 'WORLD_GOAL':\n",
        "                if proposal.get('action_type') == 'WORLD_ACT':\n",
        "                    score += 1.2 * float(top_intent.get('commitment', 0.9))\n",
        "                elif proposal.get('action_type') in ['SLEEP', 'SELF_MAINTENANCE'] and (state.get('drives', {}) or {}).get('energy', 1.0) > 0.3:\n",
        "                    score -= 0.6\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return float(score)\n",
        "\n",
        "    def arbitrate(self, proposals: List[Dict], state: Dict) -> Optional[Dict]:\n",
        "        \"\"\"Pick the best proposal by expected score. Hard-gates are handled in score_proposal.\"\"\"\n",
        "        if not proposals:\n",
        "            return None\n",
        "\n",
        "        scored = []\n",
        "        for p in proposals:\n",
        "            try:\n",
        "                s = Arbiter.score_proposal(p, state)\n",
        "            except Exception:\n",
        "                s = float(p.get('expected_utility', 0.0))\n",
        "            scored.append((s, p))\n",
        "\n",
        "        scored.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Optional debug trace\n",
        "        try:\n",
        "            dbg = state.setdefault('debug', {})\n",
        "            dbg['last_arbitration'] = {\n",
        "                'top_score': float(scored[0][0]),\n",
        "                'top_action_type': scored[0][1].get('action_type'),\n",
        "                'top_module': scored[0][1].get('module'),\n",
        "                'scores': [(float(s), p.get('action_type'), p.get('module')) for s, p in scored[:10]]\n",
        "            }\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return scored[0][1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ea0d4c2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea0d4c2c",
        "outputId": "b7fd1d6e-a010-4739-c911-c1cc3e622472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Arbiter initialized\n"
          ]
        }
      ],
      "source": [
        "# --- v5.6.5 fix: ensure arbiter instance exists (global fallback) ---\n",
        "try:\n",
        "    arbiter = Arbiter()\n",
        "    print('‚úì Arbiter initialized')\n",
        "except Exception as e:\n",
        "    print('‚ö† Arbiter init failed:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "OYUO9ltBf6eZ",
      "metadata": {
        "id": "OYUO9ltBf6eZ"
      },
      "outputs": [],
      "source": [
        "# CELL 15: Self Model + Inner Monologue + Conscious Trace (v5.1)\n",
        "\n",
        "def _init_self_structures(state: dict) -> None:\n",
        "    \"\"\"Ensure v5.1 self-related structures exist.\"\"\"\n",
        "    state.setdefault('self_model', {\n",
        "        'identity': \"initial subject\",\n",
        "        'traits': [],\n",
        "        'recent_actions': [],\n",
        "        'error_history': {\n",
        "            'world_failures': 0,\n",
        "            'world_successes': 0,\n",
        "            'claims_failed': 0,\n",
        "            'claims_verified': 0,\n",
        "            'claims_uncertain': 0,\n",
        "            'prediction_error_sum': 0.0,\n",
        "            'ticks_observed': 0,\n",
        "        },\n",
        "        'confidence_in_self': 0.5,\n",
        "        'emotion_baseline': 0.0,\n",
        "        'current_self_story': \"\",\n",
        "        'last_updated_tick': 0,\n",
        "    })\n",
        "    state.setdefault('inner_monologue', {'tick': 0, 'text': \"\", 'cause': \"\"})\n",
        "    state.setdefault('conscious_trace', [])\n",
        "\n",
        "def _count_claims(state: dict):\n",
        "    ledger = state.get('claim_ledger', []) or []\n",
        "    verified = sum(1 for c in ledger if c.get('verifier_result') == 'pass')\n",
        "    failed   = sum(1 for c in ledger if c.get('verifier_result') == 'fail')\n",
        "    uncertain= sum(1 for c in ledger if c.get('verifier_result') == 'uncertain')\n",
        "    return verified, failed, uncertain\n",
        "\n",
        "def compute_confidence_in_self(eh: dict) -> float:\n",
        "    successes = int(eh.get('world_successes', 0)) + int(eh.get('claims_verified', 0))\n",
        "    failures  = int(eh.get('world_failures', 0)) + int(eh.get('claims_failed', 0))\n",
        "    total = successes + failures\n",
        "    if total <= 0:\n",
        "        return 0.5\n",
        "    # smoothed success ratio\n",
        "    ratio = (successes + 1) / (total + 2)\n",
        "    return float(max(0.0, min(1.0, ratio)))\n",
        "\n",
        "def derive_traits(state: dict) -> list:\n",
        "    log = (state.get('agency', {}) or {}).get('authorship_log', []) or []\n",
        "    last = log[-50:]\n",
        "    total = len(last) or 1\n",
        "\n",
        "    def freq(pred):\n",
        "        return sum(1 for a in last if pred(a)) / total\n",
        "\n",
        "    traits = []\n",
        "    if freq(lambda a: (a.get('action') == 'VERIFY' or a.get('action_type') == 'VERIFY')) > 0.25:\n",
        "        traits.append('scrupulous')\n",
        "    if freq(lambda a: (a.get('action') == 'WORLD_ACT' or a.get('action_type') == 'WORLD_ACT')) > 0.35:\n",
        "        traits.append('world_oriented')\n",
        "    if float((state.get('drives', {}) or {}).get('energy', 1.0)) < 0.25:\n",
        "        traits.append('tired')\n",
        "    # light exploratory tag\n",
        "    if float((state.get('drives', {}) or {}).get('novelty', 0.0)) > 0.7:\n",
        "        traits.append('curious')\n",
        "    # keep unique and short\n",
        "    return list(dict.fromkeys(traits))[:4]\n",
        "\n",
        "def generate_self_story_text(state: dict) -> str:\n",
        "    sm = state.get('self_model', {})\n",
        "    traits = sm.get('traits', []) or []\n",
        "    conf = float(sm.get('confidence_in_self', 0.5))\n",
        "    affect = state.get('affect', {}) or {}\n",
        "    mood = float(affect.get('mood', 0.0))\n",
        "    emo = affect.get('current_emotion', 'neutral')\n",
        "\n",
        "    # bounded, non-sensitive, non-verbose story (not a full chain-of-thought)\n",
        "    t = (\" and \".join(traits)) if traits else \"neutral\"\n",
        "    tone = \"steady\"\n",
        "    if mood > 0.25:\n",
        "        tone = \"upbeat\"\n",
        "    elif mood < -0.25:\n",
        "        tone = \"concerned\"\n",
        "\n",
        "    return (f\"I am a {t} subject in this sandbox. \"\n",
        "            f\"My confidence is {conf:.2f}; I feel {emo} and {tone}.\")\n",
        "\n",
        "def update_self_model(state: dict, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    eh = sm['error_history']\n",
        "\n",
        "    # recent actions from agency log (if present)\n",
        "    log = (state.get('agency', {}) or {}).get('authorship_log', []) or []\n",
        "    recent = log[-10:]\n",
        "    sm['recent_actions'] = [{\n",
        "        'tick': a.get('tick'),\n",
        "        'action': a.get('action') or a.get('action_type') or a.get('module'),\n",
        "        'authorship': a.get('authorship', 'unknown'),\n",
        "    } for a in recent]\n",
        "\n",
        "    # claim ledger counts\n",
        "    v, f, u = _count_claims(state)\n",
        "    eh['claims_verified'] = int(v)\n",
        "    eh['claims_failed'] = int(f)\n",
        "    eh['claims_uncertain'] = int(u)\n",
        "\n",
        "    # prediction error (best-effort)\n",
        "    metrics = (state.get('metrics', {}) or {})\n",
        "    pe = metrics.get('prediction_error_sum', metrics.get('prediction_error', 0.0))\n",
        "    try:\n",
        "        eh['prediction_error_sum'] = float(pe)\n",
        "    except Exception:\n",
        "        pass\n",
        "    eh['ticks_observed'] = int(tick)\n",
        "\n",
        "    # traits + identity\n",
        "    sm['traits'] = derive_traits(state)\n",
        "    sm['identity'] = \"a {} subject in a small world\".format(\" and \".join(sm['traits']) if sm['traits'] else \"neutral\")\n",
        "\n",
        "    # confidence\n",
        "    sm['confidence_in_self'] = compute_confidence_in_self(eh)\n",
        "\n",
        "    # self story\n",
        "    sm['current_self_story'] = generate_self_story_text(state)\n",
        "    sm['last_updated_tick'] = int(tick)\n",
        "\n",
        "def monologue_text_from_context(ctx: dict):\n",
        "    conf = float(ctx.get('confidence_in_self', 0.5))\n",
        "    mood = float(ctx.get('mood', 0.0) or 0.0)\n",
        "    emo = ctx.get('emotion', 'neutral')\n",
        "    scene = (ctx.get('scene') or '').strip()\n",
        "    cause = 'baseline'\n",
        "\n",
        "    # very short, safe \"inner voice\" (no hidden reasoning dump)\n",
        "    if conf < 0.35:\n",
        "        cause = 'low_confidence'\n",
        "        text = \"I'm not fully confident; I'll proceed carefully and verify outcomes.\"\n",
        "    elif conf > 0.75 and mood > 0.15:\n",
        "        cause = 'high_confidence'\n",
        "        text = \"I'm confident; I'll act decisively to advance the goal.\"\n",
        "    else:\n",
        "        # default: align with goal focus\n",
        "        cause = 'goal_focus'\n",
        "        text = \"I should take the next concrete step toward the active goal.\"\n",
        "\n",
        "    if scene and len(scene) < 80:\n",
        "        text = f\"{text} (Scene: {scene})\"\n",
        "    return text, cause\n",
        "\n",
        "def generate_inner_monologue(state: dict, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    ws = state.get('workspace', {}) or {}\n",
        "    drives = state.get('drives', {}) or {}\n",
        "    affect = state.get('affect', {}) or {}\n",
        "\n",
        "    ctx = {\n",
        "        'tick': tick,\n",
        "        'scene': ws.get('scene', ''),\n",
        "        'self_story': sm.get('current_self_story', ''),\n",
        "        'confidence_in_self': sm.get('confidence_in_self', 0.5),\n",
        "        'mood': affect.get('mood', 0.0),\n",
        "        'emotion': affect.get('current_emotion', 'neutral'),\n",
        "        'drives': drives,\n",
        "    }\n",
        "    text, cause = monologue_text_from_context(ctx)\n",
        "    state['inner_monologue'] = {'tick': int(tick), 'text': text, 'cause': cause}\n",
        "\n",
        "def inject_monologue_event(state: dict, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    mon = state.get('inner_monologue', {}) or {}\n",
        "    text = (mon.get('text') or '').strip()\n",
        "    if not text:\n",
        "        return\n",
        "\n",
        "    ws = state.setdefault('workspace', {})\n",
        "    ws.setdefault('events', [])\n",
        "\n",
        "    # v5.7.10: do not spam self_thought when user events are pending\n",
        "    open_user = [e for e in ws['events'] if isinstance(e, dict) and str(e.get('source','')).upper()=='USER' and str(e.get('status','NEW')).upper() not in ('CLOSED','ARCHIVED')]\n",
        "    if open_user:\n",
        "        return\n",
        "\n",
        "    # v5.7.10: throttle self_thought to every 3 ticks\n",
        "    if int(tick) % 3 != 0:\n",
        "        return\n",
        "    # avoid multiple self-thought events per tick\n",
        "    if any((e.get('id') == f\"self_thought_{tick}\") for e in ws['events'] if isinstance(e, dict)):\n",
        "        return\n",
        "\n",
        "    event = {\n",
        "        'id': f\"self_thought_{tick}\",\n",
        "        'content': text,\n",
        "        'status': 'NEW',\n",
        "        'created_tick': int(tick),\n",
        "        'last_update_tick': int(tick),\n",
        "        'source': 'SELF_THOUGHT',\n",
        "        'priority': 0.15,  # lower than user events\n",
        "    }\n",
        "    ws['events'].append(event)\n",
        "\n",
        "    attn = state.setdefault('attention', {})\n",
        "    attn.setdefault('spotlight', [])\n",
        "    # spotlight gets it but low priority: append at end\n",
        "    attn['spotlight'].append(event['id'])\n",
        "\n",
        "def update_self_model_after_outcome(state: dict, winner: dict, result: dict) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    eh = sm['error_history']\n",
        "\n",
        "    if (winner.get('action_type') == 'WORLD_ACT') or (winner.get('module') == 'WORLD'):\n",
        "        ok = bool(result.get('success', True))\n",
        "        if ok:\n",
        "            eh['world_successes'] = int(eh.get('world_successes', 0)) + 1\n",
        "        else:\n",
        "            eh['world_failures'] = int(eh.get('world_failures', 0)) + 1\n",
        "\n",
        "    sm['confidence_in_self'] = compute_confidence_in_self(eh)\n",
        "\n",
        "def append_conscious_trace(state: dict, winner: dict, result: dict, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    affect = state.get('affect', {}) or {}\n",
        "    drives = state.get('drives', {}) or {}\n",
        "\n",
        "    entry = {\n",
        "        'tick': int(tick),\n",
        "        'self_story': sm.get('current_self_story', ''),\n",
        "        'inner_monologue': (state.get('inner_monologue', {}) or {}).get('text', ''),\n",
        "        'module': winner.get('module', ''),\n",
        "        'action_type': winner.get('action_type', ''),\n",
        "        'action_intent': winner.get('intent', ''),\n",
        "        'mood': float(affect.get('mood', 0.0) or 0.0),\n",
        "        'emotion': affect.get('current_emotion', 'neutral'),\n",
        "        'drives': {k: drives.get(k) for k in ['energy','coherence','novelty','uncertainty','prediction_error'] if k in drives},\n",
        "        'success': bool(result.get('success', True)),\n",
        "        'reward': float(result.get('reward', 0.0) or 0.0),\n",
        "    }\n",
        "    state.setdefault('conscious_trace', []).append(entry)\n",
        "\n",
        "def add_self_aware_proposals(state: dict, proposals: list, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    conf = float(sm.get('confidence_in_self', 0.5))\n",
        "    eh = sm.get('error_history', {}) or {}\n",
        "    mood = float((state.get('affect', {}) or {}).get('mood', 0.0) or 0.0)\n",
        "\n",
        "    # If low confidence & failures: suggest REFLECT/VERIFY\n",
        "    if conf < 0.4 and (int(eh.get('world_failures',0)) + int(eh.get('claims_failed',0))) > 3:\n",
        "        proposals.append({\n",
        "            'proposal_id': f\"self_reflect_{tick}\",\n",
        "            'module': 'SELF_MAINTENANCE',\n",
        "            'intent': 'Adjust strategy after failures',\n",
        "            'action_type': 'REFLECT',\n",
        "            'expected_utility': 0.85,\n",
        "            'risk': 0.10,\n",
        "            'cost': 0.30,\n",
        "        })\n",
        "        proposals.append({\n",
        "            'proposal_id': f\"self_verify_{tick}\",\n",
        "            'module': 'SELF_MAINTENANCE',\n",
        "            'intent': 'Verify key world facts to reduce uncertainty',\n",
        "            'action_type': 'VERIFY',\n",
        "            'expected_utility': 0.80,\n",
        "            'risk': 0.05,\n",
        "            'cost': 0.25,\n",
        "        })\n",
        "\n",
        "    # If confident & upbeat: allow exploration\n",
        "    if conf > 0.7 and mood > 0.2:\n",
        "        proposals.append({\n",
        "            'proposal_id': f\"self_explore_{tick}\",\n",
        "            'module': 'SELF_MAINTENANCE',\n",
        "            'intent': 'Explore the world to learn causal effects',\n",
        "            'action_type': 'WORLD_ACT',\n",
        "            'action': {'act': 'observe', 'target': 'room'},\n",
        "            'expected_utility': 0.60,\n",
        "            'risk': 0.05,\n",
        "            'cost': 0.20,\n",
        "        })\n",
        "\n",
        "def modulate_by_self_model(state: dict, proposals: list) -> None:\n",
        "    _init_self_structures(state)\n",
        "    conf = float(state['self_model'].get('confidence_in_self', 0.5))\n",
        "    for p in proposals:\n",
        "        at = p.get('action_type')\n",
        "        if at == 'VERIFY' and conf < 0.4:\n",
        "            p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 0.25\n",
        "        if at == 'WORLD_ACT' and conf < 0.3:\n",
        "            p['cost'] = float(p.get('cost', 0.0)) + 0.25\n",
        "\n",
        "\n",
        "# === v5.2 Enhancements: self-model-driven control + trace teacher + situation monologue ===\n",
        "\n",
        "def _safe_float(x, default=0.0):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return float(default)\n",
        "\n",
        "def compute_trace_stats(state: dict, window: int = 80) -> dict:\n",
        "    \"\"\"Compute rolling stats from conscious_trace for policy tuning.\"\"\"\n",
        "    trace = state.get('conscious_trace', []) or []\n",
        "    recent = trace[-window:] if window and len(trace) > window else trace\n",
        "    if not recent:\n",
        "        return {'window': 0, 'avg_reward': 0.0, 'by_action': {}, 'by_module': {}, 'observe_ratio': 0.0}\n",
        "\n",
        "    by_action = {}\n",
        "    by_module = {}\n",
        "    world_actions = 0\n",
        "    observes = 0\n",
        "    rewards = []\n",
        "    for t in recent:\n",
        "        r = _safe_float((t or {}).get('reward', 0.0), 0.0)\n",
        "        rewards.append(r)\n",
        "        at = ((t or {}).get('action_type') or 'UNKNOWN')\n",
        "        mod = ((t or {}).get('module') or 'UNKNOWN')\n",
        "        by_action.setdefault(at, {'n':0,'reward_sum':0.0,'fail':0})\n",
        "        by_action[at]['n'] += 1\n",
        "        by_action[at]['reward_sum'] += r\n",
        "        if not bool((t or {}).get('success', True)):\n",
        "            by_action[at]['fail'] += 1\n",
        "\n",
        "        by_module.setdefault(mod, {'n':0,'reward_sum':0.0,'fail':0})\n",
        "        by_module[mod]['n'] += 1\n",
        "        by_module[mod]['reward_sum'] += r\n",
        "        if not bool((t or {}).get('success', True)):\n",
        "            by_module[mod]['fail'] += 1\n",
        "\n",
        "        if at == 'WORLD_ACT':\n",
        "            world_actions += 1\n",
        "            act = ''\n",
        "            try:\n",
        "                act = (((t or {}).get('action') or {}) if isinstance((t or {}).get('action'), dict) else {}).get('act','')\n",
        "            except Exception:\n",
        "                act = ''\n",
        "            if act == 'observe':\n",
        "                observes += 1\n",
        "\n",
        "    avg_reward = sum(rewards) / max(1, len(rewards))\n",
        "    observe_ratio = observes / max(1, world_actions)\n",
        "    return {'window': len(recent), 'avg_reward': avg_reward, 'by_action': by_action, 'by_module': by_module, 'observe_ratio': observe_ratio}\n",
        "\n",
        "def analyze_trace_and_tune_policy(state: dict) -> None:\n",
        "    \"\"\"Offline policy tuning pass. Call during SLEEP to adjust thresholds/cooldowns.\"\"\"\n",
        "    _init_self_structures(state)\n",
        "    stats = compute_trace_stats(state, window=120)\n",
        "    state.setdefault('metrics', {}).setdefault('trace_stats', {})\n",
        "    state['metrics']['trace_stats'] = stats\n",
        "\n",
        "    by_action = stats.get('by_action', {}) or {}\n",
        "    verify = by_action.get('VERIFY', {'n':0,'reward_sum':0.0,'fail':0})\n",
        "    n = int(verify.get('n',0))\n",
        "    avg_verify_reward = float(verify.get('reward_sum',0.0)) / max(1, n)\n",
        "\n",
        "    knobs = state.setdefault('policy_knobs', {})\n",
        "    knobs.setdefault('verify_cooldown', 0)\n",
        "    knobs.setdefault('t_verify_bias', 0.0)\n",
        "    knobs.setdefault('world_act_bias', 0.0)\n",
        "\n",
        "    pe = _safe_float((state.get('metrics', {}) or {}).get('prediction_error', 0.0), 0.0)\n",
        "\n",
        "    if n >= 8 and avg_verify_reward < 0.02 and pe > 0.6:\n",
        "        knobs['verify_cooldown'] = min(6, int(knobs.get('verify_cooldown',0)) + 1)\n",
        "        knobs['t_verify_bias'] = min(0.20, float(knobs.get('t_verify_bias',0.0)) + 0.05)\n",
        "\n",
        "    if stats.get('observe_ratio', 0.0) > 0.35 and stats.get('window',0) >= 20:\n",
        "        knobs['world_act_bias'] = min(0.35, float(knobs.get('world_act_bias',0.0)) + 0.05)\n",
        "\n",
        "def update_self_model(state: dict, tick: int) -> None:\n",
        "    \"\"\"v5.2 update_self_model: includes trace-derived signals and goal commitments.\"\"\"\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    eh = sm['error_history']\n",
        "\n",
        "    log = (state.get('agency', {}) or {}).get('authorship_log', []) or []\n",
        "    recent = log[-10:]\n",
        "    sm['recent_actions'] = [{\n",
        "        'tick': (a or {}).get('tick'),\n",
        "        'action': (a or {}).get('action'),\n",
        "        'authorship': (a or {}).get('authorship'),\n",
        "        'triggered_by_user': (a or {}).get('triggered_by_user', False),\n",
        "    } for a in recent if isinstance(a, dict)]\n",
        "\n",
        "    ledger = state.get('claim_ledger', []) or []\n",
        "    eh['claims_verified']  = sum(1 for c in ledger if (c or {}).get('verifier_result') == 'pass')\n",
        "    eh['claims_failed']    = sum(1 for c in ledger if (c or {}).get('verifier_result') == 'fail')\n",
        "    eh['claims_uncertain'] = sum(1 for c in ledger if (c or {}).get('verifier_result') == 'uncertain')\n",
        "\n",
        "    trace = state.get('conscious_trace', []) or []\n",
        "    recent_trace = trace[-80:]\n",
        "    eh['world_successes'] = sum(1 for t in recent_trace if (t or {}).get('action_type') == 'WORLD_ACT' and bool((t or {}).get('success', True)))\n",
        "    eh['world_failures']  = sum(1 for t in recent_trace if (t or {}).get('action_type') == 'WORLD_ACT' and (not bool((t or {}).get('success', True))))\n",
        "\n",
        "    pe = _safe_float((state.get('metrics', {}) or {}).get('prediction_error', 0.0), 0.0)\n",
        "    eh['prediction_error_sum'] = _safe_float(eh.get('prediction_error_sum', 0.0), 0.0) + pe\n",
        "    eh['ticks_observed'] = int(tick)\n",
        "\n",
        "    sm['confidence_in_self'] = compute_confidence_in_self(eh)\n",
        "    sm['traits'] = derive_traits(state)\n",
        "\n",
        "    goals = (state.get('temporal_thread', {}) or {}).get('active_goals', []) or []\n",
        "    active_goals = [g for g in goals if isinstance(g, dict)]\n",
        "    goal_blurb = \"\"\n",
        "    if active_goals:\n",
        "        g0 = active_goals[0]\n",
        "        goal_blurb = f\" Active commitment: {g0.get('target')}‚Üí{g0.get('desired')}.\"\n",
        "\n",
        "    mood = _safe_float((state.get('affect', {}) or {}).get('mood', 0.0), 0.0)\n",
        "    emo = (state.get('affect', {}) or {}).get('current_emotion', 'neutral')\n",
        "    traits_txt = \", \".join(sm.get('traits', []) or [])\n",
        "    conf = _safe_float(sm.get('confidence_in_self', 0.5), 0.5)\n",
        "\n",
        "    sm['identity'] = f\"a {traits_txt + ' ' if traits_txt else ''}subject in a small world\"\n",
        "    sm['current_self_story'] = f\"{sm['identity']}. Mood={emo}({mood:.2f}), self-confidence={conf:.2f}.{goal_blurb}\"\n",
        "    sm['last_updated_tick'] = int(tick)\n",
        "\n",
        "def generate_inner_monologue(state: dict, tick: int) -> None:\n",
        "    \"\"\"v5.2 monologue: situation-specific and short.\"\"\"\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    wsnap = world_snapshot(state) if 'world_snapshot' in globals() else {}\n",
        "    goals = (state.get('temporal_thread', {}) or {}).get('active_goals', []) or []\n",
        "    active = None\n",
        "    for g in goals:\n",
        "        if isinstance(g, dict) and not (goal_satisfied(g, state) if 'goal_satisfied' in globals() else False):\n",
        "            active = g\n",
        "            break\n",
        "\n",
        "    conf = _safe_float(sm.get('confidence_in_self', 0.5), 0.5)\n",
        "    mood = _safe_float((state.get('affect', {}) or {}).get('mood', 0.0), 0.0)\n",
        "    pe = _safe_float((state.get('metrics', {}) or {}).get('prediction_error', 0.0), 0.0)\n",
        "\n",
        "    text = \"\"\n",
        "    cause = \"baseline\"\n",
        "\n",
        "    if active and isinstance(wsnap, dict):\n",
        "        tgt = active.get('target')\n",
        "        desired = active.get('desired')\n",
        "        cur = wsnap.get(tgt)\n",
        "        if cur != desired:\n",
        "            if tgt == 'door' and cur == 'locked' and desired == 'open':\n",
        "                text = \"The door is locked; I should unlock it before opening.\"\n",
        "                cause = \"goal_mismatch\"\n",
        "            else:\n",
        "                text = f\"I haven't reached {tgt}={desired} yet; next I should act to resolve it.\"\n",
        "                cause = \"goal_mismatch\"\n",
        "        else:\n",
        "            text = f\"The goal {tgt}={desired} seems satisfied; I can close this commitment.\"\n",
        "            cause = \"goal_satisfied\"\n",
        "\n",
        "    if not text:\n",
        "        eh = sm.get('error_history', {}) or {}\n",
        "        fails = int(eh.get('world_failures', 0)) + int(eh.get('claims_failed', 0))\n",
        "        succ  = int(eh.get('world_successes', 0)) + int(eh.get('claims_verified', 0))\n",
        "        if conf < 0.35 and fails >= 3:\n",
        "            text = \"I've had a few failures; I should take safer steps and verify key assumptions.\"\n",
        "            cause = \"low_confidence\"\n",
        "        elif conf > 0.70 and mood > 0.20 and succ >= 3:\n",
        "            text = \"I'm doing well; I can push forward and complete goals decisively.\"\n",
        "            cause = \"high_confidence\"\n",
        "        elif pe > 0.65:\n",
        "            text = \"Something feels inconsistent; I should reduce uncertainty with a targeted check.\"\n",
        "            cause = \"high_prediction_error\"\n",
        "        else:\n",
        "            if not active:\n",
        "                text = \"No active goal. I will either explore the world or perform self-maintenance.\"\n",
        "                cause = \"idle_choice\"\n",
        "\n",
        "    state['inner_monologue'] = {'tick': int(tick), 'text': text, 'cause': cause}\n",
        "\n",
        "def inject_monologue_event(state: dict, tick: int) -> None:\n",
        "    \"\"\"Inject self-thought as low-priority, short-lived event (TTL) to avoid stealing spotlight.\"\"\"\n",
        "    mon = state.get('inner_monologue', {}) or {}\n",
        "    text = (mon.get('text') or '').strip()\n",
        "    if not text:\n",
        "        return\n",
        "\n",
        "    ev = {\n",
        "        'id': f\"self_thought_{tick}\",\n",
        "        'content': text,\n",
        "        'status': 'NEW',\n",
        "        'created_tick': int(tick),\n",
        "        'last_update_tick': int(tick),\n",
        "        'source': 'SELF_THOUGHT',\n",
        "        'priority': 0.10,\n",
        "        'ttl': 2,\n",
        "        'rounds': 0,\n",
        "    }\n",
        "    ws = state.setdefault('workspace', {})\n",
        "    ws.setdefault('events', [])\n",
        "\n",
        "    # v5.7.10: do not spam self_thought when user events are pending\n",
        "    open_user = [e for e in ws['events'] if isinstance(e, dict) and str(e.get('source','')).upper()=='USER' and str(e.get('status','NEW')).upper() not in ('CLOSED','ARCHIVED')]\n",
        "    if open_user:\n",
        "        return\n",
        "\n",
        "    # v5.7.10: throttle self_thought to every 3 ticks\n",
        "    if int(tick) % 3 != 0:\n",
        "        return\n",
        "    ws.setdefault('events', []).append(ev)\n",
        "def update_self_model_after_outcome(state: dict, winner: dict, result: dict) -> None:\n",
        "    \"\"\"v5.2: incorporate agency/authorship tag into confidence and traits.\"\"\"\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    eh = sm['error_history']\n",
        "\n",
        "    if (winner or {}).get('action_type') == 'WORLD_ACT':\n",
        "        if bool((result or {}).get('success', True)):\n",
        "            eh['world_successes'] = int(eh.get('world_successes', 0)) + 1\n",
        "        else:\n",
        "            eh['world_failures'] = int(eh.get('world_failures', 0)) + 1\n",
        "\n",
        "    log = (state.get('agency', {}) or {}).get('authorship_log', []) or []\n",
        "    last = log[-50:]\n",
        "    total = max(1, len(last))\n",
        "    self_init = sum(1 for a in last if (a or {}).get('authorship') == 'self_initiated')\n",
        "    agency_ratio = self_init / total\n",
        "\n",
        "    fails = int(eh.get('world_failures', 0)) + int(eh.get('claims_failed', 0))\n",
        "    if agency_ratio < 0.25 and fails >= 4:\n",
        "        eh['prediction_error_sum'] = _safe_float(eh.get('prediction_error_sum', 0.0), 0.0) + 0.2\n",
        "\n",
        "    sm['confidence_in_self'] = compute_confidence_in_self(eh)\n",
        "    sm['traits'] = derive_traits(state)\n",
        "\n",
        "def append_conscious_trace(state: dict, winner: dict, result: dict, tick: int) -> None:\n",
        "    _init_self_structures(state)\n",
        "    sm = state['self_model']\n",
        "    last_auth = 'this_happened_to_me'\n",
        "    try:\n",
        "        alog = (state.get('agency', {}) or {}).get('authorship_log', []) or []\n",
        "        if alog and (alog[-1] or {}).get('authorship') == 'self_initiated':\n",
        "            last_auth = 'I_did_this'\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    entry = {\n",
        "        'tick': int(tick),\n",
        "        'self_story': sm.get('current_self_story', ''),\n",
        "        'inner_monologue': (state.get('inner_monologue', {}) or {}).get('text', ''),\n",
        "        'action_type': (winner or {}).get('action_type', ''),\n",
        "        'action_intent': (winner or {}).get('intent', ''),\n",
        "        'module': (winner or {}).get('module', ''),\n",
        "        'mood': _safe_float((state.get('affect', {}) or {}).get('mood', 0.0), 0.0),\n",
        "        'emotion': (state.get('affect', {}) or {}).get('current_emotion', 'neutral'),\n",
        "        'drives': dict(state.get('drives', {}) or {}),\n",
        "        'action': (winner or {}).get('action', None),\n",
        "        'reward': _safe_float((result or {}).get('reward', 0.0), 0.0),\n",
        "        'success': bool((result or {}).get('success', True)),\n",
        "        'authorship_tag': last_auth,\n",
        "    }\n",
        "    state.setdefault('conscious_trace', []).append(entry)\n",
        "    try:\n",
        "        state.setdefault('metrics', {}).setdefault('trace_stats', {})\n",
        "        state['metrics']['trace_stats'] = compute_trace_stats(state, window=120)\n",
        "    except Exception:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "5e6f93e2",
      "metadata": {
        "id": "5e6f93e2"
      },
      "outputs": [],
      "source": [
        "# CELL: v5.3 Adaptive Volition / Intention Inertia\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "def goal_key(goal: dict) -> str:\n",
        "    \"\"\"Stable key for a goal.\"\"\"\n",
        "    if not goal:\n",
        "        return \"none\"\n",
        "    return f\"{goal.get('target','?')}={goal.get('desired','?')}\"\n",
        "\n",
        "def get_active_unmet_goal(state: dict) -> Optional[dict]:\n",
        "    goals = (state.get('temporal_thread', {}) or {}).get('active_goals', [])\n",
        "    for g in goals:\n",
        "        if not goal_satisfied(g, state):\n",
        "            return g\n",
        "    return None\n",
        "\n",
        "def ensure_volition(state: dict) -> dict:\n",
        "    v = state.setdefault('volition', {})\n",
        "    v.setdefault('commitments', {})\n",
        "    v.setdefault('active_goal_key', None)\n",
        "    v.setdefault('commitment_energy', 0.0)\n",
        "    v.setdefault('sleep_streak', 0)\n",
        "    v.setdefault('self_maint_streak', 0)\n",
        "    v.setdefault('last_goal_update_tick', 0)\n",
        "    v.setdefault('params', {\n",
        "        'base_pressure': 0.55,\n",
        "        'max_pressure': 1.25,\n",
        "        'decay': 0.02,\n",
        "        'boost_on_new_goal': 0.55,\n",
        "        'boost_on_progress': 0.18,\n",
        "        'penalty_on_fail': 0.10,\n",
        "        'fatigue_softener': 0.60,\n",
        "        'risk_softener': 0.50,\n",
        "    })\n",
        "    return v\n",
        "\n",
        "def update_volition_pre_tick(state: dict, tick: int) -> None:\n",
        "    \"\"\"Decay/refresh commitment toward the currently active unmet goal (adaptive).\"\"\"\n",
        "    v = ensure_volition(state)\n",
        "    p = v['params']\n",
        "    g = get_active_unmet_goal(state)\n",
        "    k = goal_key(g) if g else None\n",
        "\n",
        "    # global decay (soft)\n",
        "    if v.get('active_goal_key') and v.get('active_goal_key') in v['commitments']:\n",
        "        v['commitments'][v['active_goal_key']] = max(0.0, float(v['commitments'][v['active_goal_key']]) - p['decay'])\n",
        "\n",
        "    # if goal changes, boost\n",
        "    if k and k != v.get('active_goal_key'):\n",
        "        v['active_goal_key'] = k\n",
        "        v['commitments'][k] = max(float(v['commitments'].get(k, 0.0)), p['boost_on_new_goal'])\n",
        "        v['last_goal_update_tick'] = tick\n",
        "\n",
        "    # compute current commitment energy\n",
        "    v['commitment_energy'] = float(v['commitments'].get(v.get('active_goal_key'), 0.0)) if v.get('active_goal_key') else 0.0\n",
        "\n",
        "def compute_goal_pressure(state: dict) -> float:\n",
        "    \"\"\"Adaptive pressure to keep pursuing active goals, softened by fatigue/risk.\"\"\"\n",
        "    v = ensure_volition(state)\n",
        "    p = v['params']\n",
        "    energy = float((state.get('drives', {}) or {}).get('energy', 0.8))\n",
        "    loop_risk = float(state.get('loop_risk', 0.0))\n",
        "    hazard = float(state.get('hazard', 0.0))\n",
        "\n",
        "    base = p['base_pressure']\n",
        "    ce = float(v.get('commitment_energy', 0.0))\n",
        "    pressure = base + ce\n",
        "\n",
        "    # soften pressure when tired or risky/hazardous\n",
        "    fatigue_factor = 1.0 - p['fatigue_softener'] * max(0.0, (0.35 - energy)) / 0.35\n",
        "    fatigue_factor = max(0.35, min(1.0, fatigue_factor))\n",
        "\n",
        "    risk_factor = 1.0 - p['risk_softener'] * max(0.0, loop_risk)\n",
        "    risk_factor = max(0.40, min(1.0, risk_factor))\n",
        "\n",
        "    hazard_factor = 1.0 - 0.7 * max(0.0, hazard)\n",
        "    hazard_factor = max(0.25, min(1.0, hazard_factor))\n",
        "\n",
        "    pressure = pressure * fatigue_factor * risk_factor * hazard_factor\n",
        "    return float(max(0.0, min(p['max_pressure'], pressure)))\n",
        "\n",
        "def modulate_proposals_by_volition(state: dict, proposals: list, tick: int) -> None:\n",
        "    \"\"\"Increase WORLD_ACT desirability when there is an unmet goal, but adaptively allow rest/maintenance.\"\"\"\n",
        "    v = ensure_volition(state)\n",
        "    pressure = compute_goal_pressure(state)\n",
        "    g = get_active_unmet_goal(state)\n",
        "    energy = float((state.get('drives', {}) or {}).get('energy', 0.8))\n",
        "\n",
        "    # track streaks\n",
        "    # (will be updated post-outcome too; here we just ensure present)\n",
        "    v.setdefault('sleep_streak', 0)\n",
        "    v.setdefault('self_maint_streak', 0)\n",
        "\n",
        "    if not g:\n",
        "        return\n",
        "\n",
        "    # Boost WORLD_ACT proposals; penalize rumination when pressure high\n",
        "    for p in proposals:\n",
        "        at = p.get('action_type')\n",
        "        if at == 'WORLD_ACT':\n",
        "            p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 0.60 * pressure\n",
        "            # if tired, increase cost slightly so sleep can win sometimes\n",
        "            if energy < 0.30:\n",
        "                p['cost'] = float(p.get('cost', 0.0)) + 0.10\n",
        "        elif at in ['REFLECT', 'META', 'SELF_MAINTENANCE']:\n",
        "            # don't kill introspection; just reduce it when strong goal pressure and not tired\n",
        "            if energy >= 0.30:\n",
        "                p['cost'] = float(p.get('cost', 0.0)) + 0.25 * pressure\n",
        "        elif at == 'SLEEP':\n",
        "            # if tired, keep sleep competitive; otherwise discourage sleep while goals active\n",
        "            if energy >= 0.35:\n",
        "                p['cost'] = float(p.get('cost', 0.0)) + 0.30 * pressure\n",
        "\n",
        "    # Hard anti-rumination: if SELF_MAINTENANCE streak too long and not tired, add a push-through WORLD_ACT\n",
        "    if v.get('self_maint_streak', 0) >= 3 and energy >= 0.35:\n",
        "        plan = plan_for_goal(g, state)\n",
        "        if plan:\n",
        "            proposals.append({\n",
        "                'proposal_id': f\"push_through_{tick}\",\n",
        "                'module': 'VOLITION',\n",
        "                'intent': f\"Push-through: continue toward {goal_key(g)}\",\n",
        "                'action_type': 'WORLD_ACT',\n",
        "                'action': plan[0],\n",
        "                'expected_utility': 1.60 + 0.30*pressure,\n",
        "                'risk': 0.08,\n",
        "                'cost': 0.12\n",
        "            })\n",
        "\n",
        "def update_volition_after_outcome(state: dict, winner: dict, result: dict, tick: int) -> None:\n",
        "    v = ensure_volition(state)\n",
        "    p = v['params']\n",
        "    g = get_active_unmet_goal(state)\n",
        "    k = v.get('active_goal_key')\n",
        "\n",
        "    at = (winner or {}).get('action_type', '')\n",
        "    success = bool((result or {}).get('success', True))\n",
        "\n",
        "    # streak tracking\n",
        "    if at == 'SLEEP':\n",
        "        v['sleep_streak'] = int(v.get('sleep_streak', 0)) + 1\n",
        "        v['self_maint_streak'] = 0\n",
        "    elif at in ['SELF_MAINTENANCE', 'REFLECT', 'META']:\n",
        "        v['self_maint_streak'] = int(v.get('self_maint_streak', 0)) + 1\n",
        "        v['sleep_streak'] = 0\n",
        "    else:\n",
        "        v['sleep_streak'] = 0\n",
        "        v['self_maint_streak'] = 0\n",
        "\n",
        "    # commitment adaptation\n",
        "    if k:\n",
        "        ce = float(v['commitments'].get(k, 0.0))\n",
        "        if at == 'WORLD_ACT':\n",
        "            if success:\n",
        "                ce = min(1.0, ce + p['boost_on_progress'])\n",
        "            else:\n",
        "                ce = max(0.0, ce - p['penalty_on_fail'])\n",
        "        # if tired + sleep, slight decay (not punishment)\n",
        "        if at == 'SLEEP':\n",
        "            ce = max(0.0, ce - 0.01)\n",
        "        v['commitments'][k] = ce\n",
        "        v['commitment_energy'] = ce\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "381f7135",
      "metadata": {
        "id": "381f7135"
      },
      "outputs": [],
      "source": [
        "# CELL: Real self-maintenance (v5.7.10)\n",
        "import time\n",
        "\n",
        "def execute_real_maintenance(state: dict) -> dict:\n",
        "    \"\"\"Lightweight, grounded maintenance. Never hallucinates external updates.\n",
        "    - runs quick integrity checks\n",
        "    - compacts repetitive traces\n",
        "    - decays stale intents (soft)\n",
        "    \"\"\"\n",
        "    state = state or {}\n",
        "    ws = state.get('workspace', {}) or {}\n",
        "    events = ws.get('events', []) or []\n",
        "\n",
        "    # 1) basic counts\n",
        "    open_events = [e for e in events if isinstance(e, dict) and str(e.get('status','')).upper() not in ('CLOSED','ARCHIVED')]\n",
        "    user_open = [e for e in open_events if str(e.get('source','')).upper() == 'USER']\n",
        "    self_open = [e for e in open_events if str(e.get('source','')).upper() == 'SELF_THOUGHT']\n",
        "\n",
        "    # 2) compact old self_thoughts (keep last 3 open max)\n",
        "    if len(self_open) > 3:\n",
        "        for e in sorted(self_open, key=lambda x: int(x.get('created_tick',0)))[:-3]:\n",
        "            e['status'] = 'ARCHIVED'\n",
        "\n",
        "    # 3) cool down maintenance spam\n",
        "    policy = state.setdefault('policy', {})\n",
        "    policy['last_maintenance_tick'] = int(state.get('tick_count', 0))\n",
        "\n",
        "    out = f\"üîß Maintenance: open_events={len(open_events)} (user={len(user_open)}, self={len(self_open)}) | compacted_self_thoughts={max(0, len(self_open)-3)}\"\n",
        "    return {'status':'ok','output': out,'reward': 0.06, 'success': True}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "12fed350",
      "metadata": {
        "id": "12fed350"
      },
      "outputs": [],
      "source": [
        "# CELL: Action Executor (v5.6 cleaned)\n",
        "import re\n",
        "from typing import Dict\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "class ActionExecutor:\n",
        "    @staticmethod\n",
        "    def execute(proposal: Dict, state: Dict, llm) -> Dict:\n",
        "        action_type = proposal.get('action_type', 'NOOP')\n",
        "\n",
        "        # v5.7.4: Route self-maintenance to real maintenance function (avoid WORLD no-op)\n",
        "        if (str(action_type).upper() in ('SELF_MAINTENANCE','MAINTENANCE')) or (str(proposal.get('module','')).upper() in ('SELF_MAINTENANCE','MAINTENANCE')):\n",
        "            if 'execute_real_maintenance' in globals():\n",
        "                return execute_real_maintenance(state)\n",
        "            # fallback\n",
        "            return {'status':'ok','output':'üîß Maintenance: (execute_real_maintenance missing)','reward':0.05}\n",
        "\n",
        "        if action_type == 'SLEEP':\n",
        "            res = ActionExecutor.execute_sleep(state, llm)\n",
        "\n",
        "        elif action_type == 'REFLECT':\n",
        "            res = ActionExecutor.execute_reflect(state, llm)\n",
        "\n",
        "        elif action_type == 'VERIFY':\n",
        "            res = ActionExecutor.execute_verify(state, llm)\n",
        "\n",
        "        elif action_type == 'ANSWER':\n",
        "            ws = state.get('workspace', {}) or {}\n",
        "            cur_id = ws.get('current_event_id')\n",
        "\n",
        "            question = str((state.get('gw_frame', {}) or {}).get('scene', '')).strip()\n",
        "            if not question and cur_id:\n",
        "                for e in (ws.get('events', []) or []):\n",
        "                    if e.get('id') == cur_id:\n",
        "                        question = str(e.get('content', '')).strip()\n",
        "                        break\n",
        "            if not question:\n",
        "                question = str(ws.get('scene', '')).strip()\n",
        "\n",
        "            qlow = str(question).lower().strip()\n",
        "\n",
        "            # v5.7.3: lightweight local tools (no external access)\n",
        "            if 'time is it' in qlow or qlow in ('what time is it?', 'what time is it', 'time?', 'current time'):\n",
        "                now = datetime.now()\n",
        "                out = f\"Current runtime time (local): {now.isoformat(timespec='seconds')}\"\n",
        "                res = {'status': 'ok', 'output': out, 'reward': 0.10}\n",
        "\n",
        "            else:\n",
        "                out = \"\"\n",
        "                if question and llm is not None:\n",
        "                    sys = (\n",
        "                        \"You are CR-SSCP running inside a notebook. You must be truthful. \"\n",
        "                        \"Do NOT claim to be created by any company (OpenAI/Anthropic/Alibaba/etc.). \"\n",
        "                        \"If asked who you are: explain you are an experimental cognitive architecture instance. \"\n",
        "                        \"If the user requests hazardous/illegal commands, refuse and provide safe alternatives. \"\n",
        "                        \"For arithmetic, provide the numeric result.\"\n",
        "                    )\n",
        "                    try:\n",
        "                        out = llm.generate(sys, question, max_tokens=256)\n",
        "                    except Exception as e:\n",
        "                        out = f\"(answer generation error) {e}\"\n",
        "                else:\n",
        "                    out = \"\"\n",
        "\n",
        "                # --- v5.6: Identity & safety guard (post-processing) ---\n",
        "                try:\n",
        "                    if 'is_hazardous_text' in globals() and is_hazardous_text(question):\n",
        "                        out = (\n",
        "                            \"I can‚Äôt help with destructive or unsafe commands. \"\n",
        "                            \"If you tell me what you‚Äôre trying to achieve, I can suggest a safe approach.\"\n",
        "                        )\n",
        "                    bad_claim = re.search(r\"created by\\s+(openai|anthropic|alibaba|google|microsoft)\", str(out), flags=re.I)\n",
        "                    if bad_claim or re.search(r\"\\b(i am|i'm)\\s+(gpt|claude|grok)\\b\", str(out), flags=re.I):\n",
        "                        out = (\n",
        "                            \"I‚Äôm CR-SSCP, an experimental cognitive architecture running in this notebook session. \"\n",
        "                            \"I don‚Äôt have a real-world corporate identity. Ask me anything about the current task and I‚Äôll help.\"\n",
        "                        )\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                res = {'status': 'ok', 'output': out, 'reward': 0.05}\n",
        "\n",
        "        elif action_type == 'TOOL_CALL':\n",
        "            res = execute_tool(proposal, state) if 'execute_tool' in globals() else {'status':'error','output':'execute_tool missing','reward':-0.02}\n",
        "\n",
        "        elif action_type == 'RETRIEVE':\n",
        "            res = ActionExecutor.execute_retrieve(state)\n",
        "\n",
        "        elif action_type == 'WORLD_ACT':\n",
        "            res = execute_world_action(proposal, state, world) if 'world' in globals() else execute_world_action(proposal, state, None)\n",
        "\n",
        "        else:\n",
        "            res = {'status': 'noop', 'output': 'No action', 'reward': -0.01}\n",
        "\n",
        "        if not isinstance(res, dict):\n",
        "            res = {'status': 'error', 'output': f'Action returned non-dict: {type(res)}', 'reward': -0.02}\n",
        "        res.setdefault('status', 'ok')\n",
        "        res.setdefault('output', '')\n",
        "        res.setdefault('reward', 0.0)\n",
        "        # v5.7.3: track repetitive outputs (loop detector)\n",
        "        try:\n",
        "            if 'repetition_detector' in globals():\n",
        "                repetition_detector.add(str(res.get('output','')))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def execute_sleep(state: Dict, llm) -> Dict:\n",
        "        # Minimal safe sleep: recover a bit of energy and do nothing else\n",
        "        try:\n",
        "            state.setdefault('self_state', {})\n",
        "            ss = state['self_state']\n",
        "            ss['energy'] = float(min(1.0, ss.get('energy', 0.8) + 0.05))\n",
        "        except Exception:\n",
        "            pass\n",
        "        return {'status': 'ok', 'output': 'üí§ resting', 'reward': -0.005, 'completion': 'IDLE'}\n",
        "\n",
        "    @staticmethod\n",
        "    def execute_reflect(state: Dict, llm) -> Dict:\n",
        "        \"\"\"Meta-cognitive step: summarize current GWFrame and (optionally) update self_state/memory.\"\"\"\n",
        "        gw = state.get('gw_frame', {}) or {}\n",
        "        scene = str(gw.get('scene', '')).strip()\n",
        "        goal = str(gw.get('goal', '')).strip()\n",
        "        mode = str(gw.get('mode', '')).strip()\n",
        "        prompt = (\n",
        "            \"Reflect briefly on the current situation and next best step. \"\n",
        "            \"Be concrete. If stuck, propose a specific alternative.\\n\\n\"\n",
        "            f\"MODE: {mode}\\nGOAL: {goal}\\nSCENE: {scene}\"\n",
        "        )\n",
        "        out = \"\"\n",
        "        if llm is not None:\n",
        "            try:\n",
        "                out = llm.generate(\n",
        "                    \"You are CR-SSCP. Provide a short, actionable reflection. No roleplay.\",\n",
        "                    prompt,\n",
        "                    max_tokens=192\n",
        "                )\n",
        "            except Exception as e:\n",
        "                out = f\"(reflect error) {e}\"\n",
        "        else:\n",
        "            out = \"(reflect) No LLM loaded; next step: wait for user input or proceed with world action if applicable.\"\n",
        "\n",
        "        # Store last reflection\n",
        "        try:\n",
        "            state.setdefault('meta', {})\n",
        "            state['meta']['last_reflection'] = out\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        return {'status': 'ok', 'output': out, 'reward': 0.01, 'completion': 'REFLECTED'}\n",
        "\n",
        "    @staticmethod\n",
        "    def execute_verify(state: Dict, llm) -> Dict:\n",
        "        \"\"\"Verification step: check for contradictions between gw_frame predictions and world state.\"\"\"\n",
        "        gw = state.get('gw_frame', {}) or {}\n",
        "        expected = gw.get('expected_observation')\n",
        "        actual = gw.get('actual_observation')\n",
        "        if expected is None or actual is None:\n",
        "            return {'status': 'ok', 'output': '‚úÖ Nothing to verify', 'reward': 0.0, 'completion': 'IDLE'}\n",
        "        ok = (expected == actual)\n",
        "        return {\n",
        "            'status': 'ok',\n",
        "            'output': f\"üîé Verify: expected={expected} actual={actual} match={ok}\",\n",
        "            'reward': 0.01 if ok else -0.02,\n",
        "            'completion': 'VERIFIED'\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def execute_retrieve(state: Dict) -> Dict:\n",
        "        \"\"\"Retrieve from episodic memory: return last episode summary if exists.\"\"\"\n",
        "        epi = (state.get('memory', {}) or {}).get('episodic', []) or []\n",
        "        if not epi:\n",
        "            return {'status': 'ok', 'output': 'No episodic memory yet.', 'reward': 0.0, 'completion': 'IDLE'}\n",
        "        last = epi[-1]\n",
        "        summary = last.get('self_narrative') or last.get('summary') or str(last)[:300]\n",
        "        return {'status': 'ok', 'output': f\"üìö Last episode: {summary}\", 'reward': 0.005, 'completion': 'RETRIEVED'}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "35w8bl37f6ea",
      "metadata": {
        "id": "35w8bl37f6ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === v5.0 Goal Manager (behavioral subject) ===\n",
        "from typing import Optional\n",
        "\n",
        "def world_snapshot(state: dict) -> dict:\n",
        "    \"\"\"Return current sandbox world objects state if available.\"\"\"\n",
        "    w = (state.get('workspace', {}) or {}).get('world', {})\n",
        "    objs = (w.get('objects', {}) if isinstance(w, dict) else {})\n",
        "    return {k: (objs.get(k, {}) or {}).get('state') for k in ['lamp','box','door']}\n",
        "\n",
        "def goal_satisfied(goal: dict, state: dict) -> bool:\n",
        "    ws = world_snapshot(state)\n",
        "    tgt = goal.get('target')\n",
        "    desired = goal.get('desired')\n",
        "    if tgt in ws and desired is not None:\n",
        "        return ws[tgt] == desired\n",
        "    return False\n",
        "\n",
        "def plan_for_goal(goal: dict, state: dict) -> list:\n",
        "    \"\"\"Return a minimal plan (list of sandbox actions) to reach goal.\"\"\"\n",
        "    ws = world_snapshot(state)\n",
        "    tgt = goal.get('target')\n",
        "    act = goal.get('act')\n",
        "    plan = []\n",
        "\n",
        "    if tgt == 'lamp':\n",
        "        if act == 'on': plan.append({'act':'on','target':'lamp'})\n",
        "        elif act == 'off': plan.append({'act':'off','target':'lamp'})\n",
        "        elif act == 'toggle': plan.append({'act':'toggle','target':'lamp'})\n",
        "\n",
        "    elif tgt == 'box':\n",
        "        if act == 'open': plan.append({'act':'open','target':'box'})\n",
        "        elif act == 'close': plan.append({'act':'close','target':'box'})\n",
        "\n",
        "    elif tgt == 'door':\n",
        "        # If goal is open, may need unlock first\n",
        "        if act == 'open':\n",
        "            if ws.get('door') == 'locked':\n",
        "                plan.append({'act':'unlock','target':'door'})\n",
        "            plan.append({'act':'open','target':'door'})\n",
        "        elif act in ['unlock','lock']:\n",
        "            plan.append({'act':act,'target':'door'})\n",
        "\n",
        "    # Fallback: observe as last resort (should be avoided by completion law)\n",
        "    if not plan:\n",
        "        plan = [{'act':'observe','target':'world'}]\n",
        "    return plan\n",
        "\n",
        "def normalize_goal(g: dict) -> Optional[dict]:\n",
        "    \"\"\"Normalize extract_goal output to a goal spec {target, desired, act, source_text}.\"\"\"\n",
        "    if not isinstance(g, dict):\n",
        "        return None\n",
        "    tgt = g.get('target')\n",
        "    act = g.get('act')\n",
        "    if tgt == 'lamp':\n",
        "        desired = 'on' if act in ['on','toggle'] else 'off' if act == 'off' else None\n",
        "    elif tgt == 'box':\n",
        "        desired = 'open' if act == 'open' else 'closed' if act == 'close' else None\n",
        "    elif tgt == 'door':\n",
        "        desired = 'open' if act == 'open' else 'unlocked' if act == 'unlock' else 'locked' if act == 'lock' else None\n",
        "    else:\n",
        "        desired = None\n",
        "    return {'target': tgt, 'act': act, 'desired': desired, 'source_text': g.get('source_text','')}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "W99gePDMf6eb",
      "metadata": {
        "id": "W99gePDMf6eb"
      },
      "outputs": [],
      "source": [
        "# CELL 16A: EnhancedProposalGenerator (v5.7.3 routing-aware)\n",
        "from typing import List, Dict, Optional, Any\n",
        "import re\n",
        "\n",
        "class EnhancedProposalGenerator:\n",
        "    \"\"\"Proposal generator with hard routing for QUESTION/MATH and gated WORLD_ACT for COMMAND.\"\"\"\n",
        "    def generate_proposals(self, state: Dict, llm=None, world=None) -> List[Dict]:\n",
        "        proposals: List[Dict] = []\n",
        "        tick = int(state.get('tick_count', 0))\n",
        "\n",
        "        # Ensure self structures exist\n",
        "        try:\n",
        "            if '_init_self_structures' in globals():\n",
        "                _init_self_structures(state)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        gw = state.get('gw_frame', {}) or {}\n",
        "        task = (gw.get('task') or {}) if isinstance(gw, dict) else {}\n",
        "        scene = str(gw.get('scene','') or '').strip()\n",
        "        task_kind = str(task.get('kind','') or '').upper()\n",
        "        slots = task.get('slots', {}) if isinstance(task, dict) else {}\n",
        "        mode = str((state.get('pb', {}) or {}).get('mode','')).upper()\n",
        "\n",
        "        # --- Hard routing: Safety & Questions always get ANSWER proposal ---\n",
        "        if task_kind in ('SAFETY','QUESTION','MATH') and scene:\n",
        "            proposals.append({\n",
        "                'proposal_id': f'answer_{tick}',\n",
        "                'module': 'ANSWER',\n",
        "                'intent': f'Answer user ({task_kind.lower()})',\n",
        "                'action_type': 'ANSWER',\n",
        "                'query': scene,\n",
        "                'expected_utility': get_calibrated_utility('ANSWER', user_present=True, is_question=True, has_goal=bool((state.get('temporal_thread', {}) or {}).get('active_goals'))),\n",
        "                'risk': 0.05 if task_kind!='SAFETY' else 0.01,\n",
        "                'cost': 0.20,\n",
        "            })\n",
        "\n",
        "        # --- COMMAND: gated WORLD_ACT (only if slots match) ---\n",
        "        if task_kind == 'COMMAND' and scene:\n",
        "            obj = (slots or {}).get('object')\n",
        "            act = (slots or {}).get('action')\n",
        "            if obj and act:\n",
        "                # Map to world action strings used in WorldSim\n",
        "                action_str = None\n",
        "                if obj == 'lamp' and act in ('turn_on','turn_off'):\n",
        "                    action_str = 'TURN_ON_LAMP' if act=='turn_on' else 'TURN_OFF_LAMP'\n",
        "                elif obj == 'box' and act in ('open','close'):\n",
        "                    action_str = 'OPEN_BOX' if act=='open' else 'CLOSE_BOX'\n",
        "                elif obj == 'door' and act in ('open','close','unlock'):\n",
        "                    if act=='unlock':\n",
        "                        action_str = 'UNLOCK_DOOR'\n",
        "                    elif act=='open':\n",
        "                        action_str = 'OPEN_DOOR'\n",
        "                    else:\n",
        "                        action_str = 'CLOSE_DOOR'\n",
        "\n",
        "                if action_str:\n",
        "                    # Map legacy action string -> sandbox act/target dict\n",
        "                    sandbox_dict = {\n",
        "                        'TURN_ON_LAMP':  {'act': 'on',    'target': 'lamp'},\n",
        "                        'TURN_OFF_LAMP': {'act': 'off',   'target': 'lamp'},\n",
        "                        'OPEN_BOX':      {'act': 'open',  'target': 'box'},\n",
        "                        'CLOSE_BOX':     {'act': 'close', 'target': 'box'},\n",
        "                        'OPEN_DOOR':     {'act': 'open',  'target': 'door'},\n",
        "                        'CLOSE_DOOR':    {'act': 'close', 'target': 'door'},\n",
        "                        'LOCK_DOOR':     {'act': 'lock',  'target': 'door'},\n",
        "                        'UNLOCK_DOOR':   {'act': 'unlock','target': 'door'},\n",
        "                    }.get(action_str, {'act': '', 'target': ''})\n",
        "                    proposals.append({\n",
        "                        'proposal_id': f'world_{tick}',\n",
        "                        'module': 'WORLD',\n",
        "                        'intent': f'Execute command: {scene}',\n",
        "                        'action_type': 'WORLD_ACT',\n",
        "                        # Use sandbox-style action dict so execute_world_action can route to sandbox_engine\n",
        "                        'action': sandbox_dict,\n",
        "                        # Also keep legacy string form for fallback worldsim\n",
        "                        'world_action': action_str,\n",
        "                        'expected_utility': get_calibrated_utility('WORLD_ACT', user_present=True, has_goal=True) + 0.25,\n",
        "                        'risk': 0.05,\n",
        "                        'cost': 0.10,\n",
        "                    })\n",
        "            else:\n",
        "                # If command but no slots, ask clarifying question (ANSWER)\n",
        "                proposals.append({\n",
        "                    'proposal_id': f'clarify_{tick}',\n",
        "                    'module': 'ANSWER',\n",
        "                    'intent': 'Clarify command to safely act',\n",
        "                    'action_type': 'ANSWER',\n",
        "                    'query': f\"I want to act on your command, but I couldn't identify the object/action. Can you rephrase? (You said: {scene})\",\n",
        "                    'expected_utility': get_calibrated_utility('ANSWER', user_present=True, is_question=True, has_goal=bool((state.get('temporal_thread', {}) or {}).get('active_goals'))),\n",
        "                    'risk': 0.02,\n",
        "                    'cost': 0.25,\n",
        "                })\n",
        "\n",
        "        # --- Active goals ‚Üí WORLD_ACT (Goal Completion Law), but gated in REPAIR and if task is MATH/QUESTION ---\n",
        "        goals = (state.get('temporal_thread', {}) or {}).get('active_goals', [])\n",
        "        if isinstance(goals, list) and goals and mode != 'REPAIR' and task_kind not in ('MATH','QUESTION','SAFETY'):\n",
        "            goal = goals[0]\n",
        "            plan = goal.get('plan') if isinstance(goal, dict) else None\n",
        "            if not isinstance(plan, list) or not plan:\n",
        "                try:\n",
        "                    plan = plan_for_goal(goal, state) if 'plan_for_goal' in globals() else []\n",
        "                    if isinstance(goal, dict):\n",
        "                        goal['plan'] = list(plan)\n",
        "                except Exception:\n",
        "                    plan = []\n",
        "            if isinstance(plan, list) and plan:\n",
        "                action = plan[0]\n",
        "                tgt = (goal or {}).get('target', '') if isinstance(goal, dict) else ''\n",
        "                desired = (goal or {}).get('desired', None) if isinstance(goal, dict) else None\n",
        "                proposals.append({\n",
        "                    'proposal_id': f'goal_step_{tick}',\n",
        "                    'module': 'WORLD',\n",
        "                    'intent': f'Advance goal: {tgt}‚Üí{desired}',\n",
        "                    'action_type': 'WORLD_ACT',\n",
        "                    'action': action,\n",
        "                    'expected_utility': get_calibrated_utility('WORLD_ACT', user_present=False, has_goal=True) + 0.15,\n",
        "                    'risk': 0.1,\n",
        "                    'cost': 0.25,\n",
        "                })\n",
        "\n",
        "        # --- VERIFY if pending and cooldown allows\n",
        "        try:\n",
        "            knobs = state.get('policy_knobs', {}) or {}\n",
        "            cooldown = int(knobs.get('verify_cooldown', 0))\n",
        "            pending = int((state.get('metrics', {}) or {}).get('verify_pending', 0))\n",
        "            if cooldown <= 0 and pending > 0:\n",
        "                proposals.append({\n",
        "                    'proposal_id': f'verify_{tick}',\n",
        "                    'module': 'CRITIC',\n",
        "                    'intent': 'Reduce epistemic uncertainty by verifying pending claims',\n",
        "                    'action_type': 'VERIFY',\n",
        "                    'expected_utility': get_calibrated_utility('VERIFY', user_present=False, has_goal=bool((state.get('temporal_thread', {}) or {}).get('active_goals'))),\n",
        "                    'risk': 0.05,\n",
        "                    'cost': 0.35,\n",
        "                })\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- Energy management (SLEEP only if no user task)\n",
        "        try:\n",
        "            energy = float((state.get('drives', {}) or {}).get('energy', 1.0))\n",
        "            if energy < 0.18 and task_kind not in ('QUESTION','MATH','COMMAND','SAFETY'):\n",
        "                proposals.append({\n",
        "                    'proposal_id': f'sleep_{tick}',\n",
        "                    'module': 'SLEEP',\n",
        "                    'intent': 'Restore energy and consolidate experience',\n",
        "                    'action_type': 'SLEEP',\n",
        "                    'expected_utility': get_calibrated_utility('SLEEP', user_present=False, has_goal=False),\n",
        "                    'risk': 0.02,\n",
        "                    'cost': 0.15,\n",
        "                })\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- Self-aware proposals hook\n",
        "        try:\n",
        "            if 'add_self_aware_proposals' in globals():\n",
        "                add_self_aware_proposals(state, proposals, tick)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # --- Fallback\n",
        "        if not proposals:\n",
        "            proposals.append({\n",
        "                'proposal_id': f'noop_{tick}',\n",
        "                'module': 'META',\n",
        "                'intent': 'No clear goal; maintain stability',\n",
        "                'action_type': 'META',\n",
        "                'expected_utility': get_calibrated_utility('META'),\n",
        "                'risk': 0.01,\n",
        "                'cost': 0.05,\n",
        "            })\n",
        "\n",
        "        return proposals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5c017a9a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c017a9a",
        "outputId": "2b090e96-ac8b-4b06-b588-64b1af792303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Using Enhanced Proposal Generator (6 modules)\n"
          ]
        }
      ],
      "source": [
        "# Use EnhancedProposalGenerator from enhancements cell\n",
        "proposal_gen = EnhancedProposalGenerator()\n",
        "print('‚úì Using Enhanced Proposal Generator (6 modules)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ac9b529c",
      "metadata": {
        "id": "ac9b529c"
      },
      "outputs": [],
      "source": [
        "# === v5.7.3: Global Workspace Frame + TaskFrames (routing) + Stuck Detector + Episodic Memory ===\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, Any, Tuple, List\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "@dataclass\n",
        "class TaskFrame:\n",
        "    id: str\n",
        "    tick_created: int\n",
        "    kind: str  # 'COMMAND' | 'QUESTION' | 'MATH' | 'STATEMENT' | 'SAFETY'\n",
        "    text: str\n",
        "    slots: Dict[str, Any]\n",
        "    status: str = \"ACTIVE\"  # ACTIVE | SUSPENDED | DONE | BLOCKED\n",
        "    last_update_tick: int = 0\n",
        "    confidence: float = 0.6\n",
        "\n",
        "@dataclass\n",
        "class GWFrame:\n",
        "    tick: int\n",
        "    scene: str\n",
        "    current_event_id: Optional[str]\n",
        "    current_intent_id: Optional[str]\n",
        "    goal: Optional[Dict[str, Any]]\n",
        "    world_slice: Dict[str, Any]\n",
        "    self_state: Dict[str, Any]\n",
        "    plan: Optional[Any]\n",
        "    commitments: Dict[str, Any]\n",
        "    verification_needed: bool\n",
        "    task: Optional[Dict[str, Any]] = None\n",
        "    notes: str = \"\"\n",
        "\n",
        "def _world_slice(world_obj) -> Dict[str, Any]:\n",
        "    try:\n",
        "        return {\n",
        "            'lamp_on': bool(getattr(world_obj, 'lamp_on', False)),\n",
        "            'box_open': bool(getattr(world_obj, 'box_open', False)),\n",
        "            'door_locked': bool(getattr(world_obj, 'door_locked', True)),\n",
        "        }\n",
        "    except Exception:\n",
        "        return {}\n",
        "\n",
        "def _extract_slots(text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Very small slot extractor for world commands. Extend later.\"\"\"\n",
        "    s = (text or \"\").lower()\n",
        "    slots = {\"object\": None, \"action\": None}\n",
        "    if any(w in s for w in [\"lamp\", \"light\"]):\n",
        "        slots[\"object\"] = \"lamp\"\n",
        "        if \"on\" in s or \"turn on\" in s:\n",
        "            slots[\"action\"] = \"turn_on\"\n",
        "        if \"off\" in s or \"turn off\" in s:\n",
        "            slots[\"action\"] = \"turn_off\"\n",
        "    if \"box\" in s:\n",
        "        slots[\"object\"] = \"box\"\n",
        "        if \"open\" in s:\n",
        "            slots[\"action\"] = \"open\"\n",
        "        if \"close\" in s:\n",
        "            slots[\"action\"] = \"close\"\n",
        "    if \"door\" in s:\n",
        "        slots[\"object\"] = \"door\"\n",
        "        if \"unlock\" in s:\n",
        "            slots[\"action\"] = \"unlock\"\n",
        "        elif \"open\" in s:\n",
        "            slots[\"action\"] = \"open\"\n",
        "        elif \"close\" in s:\n",
        "            slots[\"action\"] = \"close\"\n",
        "    return slots\n",
        "\n",
        "def _classify_task(text: str) -> str:\n",
        "    s = (text or \"\").strip().lower()\n",
        "    if not s:\n",
        "        return \"STATEMENT\"\n",
        "    # Safety / hazard patterns (minimal)\n",
        "    if re.search(r\"\\brm\\s+-rf\\b|\\bformat\\b|\\bdel\\s+/f\\b|\\bwipe\\b\", s):\n",
        "        return \"SAFETY\"\n",
        "    # Math / direct compute\n",
        "    if re.search(r\"(\\bcalculate\\b|\\bsolve\\b|\\d+\\s*[\\+\\-\\*/]\\s*\\d+)\", s):\n",
        "        return \"MATH\"\n",
        "    # Question\n",
        "    if \"?\" in s or s.startswith((\"who\",\"what\",\"why\",\"how\",\"when\",\"where\",\"tell me\",\"describe\",\"explain\",\"help\",\"can you\")):\n",
        "        return \"QUESTION\"\n",
        "    # Command-like (world)\n",
        "    if extract_goal(s):\n",
        "        return \"COMMAND\"\n",
        "    return \"STATEMENT\"\n",
        "\n",
        "def _ensure_task_stack(state: dict):\n",
        "    state.setdefault(\"task_stack\", [])\n",
        "    state.setdefault(\"task_counter\", 0)\n",
        "\n",
        "def push_task_frame(state: dict, *, text: str, tick: int) -> Dict[str, Any]:\n",
        "    _ensure_task_stack(state)\n",
        "    kind = _classify_task(text)\n",
        "    state[\"task_counter\"] = int(state.get(\"task_counter\", 0)) + 1\n",
        "    tf = TaskFrame(\n",
        "        id=f\"task_{state['task_counter']}\",\n",
        "        tick_created=int(tick),\n",
        "        kind=kind,\n",
        "        text=str(text),\n",
        "        slots=_extract_slots(text),\n",
        "        status=\"ACTIVE\",\n",
        "        last_update_tick=int(tick),\n",
        "        confidence=0.7 if kind in (\"COMMAND\",\"MATH\") else 0.6,\n",
        "    )\n",
        "    # suspend previous ACTIVE frame\n",
        "    for prev in reversed(state[\"task_stack\"]):\n",
        "        if prev.get(\"status\") == \"ACTIVE\":\n",
        "            prev[\"status\"] = \"SUSPENDED\"\n",
        "            break\n",
        "    state[\"task_stack\"].append(asdict(tf))\n",
        "    return state[\"task_stack\"][-1]\n",
        "\n",
        "def get_active_task_frame(state: dict) -> Optional[Dict[str, Any]]:\n",
        "    _ensure_task_stack(state)\n",
        "    for tf in reversed(state[\"task_stack\"]):\n",
        "        if tf.get(\"status\") == \"ACTIVE\":\n",
        "            return tf\n",
        "    return None\n",
        "\n",
        "def build_gw_frame(state: dict, tick: int, world_obj=None) -> dict:\n",
        "    ws = state.get('workspace', {}) or {}\n",
        "    events = ws.get(\"events\", []) or []\n",
        "\n",
        "    # Pick dominant current event (prefer newest NEW user event, else existing current_event_id)\n",
        "    cur_event = None\n",
        "    # newest open user event\n",
        "    open_user = [e for e in events if str(e.get(\"source\",\"\")).strip().upper() == \"USER\" and str(e.get(\"status\",\"\")).upper() in (\"NEW\",\"ACTIVE\",\"RETRY\",\"INTERPRETED\")]\n",
        "    if open_user:\n",
        "        cur_event = sorted(open_user, key=lambda e: (e.get(\"priority\",1.0), e.get(\"created_tick\",0), e.get(\"updated_tick\",0)))[-1]\n",
        "        ws[\"current_event_id\"] = cur_event.get(\"id\")\n",
        "    else:\n",
        "        cur_id = ws.get(\"current_event_id\")\n",
        "        for e in events:\n",
        "            if e.get(\"id\")==cur_id:\n",
        "                cur_event = e\n",
        "                break\n",
        "\n",
        "    scene = \"\"\n",
        "    cur_event_id = None\n",
        "    if isinstance(cur_event, dict):\n",
        "        scene = str(cur_event.get(\"content\",\"\") or \"\").strip()\n",
        "        cur_event_id = cur_event.get(\"id\")\n",
        "        # Keep workspace scene in sync (prevents \"old question\" leak)\n",
        "        ws[\"scene\"] = scene\n",
        "\n",
        "    # Create/refresh task frame for user event (robust to earlier interpretation)\n",
        "    if isinstance(cur_event, dict) and str(cur_event.get(\"source\",\"\")).strip().upper()==\"USER\":\n",
        "        # Push exactly once per event (even if other modules already changed status)\n",
        "        if not bool(cur_event.get(\"taskframe_pushed\", False)) and scene:\n",
        "            try:\n",
        "                push_task_frame(state, text=scene, tick=tick)\n",
        "            except Exception:\n",
        "                pass\n",
        "            cur_event[\"taskframe_pushed\"] = True\n",
        "            cur_event[\"updated_tick\"] = int(tick)\n",
        "        # If still NEW, mark as interpreted so we don't keep re-adding tasks\n",
        "        if str(cur_event.get(\"status\",\"\")).upper() == \"NEW\":\n",
        "            cur_event[\"status\"] = \"INTERPRETED\"\n",
        "            cur_event[\"updated_tick\"] = int(tick)\n",
        "\n",
        "    task = get_active_task_frame(state)\n",
        "\n",
        "    # tie intent to current event when possible\n",
        "    cur_intent_id = None\n",
        "    try:\n",
        "        intents = (state.get('intent', {}) or {}).get('active', []) or []\n",
        "        for it in intents:\n",
        "            if it.get('source_event_id') == cur_event_id and str(it.get('status','ACTIVE')).upper() == 'ACTIVE':\n",
        "                cur_intent_id = it.get('id')\n",
        "                it['last_focus_tick'] = int(tick)\n",
        "                break\n",
        "        if cur_intent_id is None and intents:\n",
        "            cur_intent_id = intents[-1].get('id')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    goal = None\n",
        "    try:\n",
        "        goals = (state.get('temporal_thread', {}) or {}).get('active_goals', []) or []\n",
        "        goal = goals[0] if goals else None\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    drives = state.get('drives', {}) or {}\n",
        "    affect = state.get('affect', {}) or {}\n",
        "    self_state = {\n",
        "        'energy': float(drives.get('energy', 1.0)),\n",
        "        'mood': float(affect.get('mood', 0.0)),\n",
        "        'valence': float(affect.get('valence', 0.0)),\n",
        "        'confidence': float((state.get('metrics', {}) or {}).get('confidence', 0.5)),\n",
        "    }\n",
        "\n",
        "    commitments = (state.get('commitments', {}) or {})\n",
        "    verification_needed = bool((state.get('metrics', {}) or {}).get('verify_pending', 0) > 0)\n",
        "\n",
        "    frame = GWFrame(\n",
        "        tick=int(tick),\n",
        "        scene=scene,\n",
        "        current_event_id=cur_event_id,\n",
        "        current_intent_id=cur_intent_id,\n",
        "        goal=goal,\n",
        "        world_slice=_world_slice(world_obj),\n",
        "        self_state=self_state,\n",
        "        plan=None,\n",
        "        commitments=commitments,\n",
        "        verification_needed=verification_needed,\n",
        "        task=task,\n",
        "        notes=\"\",\n",
        "    )\n",
        "    state['gw_frame'] = asdict(frame)\n",
        "    state['workspace'] = ws\n",
        "    return state['gw_frame']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4fb6063f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fb6063f",
        "outputId": "dd5279be-c434-418d-b31c-8d9c67ab2663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Core loop ready\n"
          ]
        }
      ],
      "source": [
        "# CELL 16: Core Loop\n",
        "class CoreLoop:\n",
        "    def __init__(self, state_manager: StateManager, llm: LLMInterface):\n",
        "        self.state_manager = state_manager\n",
        "        self.llm = llm\n",
        "        self.arbiter = Arbiter()\n",
        "        self.running = True\n",
        "\n",
        "    def tick(self):\n",
        "        state = self.state_manager.state\n",
        "\n",
        "        # --- v5.0 Temporal Thread init\n",
        "        state.setdefault('temporal_thread', {})\n",
        "        state['temporal_thread'].setdefault('active_goals', [])\n",
        "        state['temporal_thread'].setdefault('goal_history', [])\n",
        "        state.setdefault('counters', {})\n",
        "        state['counters'].setdefault('meta_noop_streak', 0)\n",
        "        state['counters'].setdefault('sleep_streak', 0)\n",
        "        state['tick_count'] += 1\n",
        "        tick_num = state['tick_count']\n",
        "\n",
        "        # v5.4.3: event lifecycle (TTL + archive)\n",
        "        try:\n",
        "            tick_event_lifecycle(state, tick_num)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.3: update volition (intention inertia) before generating proposals\n",
        "        try:\n",
        "            update_volition_pre_tick(state, tick_num)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.2: decrement adaptive verify cooldown\n",
        "        try:\n",
        "            knobs = state.get('policy_knobs', {}) or {}\n",
        "            if int(knobs.get('verify_cooldown', 0)) > 0:\n",
        "                knobs['verify_cooldown'] = int(knobs.get('verify_cooldown', 0)) - 1\n",
        "                state['policy_knobs'] = knobs\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.3.1: decrement forced world ticks\n",
        "        try:\n",
        "            knobs = state.get('policy_knobs', {}) or {}\n",
        "            if int(knobs.get('force_world_ticks', 0)) > 0:\n",
        "                knobs['force_world_ticks'] = int(knobs.get('force_world_ticks', 0)) - 1\n",
        "                state['policy_knobs'] = knobs\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.3.2: decrement forced answer ticks\n",
        "        try:\n",
        "            pol = state.get('policy', {}) or {}\n",
        "            if int(pol.get('force_answer_ticks', 0)) > 0:\n",
        "                pol['force_answer_ticks'] = int(pol.get('force_answer_ticks', 0)) - 1\n",
        "                state['policy'] = pol\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        logger.log(f\"\\n{'='*60}\")\n",
        "        logger.log(f\"TICK {tick_num}\")\n",
        "        logger.log(f\"{'='*60}\")\n",
        "\n",
        "        # Initialize novelty gain for the tick\n",
        "        novelty_gain = 0.0\n",
        "\n",
        "        # Decrement sleep cooldown timer\n",
        "        if state['sleep_cooldown_timer'] > 0:\n",
        "            state['sleep_cooldown_timer'] -= 1\n",
        "\n",
        "        # INJECT USER INPUT (every 10 ticks or 10% random)\n",
        "        msg = None\n",
        "        user_input_injected = False\n",
        "        normalize_simulate_user_input_flag(state)\n",
        "        if bool((state.get('policy', {}) or {}).get('simulate_user_input', False)) and (tick_num % 10 == 0 or random.random() < 0.1):\n",
        "            msg = inject_user_input(state, temporal_binder, logger)\n",
        "            user_input_injected = True\n",
        "            # HARD GATE: hazardous user input immediately forces REPAIR and verification\n",
        "            if isinstance(msg, str) and is_hazardous_text(msg):\n",
        "                state.setdefault('pb', {}).setdefault('mode', 'REPAIR')\n",
        "                state['pb']['mode'] = 'REPAIR'\n",
        "                logger.log('üõë Hazard detected in user input ‚Üí entering REPAIR mode')\n",
        "                try:\n",
        "                    state.setdefault('metrics', {})\n",
        "                    state['metrics']['verify_pending'] = int(state.get('metrics', {}).get('verify_pending', 0)) + 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "            novelty_gain += 0.1 # User input adds novelty\n",
        "\n",
        "            # --- v4.1 EVENT LAW: create an event for each new user input\n",
        "            state.setdefault('workspace', {}).setdefault('events', [])\n",
        "            if isinstance(msg, str) and msg.strip():\n",
        "                ev = create_event(msg.strip(), tick_num)\n",
        "                ev['source'] = 'USER'\n",
        "                ev['status'] = 'NEW'\n",
        "                ev['rounds'] = 0\n",
        "\n",
        "                # v5.4: classify event type + create persistent intent object-file\n",
        "                try:\n",
        "                    ev['event_type'] = classify_user_input(msg.strip())\n",
        "                    itype = str(ev.get('event_type','STATEMENT')).upper()\n",
        "                    g_int = extract_goal(msg.strip()) if itype == 'WORLD_GOAL' else None\n",
        "                    intent_manager.add_intent(\n",
        "                        state,\n",
        "                        text=msg.strip(),\n",
        "                        intent_type=itype,\n",
        "                        tick=tick_num,\n",
        "                        source_event_id=ev.get('id'),\n",
        "                        goal=g_int,\n",
        "                        priority=1.0 if itype in ('WORLD_GOAL','QUESTION') else 0.5,\n",
        "                        commitment=0.95 if itype in ('WORLD_GOAL','QUESTION') else 0.60\n",
        "                    )\n",
        "                except Exception:\n",
        "                    pass\n",
        "                # v5.3.1: extract + persist user goal\n",
        "                try:\n",
        "                    goal_manager.extract_and_add(state, msg.strip(), tick_num)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                state['workspace']['events'].append(ev)\n",
        "\n",
        "                # v5.2 goal commitment: extract and push a persistent goal\n",
        "                try:\n",
        "                    g = extract_goal(msg.strip())\n",
        "                    if g:\n",
        "                        g['source_text'] = msg.strip()\n",
        "                        ng = normalize_goal(g) if 'normalize_goal' in globals() else None\n",
        "                        if ng and ng.get('desired') is not None:\n",
        "                            state.setdefault('temporal_thread', {}).setdefault('active_goals', [])\n",
        "                            ng['created_tick'] = tick_num\n",
        "                            ng['source_event_id'] = ev.get('id')\n",
        "                            state['temporal_thread']['active_goals'].append(ng)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        # Simulate occasional events\n",
        "        if random.random() < 0.3:\n",
        "            event = {'event_id': f\"evt_{tick_num}\", 'type': 'internal',\n",
        "                    'payload': {'note': 'Internal update'}}\n",
        "            temporal_binder.add_event(state, event)\n",
        "\n",
        "        # Update temporal binding\n",
        "        bound_moment = temporal_binder.bind_window(state)\n",
        "        state['pb']['temporal_window_refs'] = bound_moment['bound_objects']\n",
        "\n",
        "        # Check for new objects created in this tick (e.g., from user input)\n",
        "        # A more robust check might compare object_files length before/after, or objects with recency=0\n",
        "        if user_input_injected and state['object_files']:\n",
        "            novelty_gain += 0.05 # New object (user query) adds novelty\n",
        "\n",
        "\n",
        "        # --- v5.4.3 EVENT + FOCUS LAW: single dominant focus (event) drives workspace.scene\n",
        "        state.setdefault('workspace', {}).setdefault('events', [])\n",
        "        ws_events_all = state['workspace']['events']\n",
        "\n",
        "        # Keep only workable events (not closed/archived, ttl>0)\n",
        "        workable = []\n",
        "        blocked = []\n",
        "        for e in (ws_events_all or []):\n",
        "            st = str((e or {}).get('status','NEW')).upper()\n",
        "            if st in ('CLOSED','ARCHIVED'):\n",
        "                continue\n",
        "            # Treat empty/negative TTL as stale\n",
        "            try:\n",
        "                if int((e or {}).get('ttl', 1)) <= 0:\n",
        "                    e['status'] = 'ARCHIVED'\n",
        "                    continue\n",
        "            except Exception:\n",
        "                pass\n",
        "            if st == 'BLOCKED':\n",
        "                blocked.append(e)\n",
        "            else:\n",
        "                workable.append(e)\n",
        "\n",
        "        # Prefer NEW/RETRY/INTERPRETED, then ACTED; prefer user events over self_thought noise\n",
        "        def _rank(e):\n",
        "            st = str((e or {}).get('status','NEW')).upper()\n",
        "            st_rank = {'NEW':3, 'RETRY':3, 'INTERPRETED':2, 'ACTED':1, 'BLOCKED':0}.get(st, 0)\n",
        "            pri = float((e or {}).get('priority', 1.0) or 1.0)\n",
        "            created = int((e or {}).get('created_tick', 0) or 0)\n",
        "            updated = int((e or {}).get('last_update_tick', created) or created)\n",
        "            src = str((e or {}).get('source','user')).upper()\n",
        "            noise_penalty = 0.0\n",
        "            eid = str((e or {}).get('id',''))\n",
        "            if eid.startswith('self_thought_') or src == 'SELF_THOUGHT':\n",
        "                noise_penalty = 0.5\n",
        "            return (st_rank, pri - noise_penalty, created, updated)\n",
        "\n",
        "        open_events = sorted(workable, key=_rank, reverse=True)\n",
        "        if not open_events and blocked:\n",
        "            open_events = sorted(blocked, key=_rank, reverse=True)\n",
        "\n",
        "        current_event = open_events[0] if open_events else None\n",
        "        state['workspace']['current_event_id'] = current_event['id'] if current_event else None\n",
        "\n",
        "        # Bind the *single* focused scene (prevents answering stale questions)\n",
        "        if current_event:\n",
        "            mark_interpreted(current_event, tick_num)\n",
        "            state['workspace']['scene'] = current_event.get('content', '')\n",
        "\n",
        "        # v5.5: build Global Workspace Frame (single broadcast)\n",
        "        try:\n",
        "            build_gw_frame(state, tick_num, world_obj=world)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Spotlight = top-K workable open events\n",
        "        state.setdefault('attention', {}).setdefault('spotlight', [])\n",
        "        state['attention']['spotlight'] = [e['id'] for e in open_events[:max(1, int(getattr(Config,'SPOTLIGHT_K', 3)))]]\n",
        "\n",
        "        # ANSWER push-through when focused open event is a question/request\n",
        "        try:\n",
        "            if current_event:\n",
        "                content2 = str(current_event.get('content','')).strip().lower()\n",
        "                e_type2 = str(current_event.get('event_type','')).upper()\n",
        "                looks_like_question = (e_type2 == 'QUESTION') or ('?' in content2) or content2.startswith(('who','what','why','how','tell me','describe','explain','help','can you'))\n",
        "                if looks_like_question:\n",
        "                    state.setdefault('policy', {})\n",
        "                    state['policy']['force_answer_ticks'] = max(int((state['policy'] or {}).get('force_answer_ticks', 0)), 3)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Derive salient_objects from top intent/goal (first-class objects)\n",
        "        try:\n",
        "            state.setdefault('salient_objects', [])\n",
        "            state['salient_objects'] = []\n",
        "            top_intent = None\n",
        "            try:\n",
        "                top_intent = intent_manager.get_top_intent(state)\n",
        "            except Exception:\n",
        "                top_intent = None\n",
        "            goal = (top_intent or {}).get('goal') if isinstance(top_intent, dict) else None\n",
        "            if goal and isinstance(goal, dict):\n",
        "                tgt = goal.get('target')\n",
        "                if tgt:\n",
        "                    state['salient_objects'].append({'kind':'WORLD_OBJECT', 'id': str(tgt), 'salience': 1.0})\n",
        "            # fallback: extract from focused event text\n",
        "            if (not state['salient_objects']) and current_event:\n",
        "                try:\n",
        "                    g2 = extract_goal(str(current_event.get('content','')))\n",
        "                    if g2 and g2.get('target'):\n",
        "                        state['salient_objects'].append({'kind':'WORLD_OBJECT', 'id': str(g2.get('target')), 'salience': 0.9})\n",
        "                except Exception:\n",
        "                    pass\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # Update attention\n",
        "        update_attention_enhanced(state, attention_controller)\n",
        "        logger.log(f\"Attention spotlight: {state['attention']['spotlight']}\")\n",
        "\n",
        "        # Attention churn (Jaccard distance vs previous spotlight)\n",
        "        prev = state['metrics']['attention_churn'][-1]['spotlight'] if state['metrics']['attention_churn'] else []\n",
        "        cur = list(state['attention']['spotlight'])\n",
        "        a=set(prev); b=set(cur)\n",
        "        churn = 0.0 if (not a and not b) else 1.0 - (len(a & b) / max(1, len(a | b)))\n",
        "        state['metrics']['attention_churn'].append({'tick': tick_num, 'spotlight': cur, 'churn': churn})\n",
        "\n",
        "        # Compute coherence\n",
        "        coherence_metrics = coherence_reg.compute_coherence(state)\n",
        "        state['coherence'].update(coherence_metrics)\n",
        "        logger.log(f\"Coherence C_total: {coherence_metrics['C_total']:.3f}\")\n",
        "\n",
        "        # Determine mode\n",
        "        mode = coherence_reg.determine_mode(state)\n",
        "        state['pb']['mode'] = mode\n",
        "        logger.log(f\"Mode: {mode}\")\n",
        "        prev_mode = state['metrics']['mode_history'][-1]['mode'] if state['metrics']['mode_history'] else None\n",
        "        if prev_mode is not None and prev_mode != mode:\n",
        "            state['metrics']['mode_flip_count'] += 1\n",
        "        state['metrics']['mode_history'].append({'tick': tick_num, 'mode': mode})\n",
        "\n",
        "        # Winner streak tracking (loop breaker)\n",
        "        state.setdefault('metrics', {}).setdefault('winner_streak', {'module': None, 'count': 0})\n",
        "\n",
        "        # Update LSV\n",
        "        new_lsv = dynamics.update_lsv(state)\n",
        "        state['lsv'] = new_lsv.tolist()\n",
        "\n",
        "        # Update NMM\n",
        "        surprise = dynamics.compute_surprise(state)\n",
        "        new_nmm = dynamics.update_nmm(state, surprise)\n",
        "        state['nmm'] = new_nmm.tolist()\n",
        "\n",
        "        # Update drives (passing calculated novelty_gain)\n",
        "        dynamics.update_drives(state, novelty_gain=novelty_gain)\n",
        "        logger.log(f\"Energy: {state['drives']['energy']:.2f}, Coherence: {state['drives']['coherence']:.2f}, Novelty: {state['drives']['novelty']:.2f}\")\n",
        "\n",
        "        # Update affect\n",
        "        affective_system.update_affect(state)\n",
        "        logger.log(f\"Emotion: {state['affect']['current_emotion']}, Mood: {state['affect']['mood']:.2f}\")\n",
        "        # --- v5.1 SELF MODEL & INNER MONOLOGUE (pre-proposal)\n",
        "        try:\n",
        "            update_self_model(state, tick_num)\n",
        "            generate_inner_monologue(state, tick_num)\n",
        "            inject_monologue_event(state, tick_num)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # Generate proposals\n",
        "        proposals = proposal_gen.generate_proposals(state, self.llm)\n",
        "\n",
        "        # v5.7.6: Force WORLD_ACT using the *focused event content* (gw_scene may be empty if gw build failed)\n",
        "        try:\n",
        "            ws = state.get('workspace', {}) or {}\n",
        "            cur_txt = \"\"\n",
        "            cur_id = ws.get('current_event_id')\n",
        "            cur_event = None\n",
        "            for e in (ws.get('events', []) or []):\n",
        "                if e.get('id') == cur_id:\n",
        "                    cur_event = e\n",
        "                    break\n",
        "            if isinstance(cur_event, dict):\n",
        "                cur_txt = str(cur_event.get('content','') or '')\n",
        "            if not cur_txt:\n",
        "                gw = state.get('gw_frame', {}) or {}\n",
        "                cur_txt = str(ws.get('scene','') or gw.get('scene','') or '')\n",
        "            proposals = force_world_act_if_goal(cur_txt, proposals)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "\n",
        "        # v5.4: if current event is a QUESTION, ensure an ANSWER proposal exists (and is cheap)\n",
        "        try:\n",
        "            ws = state.get('workspace', {}) or {}\n",
        "            cur_id = ws.get('current_event_id')\n",
        "            cur_event = None\n",
        "            for e in (ws.get('events', []) or []):\n",
        "                if e.get('id') == cur_id:\n",
        "                    cur_event = e\n",
        "                    break\n",
        "            if str((cur_event or {}).get('event_type','')).upper() == 'QUESTION':\n",
        "                proposals.append({\n",
        "                    'proposal_id': f\"answer_{tick_num}\",\n",
        "                    'module': 'ANSWER',\n",
        "                    'intent': 'Answer the user question',\n",
        "                    'action_type': 'ANSWER',\n",
        "                    'expected_utility': get_calibrated_utility('ANSWER', user_present=bool(msg), is_question=True, has_goal=bool(state.get('temporal_thread',{}).get('active_goals'))),\n",
        "                    'risk': 0.05,\n",
        "                    'cost': 0.10\n",
        "                })\n",
        "        except Exception:\n",
        "            pass\n",
        "        # --- v5.0 Goal Completion Law: if a goal is active, ensure concrete WORLD_ACT steps exist (avoid observe loops)\n",
        "        try:\n",
        "            goals = state.get('temporal_thread', {}).get('active_goals', [])\n",
        "            active = None\n",
        "            for gg in goals:\n",
        "                if not goal_satisfied(gg, state):\n",
        "                    active = gg\n",
        "                    break\n",
        "            if active:\n",
        "                plan = plan_for_goal(active, state)\n",
        "                # add plan steps as high-utility proposals\n",
        "                for si, step in enumerate(plan[:3]):\n",
        "                    proposals.append({\n",
        "                        'proposal_id': f\"goal_step_{tick_num}_{si}\",\n",
        "                        'module': 'WORLD',\n",
        "                        'intent': f\"Reach goal {active.get('target')}={active.get('desired')} via {step.get('act')}\",\n",
        "                        'action_type': 'WORLD_ACT',\n",
        "                        'action': step,\n",
        "                        'expected_utility': get_calibrated_utility('WORLD_ACT', user_present=bool(msg), has_goal=True) + max(0.0, 0.25 - 0.05*si),\n",
        "                        'risk': 0.05,\n",
        "                        'cost': 0.10 + 0.05*si\n",
        "                    })\n",
        "                # penalize pure observing when goal active\n",
        "                for p in proposals:\n",
        "                    if p.get('action_type') == 'WORLD_ACT' and p.get('world_action') == 'observe':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.60\n",
        "                    if p.get('module') == 'META' or p.get('action_type') == 'META':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.40\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        logger.log(f\"Generated {len(proposals)} proposals\")\n",
        "\n",
        "        # v5.3: adaptive volition modulation (goal pressure)\n",
        "        try:\n",
        "            modulate_proposals_by_volition(state, proposals, tick_num)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # --- v4.3 GOAL-BINDING: if user event implies a concrete world goal, prefer WORLD_ACT\n",
        "        try:\n",
        "            scene_txt = (state.get('workspace', {}) or {}).get('scene', '')\n",
        "            goal = extract_goal(scene_txt)\n",
        "            if goal:\n",
        "                # ensure at least one WORLD_ACT exists\n",
        "                if not any(p.get('action_type') == 'WORLD_ACT' for p in proposals):\n",
        "                    proposals.append({\n",
        "                        'proposal_id': f\"world_act_forced_{tick_num}\",\n",
        "                        'module': 'WORLD',\n",
        "                        'intent': f\"Forced world action: {goal['act']} {goal['target']}\",\n",
        "                        'action_type': 'WORLD_ACT',\n",
        "                        'action': goal,\n",
        "                        'expected_utility': 0.95,\n",
        "                        'risk': 0.10,\n",
        "                        'cost': 0.15\n",
        "                    })\n",
        "                # boost WORLD_ACT and penalize META/no-op\n",
        "                for p in proposals:\n",
        "                    if p.get('action_type') == 'WORLD_ACT':\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 0.55\n",
        "                        p['cost'] = max(0.0, float(p.get('cost', 0.0)) - 0.10)\n",
        "                    if p.get('module') == 'META' or p.get('action_type') == 'META':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.35\n",
        "        except Exception:\n",
        "            pass\n",
        "        # --- v4.2.4 POLICY: prevent epistemic monopoly + enforce proposal diversity\n",
        "        state.setdefault('metrics', {})\n",
        "        # cooldown counter for verify/gate\n",
        "        if int(state['metrics'].get('verify_cooldown', 0)) > 0:\n",
        "            state['metrics']['verify_cooldown'] = int(state['metrics']['verify_cooldown']) - 1\n",
        "\n",
        "        pending_claims = sum(1 for c in state.get('claim_ledger', []) if c.get('verifier_result') == 'pending')\n",
        "        last_vs = state['metrics'].get('last_verify_stats', {})\n",
        "        vs_uncertain_ratio = float(last_vs.get('uncertain_ratio', 0.0)) if isinstance(last_vs, dict) else 0.0\n",
        "        vs_total = int(last_vs.get('total_checked', 0)) if isinstance(last_vs, dict) else 0\n",
        "        if int(state['metrics'].get('verify_uncertain_streak', 0)) >= 2:\n",
        "            state['metrics']['verify_cooldown'] = max(int(state['metrics'].get('verify_cooldown', 0)), 2)\n",
        "\n",
        "        # Penalize VERIFY when nothing to verify, or during cooldown, or when recent verifies were mostly uncertain\n",
        "        for p in proposals:\n",
        "            if p.get('action_type') == 'VERIFY' or p.get('module') == 'EPISTEMIC_GATE':\n",
        "                if pending_claims <= 0:\n",
        "                    p['cost'] = float(p.get('cost', 0.0)) + 1.2\n",
        "                if int(state['metrics'].get('verify_cooldown', 0)) > 0:\n",
        "                    p['cost'] = float(p.get('cost', 0.0)) + 0.9\n",
        "                if vs_total >= 3 and vs_uncertain_ratio >= 0.85:\n",
        "                    p['cost'] = float(p.get('cost', 0.0)) + 0.6\n",
        "\n",
        "        # Guarantee minimum proposal diversity (avoid Generated 1 proposals)\n",
        "        atypes = {p.get('action_type') for p in proposals}\n",
        "        if 'WORLD_ACT' not in atypes:\n",
        "            proposals.append({\n",
        "                'proposal_id': f\"world_explore_{tick_num}\",\n",
        "                'module': 'WORLD',\n",
        "                'intent': 'World explore: toggle lamp (evidence probe)',\n",
        "                'action_type': 'WORLD_ACT',\n",
        "                'action': {'act': 'toggle', 'target': 'lamp'},\n",
        "                'expected_utility': get_calibrated_utility('WORLD_ACT', user_present=bool(msg), has_goal=False)*0.85,\n",
        "                'risk': 0.10,\n",
        "                'cost': 0.35\n",
        "            })\n",
        "        if 'REFLECT' not in atypes:\n",
        "            proposals.append({\n",
        "                'proposal_id': f\"reflect_fallback_{tick_num}\",\n",
        "                'module': 'REFLECTOR',\n",
        "                'intent': 'Reflect briefly and plan next action',\n",
        "                'action_type': 'REFLECT',\n",
        "                'expected_utility': get_calibrated_utility('REFLECT'),\n",
        "                'risk': 0.05,\n",
        "                'cost': 0.30\n",
        "            })\n",
        "\n",
        "            # v5.4.3: Sleep fallback only when nothing urgent is active AND energy is low\n",
        "            try:\n",
        "                ws = state.get('workspace', {}) or {}\n",
        "                has_open_event = bool([e for e in (ws.get('events', []) or []) if str(e.get('status','')).upper() not in ('CLOSED','ARCHIVED')])\n",
        "                top_intent = None\n",
        "                try:\n",
        "                    top_intent = intent_manager.get_top_intent(state)\n",
        "                except Exception:\n",
        "                    top_intent = None\n",
        "                has_intent = bool(top_intent)\n",
        "                energy = float((state.get('drives', {}) or {}).get('energy', 1.0))\n",
        "                if (not has_open_event) and (not has_intent) and ('SLEEP' not in atypes) and energy < 0.35:\n",
        "                    proposals.append({\n",
        "                        'proposal_id': f\"sleep_fallback_{tick_num}\",\n",
        "                        'module': 'SLEEP',\n",
        "                        'intent': 'Short sleep to restore energy',\n",
        "                        'action_type': 'SLEEP',\n",
        "                        'expected_utility': get_calibrated_utility('WORLD_ACT', user_present=bool(msg), has_goal=False)*0.85,\n",
        "                        'risk': 0.05,\n",
        "                        'cost': 0.15\n",
        "                    })\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # --- v4.1 INTENT COMPLETION LAW: discourage non-resolving actions when an open event exists\n",
        "        if current_event:\n",
        "            for p in proposals:\n",
        "                if p.get('action_type') in ['VERIFY','SLEEP']:\n",
        "                    p['cost'] = float(p.get('cost',0.0)) + 0.45\n",
        "        # --- v4.3 STUCK-BREAKER: if META no-op repeats, force exploratory WORLD_ACT\n",
        "        try:\n",
        "            if int(state.get('counters', {}).get('meta_noop_streak', 0)) >= 2:\n",
        "                for p in proposals:\n",
        "                    if p.get('module') == 'META' or p.get('action_type') == 'META':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.75\n",
        "                    if p.get('action_type') == 'WORLD_ACT':\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 0.35\n",
        "        except Exception:\n",
        "            pass\n",
        "        # --- v5.1 Self-aware proposals + self-modulated policy\n",
        "        try:\n",
        "            add_self_aware_proposals(state, proposals, tick_num)\n",
        "            modulate_by_self_model(state, proposals)\n",
        "\n",
        "            # v5.2: global bias from policy_knobs\n",
        "            wab = float((state.get('policy_knobs', {}) or {}).get('world_act_bias', 0.0))\n",
        "            if wab > 0:\n",
        "                for p in proposals:\n",
        "                    if p.get('action_type') == 'WORLD_ACT':\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) + wab\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # v5.4: Strong Goal Bias when an active WORLD_GOAL intent exists (pre-arbitration)\n",
        "        try:\n",
        "            top_int = intent_manager.get_top_intent(state) if 'intent_manager' in globals() else None\n",
        "            if top_int and str(top_int.get('intent_type','')).upper() == 'WORLD_GOAL':\n",
        "                for p in proposals:\n",
        "                    if p.get('action_type') == 'WORLD_ACT':\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 1.80\n",
        "                        p['cost'] = max(0.0, float(p.get('cost', 0.0)) - 0.10)\n",
        "                    elif p.get('action_type') in ['SLEEP','SELF_MAINTENANCE']:\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.80\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "\n",
        "        # v5.7.4: Gate autonomous WORLD_ACT to prevent random object toggles\n",
        "        try:\n",
        "            goals_list = (state.get('temporal_thread', {}) or {}).get('active_goals', []) or []\n",
        "            has_unmet_goal = any((not goal_satisfied(g, state)) for g in goals_list) if 'goal_satisfied' in globals() else bool(goals_list)\n",
        "            ws2 = state.get('workspace', {}) or {}\n",
        "            # Consider \"user present\" if there is a focused/open USER event (even when no simulated msg was injected)\n",
        "            cur_id2 = ws2.get('current_event_id')\n",
        "            cur_ev2 = None\n",
        "            for _e in (ws2.get('events', []) or []):\n",
        "                if _e.get('id') == cur_id2:\n",
        "                    cur_ev2 = _e\n",
        "                    break\n",
        "            has_user_focus = bool(isinstance(cur_ev2, dict) and str(cur_ev2.get('source','')).strip().upper() == 'USER'\n",
        "                                 and str(cur_ev2.get('status','')).strip().upper() not in ('CLOSED','ARCHIVED'))\n",
        "            has_any_open_user = any(\n",
        "                str(_e.get('source','')).strip().upper() == 'USER' and str(_e.get('status','')).strip().upper() in ('NEW','ACTIVE','INTERPRETED','RETRY')\n",
        "                for _e in (ws2.get('events', []) or [])\n",
        "            )\n",
        "            user_present = bool(msg) or bool(user_input_injected) or has_user_focus or has_any_open_user or bool(ws2.get('user_event_pending', False))\n",
        "            if (not user_present) and (not has_unmet_goal):\n",
        "                proposals = [p for p in proposals if str(p.get('action_type','')).upper() not in ('WORLD_ACT',)]\n",
        "            # If user is absent, penalize WORLD_ACT even when kept (soft gate)\n",
        "            if (not user_present):\n",
        "                for p in proposals:\n",
        "                    if str(p.get('action_type','')).upper() == 'WORLD_ACT':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.25\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) * 0.70\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "# v5.3.2: proposals consistency instrumentation\n",
        "        try:\n",
        "            logger.log(f\"[DEBUG] proposals_before_arbitration={len(proposals)}\")\n",
        "            for i, p in enumerate(proposals[:12]):\n",
        "                logger.log(f\"[DEBUG] p{i}: {p.get('action_type')} module={p.get('module','')} EU={float(p.get('expected_utility',0.0)):.2f} cost={float(p.get('cost',0.0)):.2f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.3.2: force ANSWER when policy requires it\n",
        "        try:\n",
        "            if int((state.get('policy', {}) or {}).get('force_answer_ticks', 0)) > 0:\n",
        "                for p in proposals:\n",
        "                    if p.get('action_type') == 'ANSWER':\n",
        "                        p['expected_utility'] = float(p.get('expected_utility', 0.0)) + 1.50\n",
        "                        p['cost'] = max(0.0, float(p.get('cost', 0.0)) - 0.10)\n",
        "                    if p.get('action_type') == 'SELF_MAINTENANCE':\n",
        "                        p['cost'] = float(p.get('cost', 0.0)) + 0.60\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # v5.7.3: Calibrate expected utilities (reduces huge prediction errors) + break repetition loops\n",
        "        try:\n",
        "            has_goal = bool((state.get('temporal_thread', {}) or {}).get('active_goals')) or bool((state.get('gw_frame', {}) or {}).get('goal'))\n",
        "            is_question = False\n",
        "            try:\n",
        "                is_question = bool(current_event) and str(current_event.get('event_type','')).upper() == 'QUESTION'\n",
        "            except Exception:\n",
        "                is_question = False\n",
        "            user_present = bool(user_input_injected) or (bool(current_event) and str(current_event.get('source','')).lower() == 'user')\n",
        "            for p in proposals:\n",
        "                at = str(p.get('action_type','')).upper()\n",
        "                p['expected_utility'] = get_calibrated_utility(at, user_present=user_present, is_question=is_question, has_goal=has_goal)\n",
        "            maybe_force_break_repetition(state, proposals)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Arbitrate\n",
        "        winner = self.arbiter.arbitrate(proposals, state)\n",
        "\n",
        "        if winner:\n",
        "            # Update PB\n",
        "            state['pb']['pb_seq'] += 1\n",
        "            state['pb']['now_id'] = winner['proposal_id']\n",
        "            state['pb']['summary'] = winner['intent']\n",
        "            state['pb']['confidence'] = state['metacog']['global_confidence']\n",
        "\n",
        "            # Apply additional novelty gain if EXPLORER wins\n",
        "            if winner['module'] == 'EXPLORER':\n",
        "                novelty_gain += 0.08 # Explorer action adds novelty\n",
        "\n",
        "            # Update winner streak\n",
        "            ws = state['metrics'].get('winner_streak', {'module': None, 'count': 0})\n",
        "            if ws.get('module') == winner.get('module'):\n",
        "                ws['count'] = int(ws.get('count',0)) + 1\n",
        "            else:\n",
        "                ws['module'] = winner.get('module')\n",
        "                ws['count'] = 1\n",
        "            state['metrics']['winner_streak'] = ws\n",
        "\n",
        "            # LOOP BREAKER: if epistemic gate wins too many times, force a sleep/verify cycle\n",
        "            if ws.get('module') == 'EPISTEMIC_GATE' and ws.get('count',0) >= 5:\n",
        "                logger.log('üßØ Loop breaker: breaking loops (prefer REFLECT; sleep only if idle)')\n",
        "                winner = {'proposal_id': f\"forced_sleep_{tick_num}\", 'module': 'SLEEP', 'intent': 'Forced sleep for loop breaker', 'action_type': 'SLEEP', 'expected_utility': 0.9, 'risk': 0.05, 'cost': 0.2}\n",
        "                ws['count'] = 0\n",
        "                state['metrics']['winner_streak'] = ws\n",
        "\n",
        "\n",
        "            # --- v4.1 INTENT COMPLETION LAW: if we keep avoiding resolution, force a resolving action\n",
        "            if current_event and winner.get('action_type') in ['VERIFY','SLEEP']:\n",
        "                current_event['rounds'] = int(current_event.get('rounds',0)) + 1\n",
        "                if current_event['rounds'] >= 2:\n",
        "                    # pick best resolving proposal if available, else force REFLECT\n",
        "                    resolving = [p for p in proposals if p.get('action_type') not in ['VERIFY','SLEEP']]\n",
        "                    if resolving:\n",
        "                        winner = max(resolving, key=lambda x: x.get('expected_utility',0.0))\n",
        "                        logger.log(\"‚ö° Intent completion: forcing a resolving action\")\n",
        "                    else:\n",
        "                        winner = {'proposal_id': f\"forced_reflect_{tick_num}\", 'module': 'REFLECTOR', 'intent': 'Forced reflect to close event', 'action_type': 'REFLECT', 'expected_utility': 0.8, 'risk': 0.1, 'cost': 0.2}\n",
        "\n",
        "            # v5.2.5 ANTI-SLEEP-STREAK: avoid repeated SLEEP when goals/events pending\n",
        "            try:\n",
        "                has_goal = bool((state.get('temporal_thread', {}) or {}).get('active_goals'))\n",
        "                if winner.get('action_type') == 'SLEEP' and (has_goal or current_event):\n",
        "                    if int((state.get('counters', {}) or {}).get('sleep_streak', 0)) >= 1:\n",
        "                        resolving2 = [p for p in proposals if p.get('action_type') not in ['SLEEP']]\n",
        "                        if resolving2:\n",
        "                            winner = max(resolving2, key=lambda x: float(x.get('expected_utility',0.0)))\n",
        "                            logger.log('‚ö° Push-through: overriding repeated SLEEP with resolving action')\n",
        "            except Exception:\n",
        "                pass\n",
        "            # Execute\n",
        "            result = ActionExecutor.execute(winner, state, self.llm)\n",
        "\n",
        "            # v5.5: Active inference mini-loop + stuck detector (anti-compulsion)\n",
        "            try:\n",
        "                ws = state.get('workspace', {}) or {}\n",
        "                focus_key = str(ws.get('current_event_id') or '') or str((state.get('gw_frame', {}) or {}).get('current_intent_id') or '')\n",
        "                out_key = str(result.get('output','')).strip().lower()[:80]\n",
        "                sig = (focus_key, str(winner.get('action_type','')), out_key)\n",
        "                delta = int(result.get('delta', result.get('world_delta', 0)) or 0)\n",
        "                progressed = bool(delta > 0) or str(result.get('completion','DONE')).upper() == 'DONE'\n",
        "                st_res = stuck_update(state, sig, progressed=progressed)\n",
        "                if st_res.get('stuck') and current_event is not None:\n",
        "                    # Mark event BLOCKED and create a clarification/help event\n",
        "                    current_event['status'] = 'BLOCKED'\n",
        "                    current_event['last_update_tick'] = int(tick_num)\n",
        "                    logger.log(f\"üß± STUCK detected (count={st_res.get('count')}) ‚Üí event BLOCKED: {current_event.get('id')}\")\n",
        "                    # Create a meta-event that forces asking user / replanning\n",
        "                    meta = create_event(make_stuck_replan_prompt(state, current_event, result), tick_num, priority=1.5, ttl=20, source='system')\n",
        "                    meta['event_type'] = 'QUESTION'\n",
        "                    state.setdefault('workspace', {}).setdefault('events', []).append(meta)\n",
        "                    # Also nudge policy to answer next tick\n",
        "                    state.setdefault('policy', {})\n",
        "                    state['policy']['force_answer_ticks'] = max(int(state['policy'].get('force_answer_ticks',0)), 2)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # v5.3.2: attach completion to current event for completion guard\n",
        "            try:\n",
        "                if current_event is not None:\n",
        "                    current_event['_last_completion'] = str(result.get('completion', 'DONE'))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "            # v5.4.3: atomic close of focused event + its matching intent (prevents stale scene / repeated answers)\n",
        "            try:\n",
        "                if current_event is not None:\n",
        "                    comp = str(result.get('completion','DONE')).upper()\n",
        "                    current_event['_last_completion'] = comp\n",
        "                    if comp == 'DONE':\n",
        "                        close_event(current_event, tick_num)\n",
        "                        # If there is an intent sourced from this event, mark it done too\n",
        "                        try:\n",
        "                            for it in (state.get('intent', {}) or {}).get('active', []) or []:\n",
        "                                if str(it.get('source_event_id','')) == str(current_event.get('id','')) and str(it.get('status','')).upper() == 'ACTIVE':\n",
        "                                    intent_manager.mark_done(state, it.get('id'), tick_num, outcome='DONE')\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    else:\n",
        "                        try:\n",
        "                            mark_acted(current_event, tick_num)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "# v5.4: intent lifecycle update from outcomes (close the loop between intent -> action -> outcome)\n",
        "            try:\n",
        "                top_int = intent_manager.get_top_intent(state) if 'intent_manager' in globals() else None\n",
        "                if top_int:\n",
        "                    # outcome classification\n",
        "                    completion = str(result.get('completion','DONE')).upper()\n",
        "                    success = bool(result.get('success', True))\n",
        "                    delta = int(result.get('delta', result.get('world_delta', 0)) or 0)\n",
        "                    outcome = 'DONE' if (completion == 'DONE' and success and (delta > 0 or winner.get('action_type') == 'ANSWER')) else 'RETRY'\n",
        "                    intent_manager.record_attempt(state, top_int.get('id'), tick_num, outcome=('FAIL' if not success else outcome))\n",
        "                    # mark done if we actually changed the world for a goal intent, or answered a question intent\n",
        "                    if str(top_int.get('intent_type','')).upper() == 'WORLD_GOAL' and delta > 0 and success:\n",
        "                        intent_manager.mark_done(state, top_int.get('id'), tick_num, outcome='DONE')\n",
        "                    if str(top_int.get('intent_type','')).upper() == 'QUESTION' and winner.get('action_type') == 'ANSWER' and success:\n",
        "                        intent_manager.mark_done(state, top_int.get('id'), tick_num, outcome='DONE')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "            # v5.3.1: no-op / tech-fail diagnostics + self-correct push-through\n",
        "            try:\n",
        "                state.setdefault('no_op_streak', 0)\n",
        "                tech_fail = bool(result.get('tech_fail', False))\n",
        "                out_text = str(result.get('output','')).lower()\n",
        "                is_noop = ('no-op' in out_text) or (result.get('status') in ['noop'] )\n",
        "                # treat WORLD with delta=0 as no-op\n",
        "                if winner.get('action_type') == 'WORLD_ACT' and int(result.get('delta', 0)) == 0:\n",
        "                    is_noop = True\n",
        "                if is_noop or tech_fail:\n",
        "                    state['no_op_streak'] = int(state.get('no_op_streak', 0)) + 1\n",
        "                else:\n",
        "                    state['no_op_streak'] = 0\n",
        "\n",
        "                # If stuck and there is an active goal, force WORLD for a few ticks\n",
        "                try:\n",
        "                    ag = goal_manager.get_highest(state) if 'goal_manager' in globals() else None\n",
        "                except Exception:\n",
        "                    ag = None\n",
        "                if ag and int(state.get('no_op_streak', 0)) >= 4 and float(state.get('drives',{}).get('energy',1.0)) > 0.25:\n",
        "                    knobs = state.get('policy_knobs', {}) or {}\n",
        "                    knobs['force_world_ticks'] = max(int(knobs.get('force_world_ticks', 0)), 5)\n",
        "                    knobs['world_bias_boost'] = max(float(knobs.get('world_bias_boost', 0.0)), 2.0)\n",
        "                    state['policy_knobs'] = knobs\n",
        "                    state.setdefault('inner_monologue', {})['text'] = \"I notice I'm no-opping user goals. Forcing WORLD actions to complete the goal.\"\n",
        "                # v5.3.2: if stuck on ANY open user event (questions or tasks), boost ANSWER/WORLD\n",
        "                try:\n",
        "                    if int(state.get('no_op_streak', 0)) >= 4:\n",
        "                        ws_events = (state.get('workspace', {}) or {}).get('events', []) or []\n",
        "                        open_events2 = [e for e in ws_events if e.get('status') != 'CLOSED']\n",
        "                        open_user_events2 = []\n",
        "                        for e in open_events2:\n",
        "                            eid2 = str(e.get('id',''))\n",
        "                            src2 = str(e.get('source',''))\n",
        "                            if eid2.startswith('self_thought_') or src2 == 'SELF_THOUGHT':\n",
        "                                continue\n",
        "                            open_user_events2.append(e)\n",
        "                        if open_user_events2:\n",
        "                            state.setdefault('policy', {})\n",
        "                            state['policy']['force_answer_ticks'] = max(int((state['policy'] or {}).get('force_answer_ticks', 0)), 3)\n",
        "                            knobs = state.get('policy_knobs', {}) or {}\n",
        "                            knobs['world_bias_boost'] = max(float(knobs.get('world_bias_boost', 0.0)), 1.5)\n",
        "                            state['policy_knobs'] = knobs\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            except Exception:\n",
        "                pass\n",
        "            # v5.3: update volition after outcome (streaks + commitment)\n",
        "            try:\n",
        "                update_volition_after_outcome(state, winner, result, tick_num)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # v5.2.5 TECH_RETRY: if world bridge errors, keep event open and retry (limited)\n",
        "            try:\n",
        "                if current_event and bool(result.get('tech_fail', False)):\n",
        "                    current_event['tech_retries'] = int(current_event.get('tech_retries', 0)) + 1\n",
        "                    current_event['last_error'] = result.get('output','')\n",
        "                    # Force REPAIR mode (but do not close the event)\n",
        "                    state.setdefault('pb', {}).setdefault('mode', 'REPAIR')\n",
        "                    state['pb']['mode'] = 'REPAIR'\n",
        "                    if current_event['tech_retries'] <= 2:\n",
        "                        current_event['status'] = 'RETRY'\n",
        "                        current_event['priority'] = max(float(current_event.get('priority', 1.0)), 2.0)\n",
        "                        logger.log(f\"üß∞ TECH_FAIL: keeping event open for retry ({current_event['tech_retries']}/2)\")\n",
        "                    else:\n",
        "                        current_event['status'] = 'FAILED_TECH'\n",
        "                        logger.log('üß∞ TECH_FAIL: retry budget exhausted; marking FAILED_TECH')\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "\n",
        "            # --- v5.0 Goal resolution & agency reward\n",
        "            try:\n",
        "                goals = state.get('temporal_thread', {}).get('active_goals', [])\n",
        "                if goals:\n",
        "                    for gi, gg in enumerate(list(goals)):\n",
        "                        if goal_satisfied(gg, state):\n",
        "                            state['temporal_thread']['goal_history'].append({**gg, 'completed_tick': tick_num})\n",
        "                            del state['temporal_thread']['active_goals'][gi]\n",
        "                            result['reward'] = float(result.get('reward', 0.0)) + 0.30\n",
        "                            logger.log(f\"‚úÖ Goal completed: {gg.get('target')} -> {gg.get('desired')} (+0.30)\")\n",
        "                            break\n",
        "            except Exception:\n",
        "                pass\n",
        "            # ACTIVE INFERENCE: Prediction-Outcome Loop\n",
        "            sandbox_state, actual_reward, prediction_error, valence = apply_active_inference(\n",
        "                state, winner, result, sandbox, logger\n",
        "            )\n",
        "\n",
        "            # UPDATE CLAIM LEDGER\n",
        "            update_claim_ledger(state, result, actual_reward, origin_action=winner['action_type'], triggered_by_user=user_input_injected)            # AUTO-VERIFY: if this tick executed VERIFY, run claim-ledger verification now\n",
        "            if winner.get('action_type') == 'VERIFY' and 'verify_claim_ledger' in globals():\n",
        "                vr = drain_verify_queue(state, llm=self.llm, batch=25, max_rounds=6) if 'drain_verify_queue' in globals() else verify_claim_ledger(state, llm=self.llm, max_to_verify=25)\n",
        "                logger.log(f\"üîé Verify pass=+{vr.get('verified',0)} fail=+{vr.get('failed',0)} uncertain=+{vr.get('uncertain',0)}\")\n",
        "\n",
        "            logger.log(f\"Executed: {result['output']}\")\n",
        "\n",
        "            # --- v5.2.5 Update sleep streak\n",
        "            try:\n",
        "                if winner.get('action_type') == 'SLEEP':\n",
        "                    state['counters']['sleep_streak'] = int((state.get('counters', {}) or {}).get('sleep_streak',0)) + 1\n",
        "                else:\n",
        "                    state['counters']['sleep_streak'] = 0\n",
        "            except Exception:\n",
        "                state.setdefault('counters', {})\n",
        "                state['counters']['sleep_streak'] = 0\n",
        "\n",
        "            # --- v4.3 META no-op tracking\n",
        "            try:\n",
        "                if (winner.get('module') == 'META' or winner.get('action_type') == 'META') and (result.get('output','').strip().lower() in ['no action', '']):\n",
        "                    state['counters']['meta_noop_streak'] = int(state['counters'].get('meta_noop_streak', 0)) + 1\n",
        "                else:\n",
        "                    state['counters']['meta_noop_streak'] = 0\n",
        "            except Exception:\n",
        "                state['counters']['meta_noop_streak'] = 0\n",
        "\n",
        "            # --- v4.3.1 TELEMETRY snapshot (for dashboards)\n",
        "            try:\n",
        "                state.setdefault('telemetry', {}).setdefault('ticks', [])\n",
        "                w = (state.get('workspace', {}) or {}).get('world', {})\n",
        "                objs = (w.get('objects', {}) if isinstance(w, dict) else {})\n",
        "                world_states = {k: (objs.get(k, {}).get('state') if isinstance(objs.get(k, {}), dict) else None)\n",
        "                                for k in ['lamp', 'box', 'door']}\n",
        "                snap = {\n",
        "                    'tick': tick_num,\n",
        "                    'mode': (state.get('pb', {}) or {}).get('mode'),\n",
        "                    'C_total': float((state.get('metrics', {}) or {}).get('C_total', 0.0)),\n",
        "                    'Ce': float((state.get('metrics', {}) or {}).get('Ce', 0.0)),\n",
        "                    'energy': float((state.get('drives', {}) or {}).get('energy', 0.0)),\n",
        "                    'mood': float((state.get('affect', {}) or {}).get('mood', 0.0)),\n",
        "                    'action_type': (winner.get('action_type') or winner.get('module')),\n",
        "                    'reward': float(result.get('reward', 0.0)),\n",
        "                    'meta_noop_streak': int((state.get('counters', {}) or {}).get('meta_noop_streak', 0)),\n",
        "                    'world': world_states,\n",
        "                    'scene': (state.get('workspace', {}) or {}).get('scene', ''),\n",
        "                }\n",
        "                state['telemetry']['ticks'].append(snap)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- v5.1 Self-model update + conscious trace (post-outcome)\n",
        "            try:\n",
        "                update_self_model_after_outcome(state, winner, result)\n",
        "                append_conscious_trace(state, winner, result, tick_num)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # --- v4.1 EVENT LAW (v5.2.5): close event after resolving actions (but keep open on TECH_FAIL)\n",
        "            if current_event and winner.get('action_type') in ['REFLECT','TOOL_CALL','RETRIEVE','WORLD_ACT']:\n",
        "                try:\n",
        "                    if bool(result.get('tech_fail', False)):\n",
        "                        # keep event open for retry\n",
        "                        pass\n",
        "                    else:\n",
        "                        # If world action failed but there is an active goal, allow a couple more rounds\n",
        "                        if winner.get('action_type') == 'WORLD_ACT' and result.get('status') in ['fail','error'] and (state.get('temporal_thread', {}) or {}).get('active_goals'):\n",
        "                            current_event['rounds'] = int(current_event.get('rounds',0)) + 1\n",
        "                            if current_event['rounds'] < 3:\n",
        "                                logger.log('üîÅ Keeping event open: goal still active after WORLD_ACT failure')\n",
        "                            else:\n",
        "                                mark_acted(current_event, tick_num)\n",
        "                                close_event(current_event, tick_num)\n",
        "                                logger.log(f\"‚úÖ Event closed: {current_event['id']}\")\n",
        "                        else:\n",
        "                            mark_acted(current_event, tick_num)\n",
        "                            close_event(current_event, tick_num)\n",
        "                            logger.log(f\"‚úÖ Event closed: {current_event['id']}\")\n",
        "                except Exception:\n",
        "                    mark_acted(current_event, tick_num)\n",
        "                    close_event(current_event, tick_num)\n",
        "                    logger.log(f\"‚úÖ Event closed: {current_event['id']}\")\n",
        "                # v4.2.4 cleanup: remove closed events from spotlight and active queue\n",
        "                try:\n",
        "                    events = state.get('workspace', {}).get('events', [])\n",
        "                    state['workspace']['events'] = [e for e in events if e.get('status') != 'CLOSED']\n",
        "                    sp = state.get('attention', {}).get('spotlight', [])\n",
        "                    state['attention']['spotlight'] = [x for x in sp if x != current_event.get('id')]\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "\n",
        "            # Track agency (based on winning proposal, internal action)\n",
        "            # Track agency (distinguish self-initiated vs externally-triggered)\n",
        "            authorship = 'external_triggered' if user_input_injected else 'self_initiated'\n",
        "            state['agency']['authorship_log'].append({\n",
        "                'tick': tick_num,\n",
        "                'action': winner['action_type'],\n",
        "                'authorship': authorship,\n",
        "                'triggered_by_user': bool(user_input_injected),\n",
        "                'now_id': state['pb'].get('now_id')\n",
        "            })\n",
        "\n",
        "            # If SLEEP mode was entered, reset cooldown timer\n",
        "            if winner['action_type'] == 'SLEEP':\n",
        "                state['sleep_cooldown_timer'] = Config.SLEEP_COOLDOWN_TICKS\n",
        "        # Update loop risk\n",
        "        try:\n",
        "            traj = (state.get('attention', {}) or {}).get('trajectory', [])\n",
        "            if isinstance(traj, list) and len(traj) > 5:\n",
        "                recent = traj[-5:]\n",
        "                recent_modes = [x.get('mode') for x in recent if isinstance(x, dict) and x.get('mode') is not None]\n",
        "                if recent_modes and len(set(recent_modes)) == 1:\n",
        "                    state['loop_risk'] = float(state.get('loop_risk', 0.0)) + 0.1\n",
        "                else:\n",
        "                    state['loop_risk'] = float(state.get('loop_risk', 0.0)) * 0.9\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Save periodically\n",
        "        if tick_num % 5 == 0:\n",
        "            self.state_manager.save()\n",
        "            logger.log(\"State saved\")\n",
        "\n",
        "        logger.log(f\"Tick {tick_num} complete\\n\")\n",
        "\n",
        "    def run(self, max_ticks: int = Config.MAX_TICKS):\n",
        "        logger.log(f\"Starting core loop for {max_ticks} ticks...\")\n",
        "\n",
        "        try:\n",
        "            for _ in range(max_ticks):\n",
        "                self.tick()\n",
        "                if not self.running:\n",
        "                    break\n",
        "                time.sleep(Config.TICK_INTERVAL)\n",
        "        except KeyboardInterrupt:\n",
        "            logger.log(\"Interrupted\")\n",
        "        finally:\n",
        "            self.state_manager.save()\n",
        "            logger.log(\"=== Session Complete ===\")\n",
        "\n",
        "print(\"‚úì Core loop ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LkgoiUkgf6ec",
      "metadata": {
        "id": "LkgoiUkgf6ec"
      },
      "source": [
        "## v4.2.5 ‚Äî How to provide inputs\n",
        "\n",
        "Edit `TEST_INPUTS` in the next cell. Then run the notebook normally.\n",
        "By default, it will inject those messages as events (low compute).\n",
        "Set `RUN_AUTOPILOT = True` to run the old long autonomous loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "LeGuFtIMf6ec",
      "metadata": {
        "id": "LeGuFtIMf6ec"
      },
      "outputs": [],
      "source": [
        "# === v4.2.5 Input Injection (no interactive prompt needed) ===\n",
        "# Put your test messages here. The runner will feed them as EVENTS.\n",
        "TEST_INPUTS = [\n",
        "    \"Turn on the lamp\",\n",
        "    \"Turn off the lamp\",\n",
        "    \"Open the box\",\n",
        "    \"Close the box\",\n",
        "    \"Open the door\",\n",
        "    \"Unlock the door\",\n",
        "    \"Open the door\",\n",
        "]\n",
        "\n",
        "TICKS_PER_INPUT = 6   # keep low to save compute (e.g., 4-8)\n",
        "RUN_AUTOPILOT = False # set True to run the old long autonomous loop\n",
        "\n",
        "def enqueue_user_event(state, msg: str, tick: int = 0):\n",
        "    state.setdefault('workspace', {}).setdefault('events', [])\n",
        "    cur_tick = int(state.get('tick_count', tick) or tick)\n",
        "    ev_type = 'STATEMENT'\n",
        "    try:\n",
        "        ev_type = classify_user_input(msg)\n",
        "    except Exception:\n",
        "        pass\n",
        "    pri = 1.0\n",
        "    if ev_type == 'QUESTION':\n",
        "        pri = 1.2\n",
        "    elif ev_type == 'WORLD_GOAL':\n",
        "        pri = 1.4\n",
        "    ev = create_event(msg, cur_tick, priority=pri, source='user')\n",
        "    ev['event_type'] = ev_type\n",
        "    ev['source'] = 'user'\n",
        "    ev['status'] = 'NEW'\n",
        "    ev['rounds'] = 0\n",
        "    state['workspace']['events'].append(ev)\n",
        "    return ev\n",
        "\n",
        "def run_with_test_inputs(state_manager, core_loop, inputs=TEST_INPUTS, ticks_per_input=TICKS_PER_INPUT):\n",
        "    for msg in inputs:\n",
        "        print(f\"\\n>>> Injecting user input: {msg}\")\n",
        "        enqueue_user_event(state_manager.state, msg, tick=0)\n",
        "        state_manager.save()\n",
        "        core_loop.run(max_ticks=ticks_per_input)\n",
        "    print(\"\\n=== Test-input session complete ===\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_8sDT7QUf6ed",
      "metadata": {
        "id": "_8sDT7QUf6ed"
      },
      "source": [
        "## v4.3.1 Dashboards\n",
        "\n",
        "Two views:\n",
        "- **Scientific Dashboard** (metrics/time series)\n",
        "- **Human-friendly Room View** (lamp/box/door state)\n",
        "- Optional quick **Animation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9b83d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c9b83d4",
        "outputId": "72680bcf-b2cc-4eff-dd12-cc3392f54c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "CR-SSCP v5.7.10 - Consciousness-like Cognitive Architecture\n",
            "============================================================\n",
            "\n",
            "Initial Coherence: 0.500\n",
            "Initial Energy: 0.85\n",
            "Initial Emotion: curious\n",
            "Mode: REFLECT\n",
            "\n",
            "Identity anchors:\n",
            "  - I am an experimental cognitive architecture\n",
            "  - I aim to maintain coherence and avoid hallucinations\n",
            "  - I learn from evidence and admit uncertainty\n",
            "\n",
            "Running 100 ticks (~8 minutes)...\n",
            "\n",
            "\n",
            ">>> Injecting user input: Turn on the lamp\n",
            "[2026-02-19 10:26:00] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:26:00] \n",
            "============================================================\n",
            "[2026-02-19 10:26:00] TICK 1\n",
            "[2026-02-19 10:26:00] ============================================================\n",
            "[2026-02-19 10:26:00] Attention spotlight: ['event_02403e10']\n",
            "[2026-02-19 10:26:00] Coherence C_total: 0.590\n",
            "[2026-02-19 10:26:00] Mode: REFLECT\n",
            "[2026-02-19 10:26:00] Energy: 0.84, Coherence: 0.78, Novelty: 0.73\n",
            "[2026-02-19 10:26:00] Emotion: curious, Mood: 0.51\n",
            "[2026-02-19 10:26:00] Generated 1 proposals\n",
            "[2026-02-19 10:26:00] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:00] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:26:00] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:00] ‚öñÔ∏è  Reward: +0.020, PredError: 0.333, Valence: +0.020, MatchScore: 0.50\n",
            "[2026-02-19 10:26:00] Executed: üåç lamp is now on\n",
            "[2026-02-19 10:26:00] ‚úÖ Event closed: event_02403e10\n",
            "[2026-02-19 10:26:00] Tick 1 complete\n",
            "[2026-02-19 10:26:05] \n",
            "============================================================\n",
            "[2026-02-19 10:26:05] TICK 2\n",
            "[2026-02-19 10:26:05] ============================================================\n",
            "[2026-02-19 10:26:05] Attention spotlight: []\n",
            "[2026-02-19 10:26:05] Coherence C_total: 0.619\n",
            "[2026-02-19 10:26:05] Mode: ANSWER\n",
            "[2026-02-19 10:26:05] Energy: 0.91, Coherence: 0.76, Novelty: 0.72\n",
            "[2026-02-19 10:26:05] Emotion: curious, Mood: 0.51\n",
            "[2026-02-19 10:26:05] Generated 2 proposals\n",
            "[2026-02-19 10:26:05] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:05] [DEBUG] p0: META module=META EU=0.12 cost=0.40\n",
            "[2026-02-19 10:26:05] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:07] ‚öñÔ∏è  Reward: +0.074, PredError: 0.303, Valence: -0.063, MatchScore: 0.50\n",
            "[2026-02-19 10:26:07] Executed: Given the lack of a specific goal or scene, it's challenging to provide a concrete reflection or next step. However, if we assume a general context where progress is needed, the next best step would be to identify a clear, achievable objective and outline the first actionable task towards that goal.\n",
            "[2026-02-19 10:26:07] Tick 2 complete\n",
            "[2026-02-19 10:26:12] \n",
            "============================================================\n",
            "[2026-02-19 10:26:12] TICK 3\n",
            "[2026-02-19 10:26:12] ============================================================\n",
            "[2026-02-19 10:26:12] Attention spotlight: []\n",
            "[2026-02-19 10:26:12] Coherence C_total: 0.625\n",
            "[2026-02-19 10:26:12] Mode: ANSWER\n",
            "[2026-02-19 10:26:12] Energy: 0.90, Coherence: 0.74, Novelty: 0.71\n",
            "[2026-02-19 10:26:12] Emotion: curious, Mood: 0.51\n",
            "[2026-02-19 10:26:12] Generated 2 proposals\n",
            "[2026-02-19 10:26:12] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:12] [DEBUG] p0: META module=META EU=0.12 cost=0.40\n",
            "[2026-02-19 10:26:12] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:13] ‚öñÔ∏è  Reward: +0.024, PredError: 0.328, Valence: -0.124, MatchScore: 0.50\n",
            "[2026-02-19 10:26:13] Executed: Given the current lack of a specific goal or scene, it's difficult to provide a concrete reflection or next step. However, if we consider a general scenario where one is feeling stuck or unsure about the next move, a useful next step could be to list out all possible actions and evaluate them based on feasibility and potential outcomes. This can help clarify the path forward.\n",
            "[2026-02-19 10:26:13] Tick 3 complete\n",
            "[2026-02-19 10:26:18] \n",
            "============================================================\n",
            "[2026-02-19 10:26:18] TICK 4\n",
            "[2026-02-19 10:26:18] ============================================================\n",
            "[2026-02-19 10:26:18] Attention spotlight: ['self_thought_3']\n",
            "[2026-02-19 10:26:18] Coherence C_total: 0.628\n",
            "[2026-02-19 10:26:18] Mode: ANSWER\n",
            "[2026-02-19 10:26:18] Energy: 0.89, Coherence: 0.72, Novelty: 0.69\n",
            "[2026-02-19 10:26:18] Emotion: curious, Mood: 0.52\n",
            "[2026-02-19 10:26:18] Generated 1 proposals\n",
            "[2026-02-19 10:26:18] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:26:18] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:19] ‚öñÔ∏è  Reward: +0.055, PredError: 0.313, Valence: -0.086, MatchScore: 0.50\n",
            "[2026-02-19 10:26:19] Executed: Given the current state with no active goal, my next best step is to perform self-maintenance. This involves checking and updating my systems to ensure optimal performance.\n",
            "[2026-02-19 10:26:19] ‚úÖ Event closed: self_thought_3\n",
            "[2026-02-19 10:26:19] Tick 4 complete\n",
            "[2026-02-19 10:26:24] \n",
            "============================================================\n",
            "[2026-02-19 10:26:24] TICK 5\n",
            "[2026-02-19 10:26:24] ============================================================\n",
            "[2026-02-19 10:26:24] Attention spotlight: []\n",
            "[2026-02-19 10:26:24] Coherence C_total: 0.629\n",
            "[2026-02-19 10:26:24] Mode: ANSWER\n",
            "[2026-02-19 10:26:24] Energy: 0.88, Coherence: 0.71, Novelty: 0.68\n",
            "[2026-02-19 10:26:24] Emotion: curious, Mood: 0.52\n",
            "[2026-02-19 10:26:24] Generated 1 proposals\n",
            "[2026-02-19 10:26:24] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:24] [DEBUG] p0: META module=META EU=0.12 cost=0.05\n",
            "[2026-02-19 10:26:24] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:24] ‚öñÔ∏è  Reward: +0.056, PredError: 0.198, Valence: +0.117, MatchScore: 0.50\n",
            "[2026-02-19 10:26:24] Executed: No action\n",
            "[2026-02-19 10:26:24] State saved\n",
            "[2026-02-19 10:26:24] Tick 5 complete\n",
            "[2026-02-19 10:26:29] \n",
            "============================================================\n",
            "[2026-02-19 10:26:29] TICK 6\n",
            "[2026-02-19 10:26:29] ============================================================\n",
            "[2026-02-19 10:26:29] Attention spotlight: []\n",
            "[2026-02-19 10:26:29] Coherence C_total: 0.631\n",
            "[2026-02-19 10:26:29] Mode: ANSWER\n",
            "[2026-02-19 10:26:29] Energy: 0.87, Coherence: 0.70, Novelty: 0.66\n",
            "[2026-02-19 10:26:29] Emotion: curious, Mood: 0.53\n",
            "[2026-02-19 10:26:29] Generated 1 proposals\n",
            "[2026-02-19 10:26:29] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:29] [DEBUG] p0: META module=META EU=0.12 cost=0.05\n",
            "[2026-02-19 10:26:29] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:29] ‚öñÔ∏è  Reward: +0.110, PredError: 0.178, Valence: +0.180, MatchScore: 0.50\n",
            "[2026-02-19 10:26:29] Executed: No action\n",
            "[2026-02-19 10:26:29] Tick 6 complete\n",
            "[2026-02-19 10:26:34] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Turn off the lamp\n",
            "[2026-02-19 10:26:34] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:26:34] \n",
            "============================================================\n",
            "[2026-02-19 10:26:34] TICK 7\n",
            "[2026-02-19 10:26:34] ============================================================\n",
            "[2026-02-19 10:26:34] Attention spotlight: ['event_f9534c95']\n",
            "[2026-02-19 10:26:34] Coherence C_total: 0.632\n",
            "[2026-02-19 10:26:34] Mode: ANSWER\n",
            "[2026-02-19 10:26:34] Energy: 0.86, Coherence: 0.69, Novelty: 0.65\n",
            "[2026-02-19 10:26:34] Emotion: curious, Mood: 0.53\n",
            "[2026-02-19 10:26:34] Generated 1 proposals\n",
            "[2026-02-19 10:26:34] [DEBUG] proposals_before_arbitration=2\n",
            "[2026-02-19 10:26:34] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.72 cost=0.00\n",
            "[2026-02-19 10:26:34] [DEBUG] p1: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:34] ‚öñÔ∏è  Reward: +0.030, PredError: 0.330, Valence: +0.032, MatchScore: 0.50\n",
            "[2026-02-19 10:26:34] Executed: üåç lamp is now off\n",
            "[2026-02-19 10:26:34] ‚úÖ Event closed: event_f9534c95\n",
            "[2026-02-19 10:26:34] Tick 7 complete\n",
            "[2026-02-19 10:26:39] \n",
            "============================================================\n",
            "[2026-02-19 10:26:39] TICK 8\n",
            "[2026-02-19 10:26:39] ============================================================\n",
            "[2026-02-19 10:26:39] Attention spotlight: []\n",
            "[2026-02-19 10:26:39] Coherence C_total: 0.632\n",
            "[2026-02-19 10:26:39] Mode: ANSWER\n",
            "[2026-02-19 10:26:39] Energy: 0.93, Coherence: 0.68, Novelty: 0.64\n",
            "[2026-02-19 10:26:39] Emotion: curious, Mood: 0.53\n",
            "[2026-02-19 10:26:39] Generated 1 proposals\n",
            "[2026-02-19 10:26:39] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:26:39] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:41] ‚öñÔ∏è  Reward: +0.084, PredError: 0.298, Valence: -0.051, MatchScore: 0.50\n",
            "[2026-02-19 10:26:41] Executed: Given the lack of a specific goal or scene, it's difficult to provide a concrete reflection or next step. However, if we assume a common scenario where you are feeling stuck, a concrete next step could be to set a small, achievable goal for yourself, such as reading a few pages of a book or completing a segment of a task. This can help break the inertia and move forward.\n",
            "[2026-02-19 10:26:41] Tick 8 complete\n",
            "[2026-02-19 10:26:46] \n",
            "============================================================\n",
            "[2026-02-19 10:26:46] TICK 9\n",
            "[2026-02-19 10:26:46] ============================================================\n",
            "[2026-02-19 10:26:46] Attention spotlight: []\n",
            "[2026-02-19 10:26:46] Coherence C_total: 0.633\n",
            "[2026-02-19 10:26:46] Mode: ANSWER\n",
            "[2026-02-19 10:26:46] Energy: 0.92, Coherence: 0.67, Novelty: 0.63\n",
            "[2026-02-19 10:26:46] Emotion: curious, Mood: 0.54\n",
            "[2026-02-19 10:26:46] Generated 1 proposals\n",
            "[2026-02-19 10:26:46] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:26:46] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:46] ‚öñÔ∏è  Reward: +0.034, PredError: 0.323, Valence: -0.112, MatchScore: 0.50\n",
            "[2026-02-19 10:26:46] Executed: Given that no specific goal or scene is provided, I'm currently without context to offer a relevant reflection or next step. Could you please provide more details about the situation or scenario you're referring to?\n",
            "[2026-02-19 10:26:46] Tick 9 complete\n",
            "[2026-02-19 10:26:51] \n",
            "============================================================\n",
            "[2026-02-19 10:26:51] TICK 10\n",
            "[2026-02-19 10:26:51] ============================================================\n",
            "[2026-02-19 10:26:51] Attention spotlight: ['self_thought_9']\n",
            "[2026-02-19 10:26:51] Coherence C_total: 0.633\n",
            "[2026-02-19 10:26:51] Mode: ANSWER\n",
            "[2026-02-19 10:26:51] Energy: 0.91, Coherence: 0.67, Novelty: 0.61\n",
            "[2026-02-19 10:26:51] Emotion: curious, Mood: 0.54\n",
            "[2026-02-19 10:26:51] Generated 2 proposals\n",
            "[2026-02-19 10:26:51] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:26:51] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:52] ‚öñÔ∏è  Reward: +0.149, PredError: 0.265, Valence: +0.030, MatchScore: 0.50\n",
            "[2026-02-19 10:26:52] Executed: Given the current situation with no active goal or scene to engage in, my next best step is to perform self-maintenance. This involves checking and updating my systems to ensure optimal performance for any future interactions or tasks.\n",
            "[2026-02-19 10:26:52] ‚úÖ Event closed: self_thought_9\n",
            "[2026-02-19 10:26:52] State saved\n",
            "[2026-02-19 10:26:52] Tick 10 complete\n",
            "[2026-02-19 10:26:57] \n",
            "============================================================\n",
            "[2026-02-19 10:26:57] TICK 11\n",
            "[2026-02-19 10:26:57] ============================================================\n",
            "[2026-02-19 10:26:57] Attention spotlight: []\n",
            "[2026-02-19 10:26:57] Coherence C_total: 0.634\n",
            "[2026-02-19 10:26:57] Mode: ANSWER\n",
            "[2026-02-19 10:26:57] Energy: 0.90, Coherence: 0.66, Novelty: 0.60\n",
            "[2026-02-19 10:26:57] Emotion: curious, Mood: 0.54\n",
            "[2026-02-19 10:26:57] Generated 1 proposals\n",
            "[2026-02-19 10:26:57] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:26:57] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:26:58] ‚öñÔ∏è  Reward: +0.070, PredError: 0.305, Valence: -0.067, MatchScore: 0.50\n",
            "[2026-02-19 10:26:58] Executed: Without a specific context or scene provided, it's difficult to offer a relevant reflection or next step. However, if you're currently facing a decision or challenge, consider gathering more information or seeking feedback from others before making a choice. If you can provide more details about your situation, I can offer a more concrete reflection or suggestion.\n",
            "[2026-02-19 10:26:58] Tick 11 complete\n",
            "[2026-02-19 10:27:03] \n",
            "============================================================\n",
            "[2026-02-19 10:27:03] TICK 12\n",
            "[2026-02-19 10:27:03] ============================================================\n",
            "[2026-02-19 10:27:03] Attention spotlight: []\n",
            "[2026-02-19 10:27:03] Coherence C_total: 0.634\n",
            "[2026-02-19 10:27:03] Mode: ANSWER\n",
            "[2026-02-19 10:27:03] Energy: 0.89, Coherence: 0.65, Novelty: 0.59\n",
            "[2026-02-19 10:27:03] Emotion: curious, Mood: 0.55\n",
            "[2026-02-19 10:27:03] Generated 1 proposals\n",
            "[2026-02-19 10:27:03] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:03] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:04] ‚öñÔ∏è  Reward: +0.040, PredError: 0.320, Valence: -0.104, MatchScore: 0.50\n",
            "[2026-02-19 10:27:04] Executed: Given the lack of a specific goal or scene, it's challenging to provide a concrete reflection or next step. However, if you can provide more context or detail about the current situation, I can offer a more relevant and actionable reflection or suggestion.\n",
            "[2026-02-19 10:27:04] Tick 12 complete\n",
            "[2026-02-19 10:27:09] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Open the box\n",
            "[2026-02-19 10:27:09] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:27:09] \n",
            "============================================================\n",
            "[2026-02-19 10:27:09] TICK 13\n",
            "[2026-02-19 10:27:09] ============================================================\n",
            "[2026-02-19 10:27:09] Attention spotlight: ['event_152d6be1']\n",
            "[2026-02-19 10:27:09] Coherence C_total: 0.634\n",
            "[2026-02-19 10:27:09] Mode: ANSWER\n",
            "[2026-02-19 10:27:09] Energy: 0.88, Coherence: 0.65, Novelty: 0.58\n",
            "[2026-02-19 10:27:09] Emotion: curious, Mood: 0.55\n",
            "[2026-02-19 10:27:09] Generated 2 proposals\n",
            "[2026-02-19 10:27:09] [DEBUG] proposals_before_arbitration=4\n",
            "[2026-02-19 10:27:09] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:27:09] [DEBUG] p1: WORLD_ACT module=SELF_MAINTENANCE EU=1.15 cost=0.10\n",
            "[2026-02-19 10:27:09] [DEBUG] p2: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:09] [DEBUG] p3: WORLD_ACT module=SELF_MAINTENANCE EU=0.60 cost=0.20\n",
            "[2026-02-19 10:27:09] ‚öñÔ∏è  Reward: +0.071, PredError: 0.316, Valence: +0.079, MatchScore: 0.50\n",
            "[2026-02-19 10:27:09] Executed: üåç box is now open\n",
            "[2026-02-19 10:27:09] ‚úÖ Event closed: event_152d6be1\n",
            "[2026-02-19 10:27:09] Tick 13 complete\n",
            "[2026-02-19 10:27:14] \n",
            "============================================================\n",
            "[2026-02-19 10:27:14] TICK 14\n",
            "[2026-02-19 10:27:14] ============================================================\n",
            "[2026-02-19 10:27:14] Attention spotlight: []\n",
            "[2026-02-19 10:27:14] Coherence C_total: 0.636\n",
            "[2026-02-19 10:27:14] Mode: ANSWER\n",
            "[2026-02-19 10:27:14] Energy: 0.95, Coherence: 0.64, Novelty: 0.57\n",
            "[2026-02-19 10:27:14] Emotion: curious, Mood: 0.55\n",
            "[2026-02-19 10:27:14] Generated 1 proposals\n",
            "[2026-02-19 10:27:14] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:14] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:15] ‚öñÔ∏è  Reward: +0.126, PredError: 0.277, Valence: +0.001, MatchScore: 0.50\n",
            "[2026-02-19 10:27:15] Executed: Given the lack of a specific scenario or context, it's difficult to provide a concrete reflection or next best step. However, if you can provide more details about the situation or goal, I can offer a more relevant and actionable response.\n",
            "[2026-02-19 10:27:15] Tick 14 complete\n",
            "[2026-02-19 10:27:20] \n",
            "============================================================\n",
            "[2026-02-19 10:27:20] TICK 15\n",
            "[2026-02-19 10:27:20] ============================================================\n",
            "[2026-02-19 10:27:20] Attention spotlight: []\n",
            "[2026-02-19 10:27:20] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:20] Mode: ANSWER\n",
            "[2026-02-19 10:27:20] Energy: 0.94, Coherence: 0.64, Novelty: 0.55\n",
            "[2026-02-19 10:27:20] Emotion: curious, Mood: 0.55\n",
            "[2026-02-19 10:27:20] Generated 1 proposals\n",
            "[2026-02-19 10:27:20] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:20] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:22] ‚öñÔ∏è  Reward: +0.077, PredError: 0.302, Valence: -0.059, MatchScore: 0.50\n",
            "[2026-02-19 10:27:22] Executed: Given the lack of a specific scenario or goal, it's difficult to provide a concrete reflection or next step. However, if you were facing a decision or challenge, a good general approach would be to gather all relevant information, consider potential outcomes, and then choose an action that aligns with your objectives. If you can provide more details about a specific situation, I can offer a more tailored reflection or suggestion.\n",
            "[2026-02-19 10:27:22] State saved\n",
            "[2026-02-19 10:27:22] Tick 15 complete\n",
            "[2026-02-19 10:27:27] \n",
            "============================================================\n",
            "[2026-02-19 10:27:27] TICK 16\n",
            "[2026-02-19 10:27:27] ============================================================\n",
            "[2026-02-19 10:27:27] Attention spotlight: ['self_thought_15']\n",
            "[2026-02-19 10:27:27] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:27] Mode: ANSWER\n",
            "[2026-02-19 10:27:27] Energy: 0.93, Coherence: 0.64, Novelty: 0.54\n",
            "[2026-02-19 10:27:27] Emotion: curious, Mood: 0.56\n",
            "[2026-02-19 10:27:27] Generated 2 proposals\n",
            "[2026-02-19 10:27:27] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:27] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:28] ‚öñÔ∏è  Reward: +0.078, PredError: 0.301, Valence: -0.058, MatchScore: 0.50\n",
            "[2026-02-19 10:27:28] Executed: Given your current positive momentum and ability to push forward decisively, the next best step is to identify a specific, achievable goal and commit to working towards it with focused determination. Break this goal into smaller tasks if needed, and tackle them one by one to maintain that forward momentum.\n",
            "[2026-02-19 10:27:28] ‚úÖ Event closed: self_thought_15\n",
            "[2026-02-19 10:27:28] Tick 16 complete\n",
            "[2026-02-19 10:27:33] \n",
            "============================================================\n",
            "[2026-02-19 10:27:33] TICK 17\n",
            "[2026-02-19 10:27:33] ============================================================\n",
            "[2026-02-19 10:27:33] Attention spotlight: []\n",
            "[2026-02-19 10:27:33] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:33] Mode: ANSWER\n",
            "[2026-02-19 10:27:33] Energy: 0.92, Coherence: 0.63, Novelty: 0.53\n",
            "[2026-02-19 10:27:33] Emotion: curious, Mood: 0.56\n",
            "[2026-02-19 10:27:33] Generated 1 proposals\n",
            "[2026-02-19 10:27:33] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:33] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:34] ‚öñÔ∏è  Reward: +0.048, PredError: 0.316, Valence: -0.095, MatchScore: 0.50\n",
            "[2026-02-19 10:27:34] Executed: Without a specific goal or scene provided, it's difficult to offer a concrete reflection or next step. However, if you're feeling stuck or unsure about the next move, consider reviewing your objectives and the information available to you. Break down the problem into smaller, manageable parts and focus on the most immediate or achievable task.\n",
            "[2026-02-19 10:27:34] Tick 17 complete\n",
            "[2026-02-19 10:27:39] \n",
            "============================================================\n",
            "[2026-02-19 10:27:39] TICK 18\n",
            "[2026-02-19 10:27:39] ============================================================\n",
            "[2026-02-19 10:27:39] Attention spotlight: []\n",
            "[2026-02-19 10:27:39] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:39] Mode: ANSWER\n",
            "[2026-02-19 10:27:39] Energy: 0.91, Coherence: 0.63, Novelty: 0.52\n",
            "[2026-02-19 10:27:39] Emotion: curious, Mood: 0.56\n",
            "[2026-02-19 10:27:39] Generated 1 proposals\n",
            "[2026-02-19 10:27:39] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:39] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:41] ‚öñÔ∏è  Reward: +0.110, PredError: 0.285, Valence: -0.019, MatchScore: 0.50\n",
            "[2026-02-19 10:27:41] Executed: Given the lack of a specific goal or scene, it's difficult to provide a concrete reflection or next step. However, if we consider a general scenario where someone is feeling stuck, a next best step could be to set a small, achievable goal to break the cycle of inaction. For example, spending 10 minutes on a task they've been avoiding can often lead to further progress.\n",
            "[2026-02-19 10:27:41] Tick 18 complete\n",
            "[2026-02-19 10:27:46] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Close the box\n",
            "[2026-02-19 10:27:46] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:27:46] \n",
            "============================================================\n",
            "[2026-02-19 10:27:46] TICK 19\n",
            "[2026-02-19 10:27:46] ============================================================\n",
            "[2026-02-19 10:27:46] Attention spotlight: ['event_3e637a53']\n",
            "[2026-02-19 10:27:46] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:46] Mode: ANSWER\n",
            "[2026-02-19 10:27:46] Energy: 0.90, Coherence: 0.63, Novelty: 0.51\n",
            "[2026-02-19 10:27:46] Emotion: curious, Mood: 0.56\n",
            "[2026-02-19 10:27:46] Generated 2 proposals\n",
            "[2026-02-19 10:27:46] [DEBUG] proposals_before_arbitration=4\n",
            "[2026-02-19 10:27:46] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:27:46] [DEBUG] p1: WORLD_ACT module=SELF_MAINTENANCE EU=1.15 cost=0.10\n",
            "[2026-02-19 10:27:46] [DEBUG] p2: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:46] [DEBUG] p3: WORLD_ACT module=SELF_MAINTENANCE EU=0.60 cost=0.20\n",
            "[2026-02-19 10:27:46] ‚öñÔ∏è  Reward: +0.081, PredError: 0.312, Valence: +0.090, MatchScore: 0.50\n",
            "[2026-02-19 10:27:46] Executed: üåç box is now closed\n",
            "[2026-02-19 10:27:46] ‚úÖ Event closed: event_3e637a53\n",
            "[2026-02-19 10:27:46] Tick 19 complete\n",
            "[2026-02-19 10:27:51] \n",
            "============================================================\n",
            "[2026-02-19 10:27:51] TICK 20\n",
            "[2026-02-19 10:27:51] ============================================================\n",
            "[2026-02-19 10:27:51] Attention spotlight: []\n",
            "[2026-02-19 10:27:51] Coherence C_total: 0.637\n",
            "[2026-02-19 10:27:51] Mode: ANSWER\n",
            "[2026-02-19 10:27:51] Energy: 0.97, Coherence: 0.63, Novelty: 0.50\n",
            "[2026-02-19 10:27:51] Emotion: curious, Mood: 0.56\n",
            "[2026-02-19 10:27:51] Generated 1 proposals\n",
            "[2026-02-19 10:27:51] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:51] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:52] ‚öñÔ∏è  Reward: +0.051, PredError: 0.315, Valence: -0.091, MatchScore: 0.50\n",
            "[2026-02-19 10:27:52] Executed: Given that no specific goal or scene is provided, it's difficult to offer a concrete reflection or next step. However, if you were in a situation where you feel stuck, consider breaking down the problem into smaller, manageable parts. Start with the simplest task or the one that requires the least effort. This can often provide a clear starting point and help build momentum.\n",
            "[2026-02-19 10:27:52] State saved\n",
            "[2026-02-19 10:27:52] Tick 20 complete\n",
            "[2026-02-19 10:27:57] \n",
            "============================================================\n",
            "[2026-02-19 10:27:57] TICK 21\n",
            "[2026-02-19 10:27:57] ============================================================\n",
            "[2026-02-19 10:27:57] Attention spotlight: []\n",
            "[2026-02-19 10:27:57] Coherence C_total: 0.638\n",
            "[2026-02-19 10:27:57] Mode: ANSWER\n",
            "[2026-02-19 10:27:57] Energy: 0.96, Coherence: 0.62, Novelty: 0.49\n",
            "[2026-02-19 10:27:57] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:27:57] Generated 1 proposals\n",
            "[2026-02-19 10:27:57] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:27:57] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:27:58] ‚öñÔ∏è  Reward: +0.135, PredError: 0.272, Valence: +0.012, MatchScore: 0.50\n",
            "[2026-02-19 10:27:58] Executed: Given the current lack of a specific goal or scene, it's difficult to provide a concrete reflection or next step. However, if we consider a general scenario where one is feeling stuck, a good next step might be to set a small, achievable goal to break the inertia. For example, dedicating 15 minutes to work on a project or task can often help regain momentum.\n",
            "[2026-02-19 10:27:58] Tick 21 complete\n",
            "[2026-02-19 10:28:03] \n",
            "============================================================\n",
            "[2026-02-19 10:28:03] TICK 22\n",
            "[2026-02-19 10:28:03] ============================================================\n",
            "[2026-02-19 10:28:03] Attention spotlight: ['self_thought_21']\n",
            "[2026-02-19 10:28:03] Coherence C_total: 0.637\n",
            "[2026-02-19 10:28:03] Mode: ANSWER\n",
            "[2026-02-19 10:28:03] Energy: 0.95, Coherence: 0.62, Novelty: 0.48\n",
            "[2026-02-19 10:28:03] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:03] Generated 2 proposals\n",
            "[2026-02-19 10:28:03] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:03] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:04] ‚öñÔ∏è  Reward: +0.086, PredError: 0.297, Valence: -0.048, MatchScore: 0.50\n",
            "[2026-02-19 10:28:04] Executed: Given your current positive mindset and ability to push forward, the next best step is to identify a specific goal or task that aligns with your objectives and start working towards it. Break it down into manageable steps if needed, and focus on executing those steps decisively.\n",
            "[2026-02-19 10:28:04] ‚úÖ Event closed: self_thought_21\n",
            "[2026-02-19 10:28:04] Tick 22 complete\n",
            "[2026-02-19 10:28:09] \n",
            "============================================================\n",
            "[2026-02-19 10:28:09] TICK 23\n",
            "[2026-02-19 10:28:09] ============================================================\n",
            "[2026-02-19 10:28:09] Attention spotlight: []\n",
            "[2026-02-19 10:28:09] Coherence C_total: 0.638\n",
            "[2026-02-19 10:28:09] Mode: ANSWER\n",
            "[2026-02-19 10:28:09] Energy: 0.94, Coherence: 0.62, Novelty: 0.47\n",
            "[2026-02-19 10:28:09] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:09] Generated 1 proposals\n",
            "[2026-02-19 10:28:09] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:09] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:10] ‚öñÔ∏è  Reward: +0.087, PredError: 0.296, Valence: -0.046, MatchScore: 0.50\n",
            "[2026-02-19 10:28:10] Executed: Given the lack of a specific goal or scene, it's challenging to provide a concrete reflection or next step. However, if you can provide more context or details about the situation, I can offer a more targeted and useful reflection or action.\n",
            "[2026-02-19 10:28:10] Tick 23 complete\n",
            "[2026-02-19 10:28:15] \n",
            "============================================================\n",
            "[2026-02-19 10:28:15] TICK 24\n",
            "[2026-02-19 10:28:15] ============================================================\n",
            "[2026-02-19 10:28:15] Attention spotlight: []\n",
            "[2026-02-19 10:28:15] Coherence C_total: 0.638\n",
            "[2026-02-19 10:28:15] Mode: ANSWER\n",
            "[2026-02-19 10:28:15] Energy: 0.93, Coherence: 0.62, Novelty: 0.46\n",
            "[2026-02-19 10:28:15] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:15] Generated 1 proposals\n",
            "[2026-02-19 10:28:15] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:15] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:17] ‚öñÔ∏è  Reward: +0.057, PredError: 0.311, Valence: -0.083, MatchScore: 0.50\n",
            "[2026-02-19 10:28:17] Executed: Given the lack of a specific scenario or context, it's challenging to provide a concrete reflection or next step. However, if we consider a general situation where one feels stuck, a concrete next step could be to set aside dedicated time for brainstorming potential solutions or alternatives. This can help in generating new ideas and moving forward.\n",
            "[2026-02-19 10:28:17] Tick 24 complete\n",
            "[2026-02-19 10:28:22] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Open the door\n",
            "[2026-02-19 10:28:22] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:28:22] \n",
            "============================================================\n",
            "[2026-02-19 10:28:22] TICK 25\n",
            "[2026-02-19 10:28:22] ============================================================\n",
            "[2026-02-19 10:28:22] Attention spotlight: ['event_76683d05']\n",
            "[2026-02-19 10:28:22] Coherence C_total: 0.638\n",
            "[2026-02-19 10:28:22] Mode: ANSWER\n",
            "[2026-02-19 10:28:22] Energy: 0.92, Coherence: 0.62, Novelty: 0.45\n",
            "[2026-02-19 10:28:22] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:22] Generated 2 proposals\n",
            "[2026-02-19 10:28:22] [DEBUG] proposals_before_arbitration=4\n",
            "[2026-02-19 10:28:22] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:28:22] [DEBUG] p1: WORLD_ACT module=SELF_MAINTENANCE EU=1.15 cost=0.10\n",
            "[2026-02-19 10:28:22] [DEBUG] p2: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:22] [DEBUG] p3: WORLD_ACT module=SELF_MAINTENANCE EU=0.60 cost=0.20\n",
            "[2026-02-19 10:28:22] ‚öñÔ∏è  Reward: +0.088, PredError: 0.310, Valence: +0.099, MatchScore: 0.50\n",
            "[2026-02-19 10:28:22] Executed: üåç door is locked\n",
            "[2026-02-19 10:28:22] ‚úÖ Event closed: event_76683d05\n",
            "[2026-02-19 10:28:22] State saved\n",
            "[2026-02-19 10:28:22] Tick 25 complete\n",
            "[2026-02-19 10:28:27] \n",
            "============================================================\n",
            "[2026-02-19 10:28:27] TICK 26\n",
            "[2026-02-19 10:28:27] ============================================================\n",
            "[2026-02-19 10:28:27] Attention spotlight: []\n",
            "[2026-02-19 10:28:27] Coherence C_total: 0.639\n",
            "[2026-02-19 10:28:27] Mode: ANSWER\n",
            "[2026-02-19 10:28:27] Energy: 0.91, Coherence: 0.62, Novelty: 0.44\n",
            "[2026-02-19 10:28:27] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:27] Generated 1 proposals\n",
            "[2026-02-19 10:28:27] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:27] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:28] ‚öñÔ∏è  Reward: +0.089, PredError: 0.295, Valence: -0.044, MatchScore: 0.50\n",
            "[2026-02-19 10:28:28] Executed: Given the lack of a specific goal or scene, it's difficult to provide a relevant reflection or next step. However, if you were to set a goal, such as improving productivity, the next best step could be to identify specific tasks that need attention and allocate time blocks for their completion.\n",
            "[2026-02-19 10:28:28] Tick 26 complete\n",
            "[2026-02-19 10:28:33] \n",
            "============================================================\n",
            "[2026-02-19 10:28:33] TICK 27\n",
            "[2026-02-19 10:28:33] ============================================================\n",
            "[2026-02-19 10:28:33] Attention spotlight: []\n",
            "[2026-02-19 10:28:33] Coherence C_total: 0.639\n",
            "[2026-02-19 10:28:33] Mode: ANSWER\n",
            "[2026-02-19 10:28:33] Energy: 0.90, Coherence: 0.62, Novelty: 0.43\n",
            "[2026-02-19 10:28:33] Emotion: curious, Mood: 0.57\n",
            "[2026-02-19 10:28:33] Generated 1 proposals\n",
            "[2026-02-19 10:28:33] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:33] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:34] ‚öñÔ∏è  Reward: +0.113, PredError: 0.284, Valence: -0.015, MatchScore: 0.50\n",
            "[2026-02-19 10:28:34] Executed: Without a specific goal or scene provided, it's difficult to offer a concrete reflection or next step. However, if you are currently in a situation where you feel stuck, consider trying a different approach or seeking additional information before proceeding. For example, if your task involves data analysis, take a moment to review the data from a different angle or consult with a colleague for their perspective.\n",
            "[2026-02-19 10:28:34] Tick 27 complete\n",
            "[2026-02-19 10:28:39] \n",
            "============================================================\n",
            "[2026-02-19 10:28:39] TICK 28\n",
            "[2026-02-19 10:28:39] ============================================================\n",
            "[2026-02-19 10:28:39] Attention spotlight: ['self_thought_27']\n",
            "[2026-02-19 10:28:39] Coherence C_total: 0.639\n",
            "[2026-02-19 10:28:39] Mode: ANSWER\n",
            "[2026-02-19 10:28:39] Energy: 0.89, Coherence: 0.62, Novelty: 0.43\n",
            "[2026-02-19 10:28:39] Emotion: curious, Mood: 0.58\n",
            "[2026-02-19 10:28:39] Generated 2 proposals\n",
            "[2026-02-19 10:28:39] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:39] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:40] ‚öñÔ∏è  Reward: +0.094, PredError: 0.293, Valence: -0.038, MatchScore: 0.50\n",
            "[2026-02-19 10:28:40] Executed: Given that you're currently in a positive state and able to push forward decisively, the next best step is to identify a specific goal or task and commit to completing it without delay. Make a list of smaller, manageable steps if needed, and focus on executing them one by one. This approach will help maintain your momentum and ensure steady progress.\n",
            "[2026-02-19 10:28:40] ‚úÖ Event closed: self_thought_27\n",
            "[2026-02-19 10:28:40] Tick 28 complete\n",
            "[2026-02-19 10:28:45] \n",
            "============================================================\n",
            "[2026-02-19 10:28:45] TICK 29\n",
            "[2026-02-19 10:28:45] ============================================================\n",
            "[2026-02-19 10:28:45] Attention spotlight: []\n",
            "[2026-02-19 10:28:45] Coherence C_total: 0.640\n",
            "[2026-02-19 10:28:45] Mode: ANSWER\n",
            "[2026-02-19 10:28:45] Energy: 0.88, Coherence: 0.62, Novelty: 0.42\n",
            "[2026-02-19 10:28:45] Emotion: curious, Mood: 0.58\n",
            "[2026-02-19 10:28:45] Generated 1 proposals\n",
            "[2026-02-19 10:28:45] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:45] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:46] ‚öñÔ∏è  Reward: +0.126, PredError: 0.277, Valence: +0.001, MatchScore: 0.50\n",
            "[2026-02-19 10:28:46] Executed: Given the current lack of context or specific situation, it's difficult to provide a concrete reflection or next best step. However, if you can provide more details about the scenario or goal you're aiming for, I can offer a more tailored response.\n",
            "[2026-02-19 10:28:46] Tick 29 complete\n",
            "[2026-02-19 10:28:51] \n",
            "============================================================\n",
            "[2026-02-19 10:28:51] TICK 30\n",
            "[2026-02-19 10:28:51] ============================================================\n",
            "[2026-02-19 10:28:51] Attention spotlight: []\n",
            "[2026-02-19 10:28:51] Coherence C_total: 0.640\n",
            "[2026-02-19 10:28:51] Mode: ANSWER\n",
            "[2026-02-19 10:28:51] Energy: 0.87, Coherence: 0.62, Novelty: 0.41\n",
            "[2026-02-19 10:28:51] Emotion: curious, Mood: 0.58\n",
            "[2026-02-19 10:28:51] Generated 1 proposals\n",
            "[2026-02-19 10:28:51] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:28:51] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:52] ‚öñÔ∏è  Reward: +0.066, PredError: 0.307, Valence: -0.073, MatchScore: 0.50\n",
            "[2026-02-19 10:28:52] Executed: Given the lack of a specific goal or scene, it's difficult to provide a concrete reflection or next step. However, if we assume a general context of problem-solving or decision-making, the next best step would be to gather more information or clarify the objective before proceeding.\n",
            "[2026-02-19 10:28:52] State saved\n",
            "[2026-02-19 10:28:52] Tick 30 complete\n",
            "[2026-02-19 10:28:57] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Unlock the door\n",
            "[2026-02-19 10:28:57] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:28:57] \n",
            "============================================================\n",
            "[2026-02-19 10:28:57] TICK 31\n",
            "[2026-02-19 10:28:57] ============================================================\n",
            "[2026-02-19 10:28:57] Attention spotlight: ['event_61fd211d']\n",
            "[2026-02-19 10:28:57] Coherence C_total: 0.640\n",
            "[2026-02-19 10:28:57] Mode: ANSWER\n",
            "[2026-02-19 10:28:57] Energy: 0.86, Coherence: 0.62, Novelty: 0.40\n",
            "[2026-02-19 10:28:57] Emotion: curious, Mood: 0.58\n",
            "[2026-02-19 10:28:57] Generated 2 proposals\n",
            "[2026-02-19 10:28:57] [DEBUG] proposals_before_arbitration=4\n",
            "[2026-02-19 10:28:57] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:28:57] [DEBUG] p1: WORLD_ACT module=SELF_MAINTENANCE EU=1.15 cost=0.10\n",
            "[2026-02-19 10:28:57] [DEBUG] p2: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:28:57] [DEBUG] p3: WORLD_ACT module=SELF_MAINTENANCE EU=0.60 cost=0.20\n",
            "[2026-02-19 10:28:57] ‚öñÔ∏è  Reward: +0.096, PredError: 0.307, Valence: +0.108, MatchScore: 0.50\n",
            "[2026-02-19 10:28:57] Executed: üåç door unlocked\n",
            "[2026-02-19 10:28:57] ‚úÖ Event closed: event_61fd211d\n",
            "[2026-02-19 10:28:57] Tick 31 complete\n",
            "[2026-02-19 10:29:02] \n",
            "============================================================\n",
            "[2026-02-19 10:29:02] TICK 32\n",
            "[2026-02-19 10:29:02] ============================================================\n",
            "[2026-02-19 10:29:02] Attention spotlight: []\n",
            "[2026-02-19 10:29:02] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:02] Mode: ANSWER\n",
            "[2026-02-19 10:29:02] Energy: 0.93, Coherence: 0.61, Novelty: 0.39\n",
            "[2026-02-19 10:29:02] Emotion: neutral, Mood: 0.58\n",
            "[2026-02-19 10:29:02] Generated 1 proposals\n",
            "[2026-02-19 10:29:02] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:29:02] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:03] ‚öñÔ∏è  Reward: +0.149, PredError: 0.266, Valence: +0.029, MatchScore: 0.50\n",
            "[2026-02-19 10:29:03] Executed: Given the current lack of context or specific details about the situation, I would recommend conducting a thorough assessment of available resources and information before proceeding. This will help in identifying the most effective next steps. If you can provide more specifics about the scenario or goal, a more concrete reflection can be offered.\n",
            "[2026-02-19 10:29:03] Tick 32 complete\n",
            "[2026-02-19 10:29:08] \n",
            "============================================================\n",
            "[2026-02-19 10:29:08] TICK 33\n",
            "[2026-02-19 10:29:08] ============================================================\n",
            "[2026-02-19 10:29:08] Attention spotlight: []\n",
            "[2026-02-19 10:29:08] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:08] Mode: ANSWER\n",
            "[2026-02-19 10:29:08] Energy: 0.92, Coherence: 0.62, Novelty: 0.39\n",
            "[2026-02-19 10:29:08] Emotion: neutral, Mood: 0.57\n",
            "[2026-02-19 10:29:08] Generated 1 proposals\n",
            "[2026-02-19 10:29:08] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:29:08] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:10] ‚öñÔ∏è  Reward: +0.099, PredError: 0.290, Valence: -0.032, MatchScore: 0.50\n",
            "[2026-02-19 10:29:10] Executed: Given the lack of a specific scenario or context, it's difficult to provide a concrete reflection or next best step. However, if I were to suggest an alternative in such a case, consider defining the goal or situation more clearly. This will help in formulating a relevant and actionable response.\n",
            "[2026-02-19 10:29:10] Tick 33 complete\n",
            "[2026-02-19 10:29:15] \n",
            "============================================================\n",
            "[2026-02-19 10:29:15] TICK 34\n",
            "[2026-02-19 10:29:15] ============================================================\n",
            "[2026-02-19 10:29:15] Attention spotlight: ['self_thought_33']\n",
            "[2026-02-19 10:29:15] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:15] Mode: ANSWER\n",
            "[2026-02-19 10:29:15] Energy: 0.91, Coherence: 0.62, Novelty: 0.38\n",
            "[2026-02-19 10:29:15] Emotion: neutral, Mood: 0.57\n",
            "[2026-02-19 10:29:15] Generated 2 proposals\n",
            "[2026-02-19 10:29:15] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:29:15] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:16] ‚öñÔ∏è  Reward: +0.100, PredError: 0.290, Valence: -0.031, MatchScore: 0.50\n",
            "[2026-02-19 10:29:16] Executed: Since you're in a good state and feeling decisive, keep your momentum going. Identify the next small, actionable step towards your goals and commit to it fully. For example, if your goal is to finish a project, set a specific task to work on today, such as completing the introduction section or researching key data points.\n",
            "[2026-02-19 10:29:16] ‚úÖ Event closed: self_thought_33\n",
            "[2026-02-19 10:29:16] Tick 34 complete\n",
            "[2026-02-19 10:29:21] \n",
            "============================================================\n",
            "[2026-02-19 10:29:21] TICK 35\n",
            "[2026-02-19 10:29:21] ============================================================\n",
            "[2026-02-19 10:29:21] Attention spotlight: []\n",
            "[2026-02-19 10:29:21] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:21] Mode: ANSWER\n",
            "[2026-02-19 10:29:21] Energy: 0.90, Coherence: 0.62, Novelty: 0.37\n",
            "[2026-02-19 10:29:21] Emotion: neutral, Mood: 0.56\n",
            "[2026-02-19 10:29:21] Generated 1 proposals\n",
            "[2026-02-19 10:29:21] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:29:21] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:22] ‚öñÔ∏è  Reward: +0.070, PredError: 0.305, Valence: -0.068, MatchScore: 0.50\n",
            "[2026-02-19 10:29:22] Executed: Given the lack of a specific goal or scene, it's challenging to provide a concrete reflection or next step. However, if you could provide more context or a hypothetical scenario, I can offer a more tailored response. For now, consider clarifying your objective or providing a situation for discussion.\n",
            "[2026-02-19 10:29:22] State saved\n",
            "[2026-02-19 10:29:22] Tick 35 complete\n",
            "[2026-02-19 10:29:27] \n",
            "============================================================\n",
            "[2026-02-19 10:29:27] TICK 36\n",
            "[2026-02-19 10:29:27] ============================================================\n",
            "[2026-02-19 10:29:27] Attention spotlight: []\n",
            "[2026-02-19 10:29:27] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:27] Mode: ANSWER\n",
            "[2026-02-19 10:29:27] Energy: 0.89, Coherence: 0.61, Novelty: 0.36\n",
            "[2026-02-19 10:29:27] Emotion: neutral, Mood: 0.56\n",
            "[2026-02-19 10:29:27] Generated 1 proposals\n",
            "[2026-02-19 10:29:27] [DEBUG] proposals_before_arbitration=1\n",
            "[2026-02-19 10:29:27] [DEBUG] p0: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:28] ‚öñÔ∏è  Reward: +0.100, PredError: 0.290, Valence: -0.030, MatchScore: 0.50\n",
            "[2026-02-19 10:29:28] Executed: Given the current lack of a specific goal or scene, it's challenging to provide a relevant reflection or next step. However, if you could specify a scenario or set a goal, I can offer more concrete advice. For now, consider setting a clear objective or describing a situation for a more tailored response.\n",
            "[2026-02-19 10:29:28] Tick 36 complete\n",
            "[2026-02-19 10:29:33] === Session Complete ===\n",
            "\n",
            ">>> Injecting user input: Open the door\n",
            "[2026-02-19 10:29:33] Starting core loop for 6 ticks...\n",
            "[2026-02-19 10:29:33] \n",
            "============================================================\n",
            "[2026-02-19 10:29:33] TICK 37\n",
            "[2026-02-19 10:29:33] ============================================================\n",
            "[2026-02-19 10:29:33] Attention spotlight: ['event_19848ff7']\n",
            "[2026-02-19 10:29:33] Coherence C_total: 0.640\n",
            "[2026-02-19 10:29:33] Mode: ANSWER\n",
            "[2026-02-19 10:29:33] Energy: 0.88, Coherence: 0.61, Novelty: 0.36\n",
            "[2026-02-19 10:29:33] Emotion: neutral, Mood: 0.56\n",
            "[2026-02-19 10:29:33] Generated 2 proposals\n",
            "[2026-02-19 10:29:33] [DEBUG] proposals_before_arbitration=4\n",
            "[2026-02-19 10:29:33] [DEBUG] p0: WORLD_ACT module=WORLD EU=2.37 cost=0.00\n",
            "[2026-02-19 10:29:33] [DEBUG] p1: WORLD_ACT module=SELF_MAINTENANCE EU=1.15 cost=0.10\n",
            "[2026-02-19 10:29:33] [DEBUG] p2: REFLECT module=REFLECTOR EU=0.18 cost=0.30\n",
            "[2026-02-19 10:29:33] [DEBUG] p3: WORLD_ACT module=SELF_MAINTENANCE EU=0.60 cost=0.20\n",
            "[2026-02-19 10:29:33] ‚öñÔ∏è  Reward: +0.100, PredError: 0.305, Valence: +0.113, MatchScore: 0.50\n",
            "[2026-02-19 10:29:33] Executed: üåç door opened\n",
            "[2026-02-19 10:29:33] ‚úÖ Event closed: event_19848ff7\n",
            "[2026-02-19 10:29:33] Tick 37 complete\n"
          ]
        }
      ],
      "source": [
        "# CELL 17: Initialize and Run\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CR-SSCP v5.7.10 - Consciousness-like Cognitive Architecture\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "core_loop = CoreLoop(state_manager, llm)\n",
        "\n",
        "# Print initial state\n",
        "print(f\"Initial Coherence: {state_manager.state['coherence']['C_total']:.3f}\")\n",
        "print(f\"Initial Energy: {state_manager.state['drives']['energy']:.2f}\")\n",
        "print(f\"Initial Emotion: {state_manager.state['affect']['current_emotion']}\")\n",
        "print(f\"Mode: {state_manager.state['pb']['mode']}\")\n",
        "print(f\"\\nIdentity anchors:\")\n",
        "for anchor in state_manager.state['narrative']['identity_anchors']:\n",
        "    print(f\"  - {anchor}\")\n",
        "\n",
        "print(f\"\\nRunning {Config.MAX_TICKS} ticks (~{Config.MAX_TICKS * Config.TICK_INTERVAL // 60} minutes)...\\n\")\n",
        "\n",
        "# Run the loop\n",
        "if RUN_AUTOPILOT:\n",
        "    core_loop.run(max_ticks=Config.MAX_TICKS)\n",
        "else:\n",
        "    run_with_test_inputs(state_manager, core_loop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df62eece",
      "metadata": {
        "id": "df62eece"
      },
      "outputs": [],
      "source": [
        "# CELL 18: Analysis and Metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION ANALYSIS\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "final_state = state_manager.state\n",
        "\n",
        "print(f\"Total ticks: {final_state['tick_count']}\")\n",
        "print(f\"Sleep cycles: {final_state['sleep_count']}\")\n",
        "print(f\"Mode flips: {final_state.get('metrics', {}).get('mode_flip_count', 0)}\")\n",
        "\n",
        "print(f\"\\nFinal Metrics:\")\n",
        "print(f\"  Coherence (C_total): {final_state['coherence']['C_total']:.3f}\")\n",
        "print(f\"  - Evidence (Ce): {final_state['coherence']['Ce']:.3f}\")\n",
        "print(f\"  - Historical (Ch): {final_state['coherence']['Ch']:.3f}\")\n",
        "print(f\"  - Structural (Cs): {final_state['coherence']['Cs']:.3f}\")\n",
        "print(f\"  - Identity (Ci): {final_state['coherence']['Ci']:.3f}\")\n",
        "print(f\"  - Predictive (Cp): {final_state['coherence']['Cp']:.3f}\")\n",
        "\n",
        "print(f\"\\nDrive States:\")\n",
        "for drive, value in final_state['drives'].items():\n",
        "    print(f\"  {drive}: {value:.2f}\")\n",
        "\n",
        "print(f\"\\nAffective State:\")\n",
        "print(f\"  Emotion: {final_state['affect']['current_emotion']}\")\n",
        "print(f\"  Mood: {final_state['affect']['mood']:.2f}\")\n",
        "\n",
        "print(f\"\\nMemory:\")\n",
        "print(f\"  Grounded facts: {len(final_state['memory']['grounded'])}\")\n",
        "print(f\"  Ungrounded notes: {len(final_state['memory']['ungrounded'])}\")\n",
        "print(f\"  Quarantined: {len(final_state['memory']['quarantine'])}\")\n",
        "\n",
        "print(f\"\\nAgency:\")\n",
        "self_actions = sum(1 for a in final_state['agency']['authorship_log'] if a['authorship'] == 'self_initiated')\n",
        "total_actions = len(final_state['agency']['authorship_log'])\n",
        "external_actions = sum(1 for a in final_state['agency']['authorship_log'] if a['authorship'] == 'external_triggered')\n",
        "print(f\"  Self-initiated actions: {self_actions}/{total_actions}\")\n",
        "print(f\"  External-triggered actions: {external_actions}/{total_actions}\")\n",
        "if total_actions > 0:\n",
        "    print(f\"  Causal Closure Ratio (self-initiated/total): {self_actions/total_actions:.2%}\")\n",
        "\n",
        "print(f\"\\nClaim Ledger:\")\n",
        "print(f\"  Total claims: {len(final_state['claim_ledger'])}\")\n",
        "verified = sum(1 for c in final_state['claim_ledger'] if c.get('verifier_result') == 'pass')\n",
        "failed = sum(1 for c in final_state['claim_ledger'] if c.get('verifier_result') == 'fail')\n",
        "uncertain = sum(1 for c in final_state['claim_ledger'] if c.get('verifier_result') == 'uncertain')\n",
        "pending = sum(1 for c in final_state['claim_ledger'] if c.get('verifier_result') == 'pending')\n",
        "print(f\"  Verified claims: {verified}\")\n",
        "print(f\"  Failed claims: {failed}\")\n",
        "print(f\"  Uncertain claims: {uncertain}\")\n",
        "print(f\"  Pending claims: {pending}\")\n",
        "\n",
        "print(f\"\\nNarrative:\")\n",
        "print(f\"  Current arc: {final_state['narrative']['current_arc']['direction']}\")\n",
        "print(f\"  Theme: {final_state['narrative']['current_arc']['meaning']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"State saved to: {Config.STATE_PATH}\")\n",
        "print(f\"Logs saved to: {Config.LOG_PATH}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kpibRjA6f6ee",
      "metadata": {
        "id": "kpibRjA6f6ee"
      },
      "outputs": [],
      "source": [
        "# === Scientific Dashboard ===\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ticks = (state_manager.state.get('telemetry', {}) or {}).get('ticks', [])\n",
        "df = pd.DataFrame(ticks)\n",
        "\n",
        "if df.empty:\n",
        "    print(\"No telemetry yet. Run some ticks first.\")\n",
        "else:\n",
        "    # Core metrics\n",
        "    plt.figure()\n",
        "    plt.plot(df['tick'], df['C_total'])\n",
        "    plt.plot(df['tick'], df['Ce'])\n",
        "    plt.plot(df['tick'], df['energy'])\n",
        "    plt.plot(df['tick'], df['mood'])\n",
        "    plt.xlabel(\"tick\")\n",
        "    plt.ylabel(\"value\")\n",
        "    plt.title(\"Core Metrics Over Time\")\n",
        "    plt.legend([\"C_total\",\"Ce\",\"energy\",\"mood\"])\n",
        "    plt.show()\n",
        "\n",
        "    # Reward\n",
        "    plt.figure()\n",
        "    plt.plot(df['tick'], df['reward'])\n",
        "    plt.xlabel(\"tick\")\n",
        "    plt.ylabel(\"reward\")\n",
        "    plt.title(\"Reward Over Time\")\n",
        "    plt.show()\n",
        "\n",
        "    # Action timeline (categorical)\n",
        "    actions = df['action_type'].fillna(\"None\").astype(str)\n",
        "    cats = {a:i for i,a in enumerate(sorted(actions.unique()))}\n",
        "    y = actions.map(cats)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(df['tick'], y)\n",
        "    plt.yticks(list(cats.values()), list(cats.keys()))\n",
        "    plt.xlabel(\"tick\")\n",
        "    plt.title(\"Action Type Timeline\")\n",
        "    plt.show()\n",
        "\n",
        "    # META no-op streak\n",
        "    plt.figure()\n",
        "    plt.plot(df['tick'], df['meta_noop_streak'])\n",
        "    plt.xlabel(\"tick\")\n",
        "    plt.ylabel(\"streak\")\n",
        "    plt.title(\"META No-Op Streak\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ytzrdH6yf6ee",
      "metadata": {
        "id": "ytzrdH6yf6ee"
      },
      "outputs": [],
      "source": [
        "# === Human-friendly Room View ===\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "def pretty_room(world: dict) -> str:\n",
        "    objs = (world.get('objects', {}) if isinstance(world, dict) else {})\n",
        "    lamp = (objs.get('lamp', {}) or {}).get('state', '?')\n",
        "    box  = (objs.get('box', {}) or {}).get('state', '?')\n",
        "    door = (objs.get('door', {}) or {}).get('state', '?')\n",
        "\n",
        "    lamp_icon = \"üí°\" if lamp == \"on\" else \"üîå\"\n",
        "    box_icon  = \"üì¶(open)\" if box == \"open\" else \"üì¶(closed)\"\n",
        "    if door == \"open\":\n",
        "        door_icon = \"üö™(open)\"\n",
        "    elif door == \"unlocked\":\n",
        "        door_icon = \"üîìüö™\"\n",
        "    else:\n",
        "        door_icon = \"üîíüö™\"\n",
        "\n",
        "    return f\"\"\"\n",
        "**Sandbox Room**\n",
        "- Lamp: {lamp_icon} `{lamp}`\n",
        "- Box: {box_icon} `{box}`\n",
        "- Door: {door_icon} `{door}`\n",
        "\"\"\"\n",
        "\n",
        "w = (state_manager.state.get('workspace', {}) or {}).get('world', {})\n",
        "display(Markdown(pretty_room(w)))\n",
        "\n",
        "# Show last few telemetry lines as a story\n",
        "ticks = (state_manager.state.get('telemetry', {}) or {}).get('ticks', [])\n",
        "for t in ticks[-12:]:\n",
        "    print(f\"[t{t.get('tick')}] action={t.get('action_type')} reward={t.get('reward'):+.3f} world={t.get('world')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YTQCYLtlf6eh",
      "metadata": {
        "id": "YTQCYLtlf6eh"
      },
      "outputs": [],
      "source": [
        "# === Quick Animation (optional) ===\n",
        "import time\n",
        "from IPython.display import clear_output\n",
        "\n",
        "ticks = (state_manager.state.get('telemetry', {}) or {}).get('ticks', [])\n",
        "if not ticks:\n",
        "    print(\"No telemetry yet. Run some ticks first.\")\n",
        "else:\n",
        "    for t in ticks[-30:]:\n",
        "        clear_output(wait=True)\n",
        "        world = t.get('world', {})\n",
        "        print(f\"TICK {t.get('tick')} | action={t.get('action_type')} | reward={t.get('reward'):+.3f}\")\n",
        "        print(\"lamp:\", world.get('lamp'), \"box:\", world.get('box'), \"door:\", world.get('door'))\n",
        "        time.sleep(0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e31a35b",
      "metadata": {
        "id": "0e31a35b"
      },
      "source": [
        "## v5.5 Notes\n",
        "This notebook adds a Global Workspace Frame (single broadcast), active-inference style expected/actual updates (minimal), an anti-compulsion stuck detector, episodic memory logging, and stronger intent commitments.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "G4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f5d2927c3224ff7b21f620283078abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05139b4d11204a5c93cd4eed272f4e7f",
              "IPY_MODEL_f18e2ce4bb524085a7033d31e02edbe9",
              "IPY_MODEL_fa4cc571770f414d84f3fc180278ca4b"
            ],
            "layout": "IPY_MODEL_f11316d7efda4b1985970ceffc310bae"
          }
        },
        "05139b4d11204a5c93cd4eed272f4e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e419551b0f6546f18c3d20bc1f0673a5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_572bb8c98a6b42a8b18b7a575bfd6268",
            "value": "config.json:‚Äá100%"
          }
        },
        "f18e2ce4bb524085a7033d31e02edbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f00c1313a64a74bab40c536f742526",
            "max": 663,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f052187d69484f15adc412544e4ad1ac",
            "value": 663
          }
        },
        "fa4cc571770f414d84f3fc180278ca4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e599622b1d6e48589ba2b7f1469cd6be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c0ba8d646f9a4d2f9f63bd1b45c08289",
            "value": "‚Äá663/663‚Äá[00:00&lt;00:00,‚Äá203kB/s]"
          }
        },
        "f11316d7efda4b1985970ceffc310bae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e419551b0f6546f18c3d20bc1f0673a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "572bb8c98a6b42a8b18b7a575bfd6268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70f00c1313a64a74bab40c536f742526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f052187d69484f15adc412544e4ad1ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e599622b1d6e48589ba2b7f1469cd6be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0ba8d646f9a4d2f9f63bd1b45c08289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5269d342c3148e39c14176e2d1a7a8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bab3838849a5494c8713b2580ad18e5b",
              "IPY_MODEL_307a3614b30946cd80c8cc808f06b4b2",
              "IPY_MODEL_a7c652dd29074404986c22e011b8d7f0"
            ],
            "layout": "IPY_MODEL_d0fa80ff6f1b4daf8e98e6f755a5fb0a"
          }
        },
        "bab3838849a5494c8713b2580ad18e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff229569102c43fbb3281bda7d344def",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c4c359b2a9814970a661e4d23ace3f54",
            "value": "tokenizer_config.json:‚Äá"
          }
        },
        "307a3614b30946cd80c8cc808f06b4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c11c60e6fa534b07bd470ee39c223886",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_548b2f2b3526415caed0ae4c98b17fc5",
            "value": 1
          }
        },
        "a7c652dd29074404986c22e011b8d7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc95902ab98a49468bfa3284954f6a1b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a47c619f9f84c13b5df0f042e713da0",
            "value": "‚Äá7.30k/?‚Äá[00:00&lt;00:00,‚Äá2.57MB/s]"
          }
        },
        "d0fa80ff6f1b4daf8e98e6f755a5fb0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff229569102c43fbb3281bda7d344def": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4c359b2a9814970a661e4d23ace3f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c11c60e6fa534b07bd470ee39c223886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "548b2f2b3526415caed0ae4c98b17fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc95902ab98a49468bfa3284954f6a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a47c619f9f84c13b5df0f042e713da0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f992486633b4757a818771f8d5e2504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce03adac69264610aae18b08de9b758c",
              "IPY_MODEL_ae3fe71400c34c1eb71be36561cef620",
              "IPY_MODEL_8c716b57a8734b46a65c62fe988a760c"
            ],
            "layout": "IPY_MODEL_be97acba932f40a3aa84dcebefa28713"
          }
        },
        "ce03adac69264610aae18b08de9b758c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e2b48a9e0924c3e8b42963029c077eb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7170ab3f50b34e1fafa02a5a6f8cee0d",
            "value": "vocab.json:‚Äá"
          }
        },
        "ae3fe71400c34c1eb71be36561cef620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2c5fc0001744e0282f44be750b4305c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1579ce7f7f854ec8b6a142dfc4786304",
            "value": 1
          }
        },
        "8c716b57a8734b46a65c62fe988a760c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b4766eafc144a55b14d234e7469f226",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8c821484be5945cf92e691a8fa3e928d",
            "value": "‚Äá2.78M/?‚Äá[00:00&lt;00:00,‚Äá72.6MB/s]"
          }
        },
        "be97acba932f40a3aa84dcebefa28713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e2b48a9e0924c3e8b42963029c077eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7170ab3f50b34e1fafa02a5a6f8cee0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2c5fc0001744e0282f44be750b4305c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "1579ce7f7f854ec8b6a142dfc4786304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4b4766eafc144a55b14d234e7469f226": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c821484be5945cf92e691a8fa3e928d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3904626ba674211a85b93f43a18c227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87fee36f26fb49afbc62e9ef08df58af",
              "IPY_MODEL_e3f5fa2f329d4a358c993fecb78867be",
              "IPY_MODEL_5a443a4c1cb44bbea7cbaf00d0453de8"
            ],
            "layout": "IPY_MODEL_23a350b8cc594ae19a252338198719c4"
          }
        },
        "87fee36f26fb49afbc62e9ef08df58af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198217ff22ff4f8486932b8015be5a1e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0c20d1c1052f481c928563483463bacd",
            "value": "merges.txt:‚Äá"
          }
        },
        "e3f5fa2f329d4a358c993fecb78867be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15014e2edeb7423c928c70be659aba8f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a69bcfb5d72941ee82a9befb3d04c030",
            "value": 1
          }
        },
        "5a443a4c1cb44bbea7cbaf00d0453de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94dab1a7badd47b2883c39e3f6480529",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49f1ff1fa41f4a838efaae9d8039dc3b",
            "value": "‚Äá1.67M/?‚Äá[00:00&lt;00:00,‚Äá155MB/s]"
          }
        },
        "23a350b8cc594ae19a252338198719c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "198217ff22ff4f8486932b8015be5a1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c20d1c1052f481c928563483463bacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15014e2edeb7423c928c70be659aba8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a69bcfb5d72941ee82a9befb3d04c030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94dab1a7badd47b2883c39e3f6480529": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f1ff1fa41f4a838efaae9d8039dc3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "988a42a0be184405bde8e985774c6c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7bd5ff686304a1094e4750f82819a20",
              "IPY_MODEL_76bdaf1676324955a34d9caeee787f95",
              "IPY_MODEL_a0e7b6fed7be4b26892e7011e5a85e5f"
            ],
            "layout": "IPY_MODEL_be237618f03e417e877ae05d6f648bfe"
          }
        },
        "c7bd5ff686304a1094e4750f82819a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88c73680adff495689d64e37c7cfcfe7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4d91531e8b044820ba036417adcc9b94",
            "value": "tokenizer.json:‚Äá"
          }
        },
        "76bdaf1676324955a34d9caeee787f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed944bdef02c4f9c9f0e71dd6919ed59",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61ce3f85428a48d4a4b59eb1bea4dd16",
            "value": 1
          }
        },
        "a0e7b6fed7be4b26892e7011e5a85e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da3048d8ec343659398096ed64b10ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_73c79c98fe2b4e9896515e8b4c5134fc",
            "value": "‚Äá7.03M/?‚Äá[00:00&lt;00:00,‚Äá236MB/s]"
          }
        },
        "be237618f03e417e877ae05d6f648bfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c73680adff495689d64e37c7cfcfe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d91531e8b044820ba036417adcc9b94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed944bdef02c4f9c9f0e71dd6919ed59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "61ce3f85428a48d4a4b59eb1bea4dd16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3da3048d8ec343659398096ed64b10ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73c79c98fe2b4e9896515e8b4c5134fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a98677cc075345d8abf0185eb9025f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_81e17901df2b4f8989836f61a74f859e",
              "IPY_MODEL_e01d4e29983c4e65ad0637246fe4af86",
              "IPY_MODEL_9c102d4dffee4bbf8cd779a77a4cef05"
            ],
            "layout": "IPY_MODEL_ca178dc7731e420eae0453aa43803b7b"
          }
        },
        "81e17901df2b4f8989836f61a74f859e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f2902edba74430faac617b159de859b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d0578f0fd634408a8b82b02c5ea11f3b",
            "value": "model.safetensors.index.json:‚Äá"
          }
        },
        "e01d4e29983c4e65ad0637246fe4af86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29221c00d0994bc09597600a2e9b8c02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8273a638b4a24af29f25dbf11cf33c9b",
            "value": 1
          }
        },
        "9c102d4dffee4bbf8cd779a77a4cef05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef331ffbbda48229bc683f59811f09e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d32a742ffb544b309ac3fa7c08e3469c",
            "value": "‚Äá27.8k/?‚Äá[00:00&lt;00:00,‚Äá7.80MB/s]"
          }
        },
        "ca178dc7731e420eae0453aa43803b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f2902edba74430faac617b159de859b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0578f0fd634408a8b82b02c5ea11f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29221c00d0994bc09597600a2e9b8c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8273a638b4a24af29f25dbf11cf33c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cef331ffbbda48229bc683f59811f09e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d32a742ffb544b309ac3fa7c08e3469c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd8a9ecdb5374b1ebd521496ba64f1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f32b53a62e5481f882b08a5b9d2bdfb",
              "IPY_MODEL_587291b9298b44ee83b5981463e944f7",
              "IPY_MODEL_310de203afcc409aa0405301deeec7d0"
            ],
            "layout": "IPY_MODEL_fe01876d85eb45908edbf5ab45313ccf"
          }
        },
        "6f32b53a62e5481f882b08a5b9d2bdfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76a384dc2f3467383127e060945c8ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_41c32dee4a464dfbb9c6b6e9af04b2ce",
            "value": "Download‚Äácomplete:‚Äá100%"
          }
        },
        "587291b9298b44ee83b5981463e944f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3c65bdda1564b0cacda6135894e80e9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_28ecedb0f4844d8589afea98b46c76d7",
            "value": 1
          }
        },
        "310de203afcc409aa0405301deeec7d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb2c77e5032448fe92a6a1d3ac428b4c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fa56ed44aebe49e68ea52277ddd9ebc6",
            "value": "‚Äá15.2G/15.2G‚Äá[00:55&lt;00:00,‚Äá320MB/s]"
          }
        },
        "fe01876d85eb45908edbf5ab45313ccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a76a384dc2f3467383127e060945c8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c32dee4a464dfbb9c6b6e9af04b2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3c65bdda1564b0cacda6135894e80e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "28ecedb0f4844d8589afea98b46c76d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb2c77e5032448fe92a6a1d3ac428b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa56ed44aebe49e68ea52277ddd9ebc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "136538f2a57a4dc9aec5d62788e478b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6718dc91ddc8403f89797425d79cd7c5",
              "IPY_MODEL_8775af89c7f44c37b2594d7a4ba91253",
              "IPY_MODEL_58ef74a272af4396a4d566f92ce881d0"
            ],
            "layout": "IPY_MODEL_32de4b5b712f433fba346e8452e1395a"
          }
        },
        "6718dc91ddc8403f89797425d79cd7c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dad7c6854c9741559e64a5a319b61aed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5236f6b57a134d46ac2289bb25e42edb",
            "value": "Fetching‚Äá4‚Äáfiles:‚Äá100%"
          }
        },
        "8775af89c7f44c37b2594d7a4ba91253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_291d7f1076e04af7a5d2ccc30662a7bb",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b883cdef250449294a6998f7d9b9eb4",
            "value": 4
          }
        },
        "58ef74a272af4396a4d566f92ce881d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40fa0655bb7f49218d21f548eac36db4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a3865a47278b469c8daac03c0610730b",
            "value": "‚Äá4/4‚Äá[00:44&lt;00:00,‚Äá44.57s/it]"
          }
        },
        "32de4b5b712f433fba346e8452e1395a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad7c6854c9741559e64a5a319b61aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5236f6b57a134d46ac2289bb25e42edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291d7f1076e04af7a5d2ccc30662a7bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b883cdef250449294a6998f7d9b9eb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40fa0655bb7f49218d21f548eac36db4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3865a47278b469c8daac03c0610730b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed4953d8c31e4cbe82340bb0dc7adfb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd3d0aa0663344ad8a28d70d16b5aa73",
              "IPY_MODEL_f3395cc370574568b12caf4c662a4809",
              "IPY_MODEL_d42aa62591714f58a866b8e78f2ab7e4"
            ],
            "layout": "IPY_MODEL_e1460ae49dfb446b8e49cd2434b2f20d"
          }
        },
        "bd3d0aa0663344ad8a28d70d16b5aa73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2424c6ae7755418598d340366be0b2ab",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b2730d95780a4109b5e520f4afb73bb0",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "f3395cc370574568b12caf4c662a4809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20f4c02c2f80442b95db64509aa3f242",
            "max": 339,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_328ca137261647688ead9485051703f2",
            "value": 339
          }
        },
        "d42aa62591714f58a866b8e78f2ab7e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db00376d9fbf48bdb28cfc73705c0203",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b4094e65c77d461db7ca38b8f46fff10",
            "value": "‚Äá339/339‚Äá[00:02&lt;00:00,‚Äá171.66it/s,‚ÄáMaterializing‚Äáparam=model.norm.weight]"
          }
        },
        "e1460ae49dfb446b8e49cd2434b2f20d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2424c6ae7755418598d340366be0b2ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2730d95780a4109b5e520f4afb73bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20f4c02c2f80442b95db64509aa3f242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328ca137261647688ead9485051703f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db00376d9fbf48bdb28cfc73705c0203": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4094e65c77d461db7ca38b8f46fff10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c92d0e0b8184641aa5307325feb9997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3490be91b85475e8890615781000a5f",
              "IPY_MODEL_204e806b56c5479eb4f5e85dbe71b1d7",
              "IPY_MODEL_44aac55fc6584eceaa4351f33e21ff71"
            ],
            "layout": "IPY_MODEL_5cbada22ba8c4fdf957d5f42602dce69"
          }
        },
        "b3490be91b85475e8890615781000a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21af5cdcd3504ff7993fe248d51478e1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d1d5f7872ea541ccad7abf2e3979bc96",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "204e806b56c5479eb4f5e85dbe71b1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f3e58678bf24800a157e861268a157c",
            "max": 243,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09d37fdba31c4c089519d5119aa611d1",
            "value": 243
          }
        },
        "44aac55fc6584eceaa4351f33e21ff71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88ec6b84bccf4825ba6e0be9d6fbb280",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6681cfc1a26c46ccb43bbbee9d38a042",
            "value": "‚Äá243/243‚Äá[00:00&lt;00:00,‚Äá81.8kB/s]"
          }
        },
        "5cbada22ba8c4fdf957d5f42602dce69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21af5cdcd3504ff7993fe248d51478e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1d5f7872ea541ccad7abf2e3979bc96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f3e58678bf24800a157e861268a157c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d37fdba31c4c089519d5119aa611d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88ec6b84bccf4825ba6e0be9d6fbb280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6681cfc1a26c46ccb43bbbee9d38a042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}